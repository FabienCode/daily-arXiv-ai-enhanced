<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 81]
- [cs.CL](#cs.CL) [Total: 57]
- [cs.RO](#cs.RO) [Total: 29]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.LG](#cs.LG) [Total: 71]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 本文提出通过在训练过程中强制一致的字幕结构来提高生成文本到图像模型的提示依赖性问题，构建了一个19百万高质量带结构化字幕的图像数据集Re-LAION-Caption 19M，显著提升了模型的文本-图像对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有大型数据集如LAION-5B数据噪声大且结构不统一，导致生成模型难以准确遵守提示词，需大量提示工程。

Method: 构建了包含19百万高分辨率图像及按照主题、场景、美学、摄影细节四部分结构化字幕的Re-LAION-Caption 19M数据集，利用该数据集对PixArt-Σ和Stable Diffusion 2进行微调，并对比使用结构化和随机字幕的效果。

Result: 采用结构化字幕训练的模型在视觉问答（VQA）任务中表现出更高的文本与图像对齐评分，证明了结构化字幕对模型控制性和对齐性的正面影响。

Conclusion: 强制一致的字幕结构训练能有效提高文本到图像生成模型的可控性和提示词对齐能力，相关数据集已公开，具备较大应用价值。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [2] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 提出了一种名为CorrDetail的可解释性面部伪造检测框架，结合视觉细节增强和自我纠正机制，提升伪造细节识别能力和检测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前面部伪造检测技术存在缺乏可解释性或易产生错误信息的问题，亟需一种能够准确揭示伪造细节且鲁棒性强的方法。

Method: 设计了视觉细节增强模块和基于错误引导提问的自我纠正机制，结合视觉信息补偿和模型偏差减少的融合决策策略，提高伪造细节识别和决策能力。

Result: 实验表明，CorrDetail不仅在性能上超越最新方法，还能精准识别伪造细节，具有良好泛化性。

Conclusion: CorrDetail有效提升了面部伪造检测的准确性和可解释性，具备较强的应用潜力。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [3] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: 本文提出了YOLO-APD，一种改进YOLOv8的行人检测算法，针对复杂几何路面提升检测精度和效率，在定制数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶视觉感知系统在复杂几何路面（如Type-S曲面）上存在行人检测困难，传统基于RGB摄像头的方法效果有限。

Method: YOLO-APD引入了参数免费的SimAM注意力机制、高效的C3Ghost模块、新的SimSPPF多尺度特征池化模块、Mish激活函数和智能聚集分发（IGD）模块，并结合车辆转向动态实现自适应兴趣区域处理。

Result: 在CARLA定制数据集上，YOLO-APD达到了77.7%的mAP@0.5:0.95和超过96%的行人召回率，显著优于YOLOv8基线，同时保持100FPS的实时处理速度。在KITTI数据集验证中表现良好，但需进行领域适应。

Conclusion: YOLO-APD在复杂非结构化驾驶环境中实现了高精度、高效和适应性强的行人检测，推动了基于低成本传感器的自动驾驶感知系统的发展，提高了安全性与可靠性。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [4] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: 本文提出了一种名为Spotlight的虚拟染色方法，利用机器学习从无标记的显微镜图像预测荧光图像，重点关注细胞结构，提升了形态学表示与像素精度。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色方法在训练时对所有像素一视同仁，导致背景噪音和伪影被重现，无法专注于有生物学意义的信号。

Method: Spotlight采用基于直方图的前景估计来掩盖像素级损失，并通过软阈值预测计算Dice损失，实现形状感知学习。

Result: 在三维基准数据集上，Spotlight提升了形态学表示能力，同时保持了像素级精度，生成的虚拟染色更适用于分割和细胞轮廓分析等下游任务。

Conclusion: Spotlight方法有效引导模型关注相关细胞结构，改善虚拟染色的质量和实用性。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [5] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: 本文评估了基础模型在农业监测中的应用效果，并提出了理想农业基础模型的需求框架，强调了开发专门农业基础模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着人口增长和气候变化加剧，全球粮食安全面临挑战，需求创新的农业生产力解决方案。基础模型在遥感和气候科学中表现优异，具备在农业监测中应用的潜力，但相关应用尚未充分探索。

Method: 从农业领域需求出发，构建理想农业基础模型（CropFM）框架，调研比较现有通用基础模型，并在三类农业任务（作物类型映射、作物物候估计、作物产量估计）中对两种模型进行定量评估。

Result: 评估显示现有通用基础模型在农业特定任务上的表现各异，存在不足，支持开发专门的农业基础模型以更好满足农业监测需求。

Conclusion: 现有通用基础模型不足以全面满足农业监测需求，亟需设计针对农业的专属基础模型以提升农业任务的效果和效率。

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [6] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的方法，结合人工主观评估，通过训练分类器和生成对抗网络提升水下图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下图像通常质量较差，需要自动增强方法以提升图像清晰度和视觉效果。

Method: 使用带有专家标注的公开水下图像数据集训练分类器判别图像质量，随后基于多种增强标准训练生成对抗网络优化低质量图像。

Result: 生成对抗网络模型在PSNR、SSIM和UIQM等指标上表现优异，尤其在颜色保真度和图像锐度方面提升显著。

Conclusion: 结合主观评估和多重增强指标的深度学习模型有效提升了水下图像的视觉和测量质量。

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [7] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 提出了pFedMMA，一种结合多模态适配器的个性化联邦学习框架，有效提升视觉-语言模型在去中心化异构数据上的适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在去中心化且异质数据环境中高效适配仍有挑战，且现有基于提示调优的方法常在个性化与泛化之间难以平衡。

Method: 设计了包含模态特定上/下投影层及全局共享投影的多模态适配器，并通过非对称优化策略实现本地个性化和全局泛化的协同训练，且仅共享全局部分以节省通信成本。

Result: 在包含域和标签漂移的11个数据集上，pFedMMA表现出个性化和泛化的最佳折中，优于最新的联邦提示调优方法。

Conclusion: pFedMMA有效解决了视觉-语言模型在个性化联邦学习中的泛化与个性化权衡问题，具备通信效率高的优势。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [8] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: 本文提出了LoongX，一个基于多模态神经生理信号的无手图像编辑方法，结合脑电图、功能近红外光谱等信号，利用扩散模型实现直观高效的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑依赖手动输入，操作繁琐且对行动或语言能力受限的人群不友好，因此提出利用脑机接口信号实现无手图像编辑。

Method: LoongX通过跨尺度状态空间模块提取多模态信号特征，动态门控融合模块整合特征，并通过对扩散变换器微调实现编辑语义对齐，同时利用对比学习预训练编码器以对应认知状态与语义意图。

Result: 实验显示LoongX在多种评测指标上达到或超过传统文本驱动方法，且结合神经信号与语音时表现更优，验证了该方法的有效性。

Conclusion: 神经驱动生成模型在实现无手、直观的图像编辑方面展现出巨大潜力，为认知驱动的创意技术开辟了新方向。

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [9] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 本文对2023年以来动作生成的最新研究进行了系统性综述，重点根据生成策略对方法进行分类，分析架构、条件机制和生成设置，并总结评价指标和数据集，旨在帮助研究者更好理解和比较现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着动作生成领域迅速发展，出现了多种生成范式和方法，研究者需要一个全面系统的综述，以便更清晰地比较不同方法及识别开放问题。

Method: 基于生成策略对多种动作生成方法进行深入分类，分析其架构原则、条件输入、生成设置，并汇总评价指标和数据集。综述重点覆盖2023年之后的顶会论文，反映最新进展。

Result: 本文归纳了动作生成领域多样的生成模型及其优缺点，系统整理了评价标准和数据集，为该领域研究提供了全面参考和比较基础。

Conclusion: 通过详尽的分类和分析，本文为动作生成研究提供了一个及时且系统的基础参考，帮助推动领域未来的发展和挑战解决。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [10] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D扩散编辑和逆向渲染的3D高斯点阵局部编辑方法，实现了视角一致的高效精细修改。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义解析性能不足，难以在3D场景中精确定位和编辑局部区域，限制了编辑的质量和控制能力。

Method: 利用2D扩散编辑准确识别修改区域，通过逆向渲染实现3D定位，结合2D基础模型预测深度图初始化粗略的3D高斯点阵，支持迭代视角一致的细节和纹理增强。

Result: 实验表明该方法达到了最先进的性能，编辑速度提高了4倍，提升了效率和效果。

Conclusion: 该方法为3D场景的局部编辑提供了更精准、高效和视角一致的解决方案，显著改进了编辑质量和操作便捷性。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [11] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: OpenWorldSAM是一种基于Segment Anything Model v2扩展的开域分割框架，能处理未见类别，实现灵活高效的开域语义和实例分割。


<details>
  <summary>Details</summary>
Motivation: 当前模型难以基于开放语言提示进行准确的空间对象分割，尤其面对多样化和未见类别时表现不足。

Method: OpenWorldSAM结合轻量级视觉语言模型的多模态嵌入，采用统一提示、多样化语言描述、冻结预训练模块仅训练4.5M参数，并通过定位破局嵌入与交叉注意力提升实例感知能力。

Result: 在多个数据集（如ADE20k、PASCAL等）上，OpenWorldSAM实现了开词汇语义、实例和全景分割的最新性能，展现强大的零样本泛化能力。

Conclusion: OpenWorldSAM有效结合多模态信息和高效训练策略，成功实现开域分割任务中的灵活性、资源效率和泛化能力。

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [12] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 本文开发了一种基于视觉引导和AI驱动的变速喷雾系统，能够实时检测杂草、估计冠层大小，并动态调整喷嘴控制，实现精准施药。


<details>
  <summary>Details</summary>
Motivation: 传统农业中均匀且过量的除草剂喷洒带来高成本、环境污染及抗药性杂草问题，亟需精准施药技术减少资源浪费和环境负担。

Method: 建设了集成轻量级YOLO11n和YOLO11n-seg深度学习模型的系统，部署在NVIDIA Jetson Orin Nano平台，通过Arduino Uno控制电磁阀喷嘴，实现基于冠层分割结果的动态喷雾。

Result: YOLO11n模型在室内试验中表现优异，mAP@50达0.98，喷雾覆盖率与冠层大小正相关，最大达24.22%，验证了系统能实时依据冠层调整喷雾强度。

Conclusion: 结合实时深度学习与低成本嵌入式硬件的定点除草剂喷洒系统展示出良好的应用潜力，未来将扩展检测多种杂草并进行室内及田间验证。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [13] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 该论文通过分析自然驾驶视频和大型视觉模型，旨在早期识别老年驾驶员的认知状态，特别是轻度认知障碍和阿尔茨海默病。


<details>
  <summary>Details</summary>
Motivation: 认知衰退（包括阿尔茨海默病和轻度认知障碍）诊断昂贵且耗时，常被漏诊。利用驾驶行为作为认知状态的数字指纹有助于早期检测。

Method: 提出一种结合大型视觉模型和自然驾驶视频的框架，分析驾驶行为以分类认知状态并预测疾病进展，将车辆作为诊断工具。

Result: 该方法能识别功能损害的早期预警信号，实现对认知衰退的早期检测。

Conclusion: 研究提升了早期检测能力，支持非侵入式、可扩展的监测系统开发，有助于减轻老年认知衰退带来的社会与经济负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [14] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 本文提出了基于尺度不变噪声的云扩散模型，替代传统白噪声扩散模型，以改善图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用的白噪声与自然图像的尺度不变统计特性不符，限制了模型性能。

Method: 将具有幂律尺度不变特性的噪声引入扩散模型，形成云扩散模型。

Result: 云扩散模型预计可实现更快推断、更丰富高频细节和更强控制能力。

Conclusion: 利用自然图像的尺度统计特性设计扩散模型有望提升图像生成效果，后续工作将进行实证比较。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [15] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet提出了一种新颖的多视角扩散架构，通过共享潜编码空间，实现了从单张图像生成多视角一致性图像，显著提升了图像质量和三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在从单张图像生成多视角图像时难以保证视角之间的空间一致性，导致三维网格重建质量下降。

Method: LoomNet采用相同的扩散模型多次并行处理，结合三正交平面上的视角特定编码，通过编码融合和信息传播，构建统一且一致的潜编码空间，最终渲染多视角图像。

Result: LoomNet在15秒内生成16个高质量且一致的视角图像，实验中在图像质量和重建指标上均优于现有最先进方法，并能创造性地生成多样且合理的新视角。

Conclusion: LoomNet有效解决了单图生成多视角图像的一致性问题，提升了图像质量和三维重建表现，展现了较强的创造力和实用价值。

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [16] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 该论文提出了统一的文本-图像检索模型llama-nemoretriever-colembed，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了满足跨模态检索系统日益增长的需求，提出一个统一模型提升检索性能。

Method: 基于NVIDIA Eagle2视觉语言模型，采用双向注意力替代因果注意力，结合ColBERT风格的晚期交互机制，并通过两阶段训练策略提升检索能力。

Result: 3B模型在ViDoRe V1和V2数据集上分别取得91.0和63.5的NDCG@5分数，均名列榜首。

Conclusion: 该模型在文本-图像跨模态检索领域实现了领先性能，同时对存储和效率的权衡进行了详尽分析。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [17] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种程序化增强管道，通过模拟光学折射和天气引起的失真，增强非洲驾驶场景的低成本单目行车记录仪视频，提升自动驾驶感知能力。


<details>
  <summary>Details</summary>
Motivation: 非洲等发展中地区自动驾驶数据集匮乏，尤其是在不同类型道路上的数据缺失，限制了自动驾驶感知技术的有效应用。

Method: 设计了包括光学折射模块和天气模块的增强管道，模拟镜头失真、空气湍流、雾霾和镜头眩光等影响，并对生成的数据进行基线性能测试。

Result: 构建了带有真实折射和天气失真效果的数据集，并提供了三个图像恢复模型的基线测试结果。

Conclusion: 该工具包和数据集将推动低资源环境下自动驾驶感知的研究，尤其促进非洲驾驶场景的相关研究，无需高成本数据采集和标注。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [18] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReLayout的新方法，利用relation-CoT和显式关系定义，改进大语言模型在内容感知布局生成中的表现，生成结构更合理、美观多样的设计布局。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的布局生成方法无法充分理解视觉主题和设计元素之间的空间关系，导致生成布局存在结构和多样性问题。

Method: 引入显式关系定义（如区域、显著性、边距）细化布局注释，并将布局分解为更小、结构化和递归的子布局；基于这些关系设计了布局原型重平衡采样器，解决样本偏差带来的生成均匀性问题。

Result: 实验结果表明，ReLayout在生成结构合理、风格多样且符合人类审美的布局方面优于基线方法，且生成结果更具解释性。

Conclusion: 通过引入关系推理和采样器，ReLayout有效提升了内容感知布局生成的质量，实现了更结构化、多样且美观的设计布局。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [19] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的跨模态转换引导网络（CTNet）用于多模态人脸反欺骗，旨在提升多模态识别在跨域和缺失模态情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态人脸反欺骗因不同模态间数据分布差异大及部分模态缺失，导致识别难度加大。活体人脸在单模态内差异小，跨模态特征转换更一致，为解决该问题提供思路。

Method: 设计跨模态转换引导网络，学习活体样本一致的跨模态特征转换构建泛化特征空间，同时学习活体与欺骗样本不一致的转换检测OOD攻击，并通过从RGB模态中辅助生成红外和深度特征以应对模态缺失。

Result: 实验结果表明，CTNet在多种协议下均优于现有两类多模态人脸反欺骗方法，展示出更好的泛化能力和鲁棒性。

Conclusion: CTNet有效解决了多模态人脸反欺骗中跨域分布不一致和模态缺失的问题，提升了系统的安全性和实用性。

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [20] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 该文提出了一种基于条件扩散模型的半监督缺陷检测框架DSYM，通过两阶段协同训练和分阶段联合优化，有效利用有标签和无标签数据提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业质检中的缺陷检测依赖人工或早期图像处理方法，存在效率低、成本高及鲁棒性差的问题，需要利用半监督学习提升检测效率和数据利用率。

Method: 本文设计了一个基于条件扩散模型的半监督缺陷检测框架，采用两阶段协同训练机制和分阶段联合优化策略，结合CLIP跨模态特征进行噪声过滤，生成多尺度伪缺陷样本，提升伪标签质量。

Result: 在NEU-DET数据集上，提出方法以相同有标签数据实现了78.4%的mAP@0.5指标；仅用40%有标签数据时依然取得75.1%的mAP@0.5，显示出优越的数据效率和检测性能。

Conclusion: 该框架为工业质检缺陷检测提供了一种高精度、低标注依赖的解决方案，有效提升了半监督学习在工业缺陷检测中的应用价值。

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [21] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于二维高斯的快速视频隐式表示方法GSVR，实现了高帧率解码和快速训练，显著提升了视频隐式表示的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积网络的视频隐式表示方法训练时间长且解码速度慢，限制了其应用。

Method: 设计混合形变场结合三平面运动和多项式运动，提出基于动态的时间切分策略，以及量化感知微调和高斯压缩方法以提升效率和表现。

Result: 在Bunny和UVG数据集上达到35+ PSNR，训练时间降至2秒/帧，解码速度超800 FPS，解码速度比现有方法提升10倍，视频插值任务性能与SOTA相当，视频压缩优于NeRV。

Conclusion: GSVR有效解决了视频隐式表示中的训练速度和解码效率问题，具有较高的实用价值和推广潜力。

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [22] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0是一个开源OCR和文档解析工具包，具备多语种文本识别、层级文档解析和关键信息提取功能。


<details>
  <summary>Details</summary>
Motivation: 为满足大语言模型时代对文档理解日益增长的需求，提升OCR及文档解析效率和准确性。

Method: 提出PP-OCRv5、多语种文本识别，PP-StructureV3层级文档解析，PP-ChatOCRv4关键信息提取三大方案，模型参数少于1亿但准确率高。

Result: 模型在保持较小参数量的同时，准确率和效率与大规模视觉语言模型相当，且支持异构硬件加速。

Conclusion: PaddleOCR 3.0提供高质量模型库和高效训练推理部署工具，方便开发者构建智能文档应用。

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [23] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了Accordion框架，将AI生成的像素图设计转换为可编辑的分层设计，并通过用户提示优化文本内容。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI能提供高质量的非分层设计，但这类设计缺少编辑性，而人类设计师仍受其启发，因此希望实现从像素图到分层设计的转换。

Method: 利用视觉语言模型在三个阶段执行不同任务，采用自上而下的方法基于参考图像全局指导分层设计，结合多种视觉专家模型辅助生成分层元素。

Result: 在自建数据集Design39K上训练并测试，实验证明Accordion在DesignIntention基准任务中表现优异，包括文字到模板、添加文字到背景和文字反解码等任务，并在设计变体生成方面效果出色。

Conclusion: Accordion有效实现了AI像素图设计向可编辑分层设计的转变，提升了AI设计作品的实用性和灵活性，具有较高的应用价值。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [24] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Kernel Density Steering (KDS)的新推断框架，通过集体局部模式搜索提升扩散模型的图像修复质量，显著减少伪影，增强保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像修复时存在保真度不稳定和伪影问题，需一种方法提升生成图像的质量和稳定性。

Method: KDS利用N个扩散样本形成的粒子集，通过计算它们在图像块级别的核密度估计梯度，引导每个粒子的图像块向集体中高密度区域聚集，实现“集体智慧”的局部模式搜索。该方法作为插件框架，无需重训练即可应用于不同扩散采样器。

Result: 大量数值验证显示，KDS在真实世界的超分辨率和图像修复任务中，显著提升了生成图像的定量指标和视觉质量。

Conclusion: KDS通过利用多样本集体智慧，有效避免了独立采样或模型缺陷导致的伪影问题，显著改善扩散模型的图像修复性能，且易于集成应用。

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [25] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的生成方法GenHMC，通过利用大量无配对的头戴摄像(HMC)捕获数据，生成高质量的合成HMC图像，实现了虚拟现实和增强现实中头像动画的更精确表达和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以获得同步的真实脸部状态图像，且依赖大量的配对数据采集，收集成本高且难以复用，不利于虚拟和增强现实中实现真实感的头像动画。

Method: 提出GenHMC，通过生成模型直接基于大量无配对的HMC数据，结合外部球面摄像头捕获的头像状态，生成高质量的合成HMC图像，实现表情、视角和面部外观的解耦和准确建模。

Result: 方法有效解耦了面部表情和视角与面部外观，实现了更准确的地面真实数据生成，且能泛化到未见身份，训练的面部编码器表现出更高的数据效率和先进的准确率。

Conclusion: GenHMC突破了传统依赖配对数据的限制，提升了头像动画的真实性和泛化能力，为VR/AR头像合成提供了有效的新思路。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [26] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: 本文提出AdaptaGen，一种层次化语义优化框架，结合矩阵式提示优化与多角度语义理解，有效提升领域特定图像生成的质量和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定图像生成方法存在语义理解与视觉表现分离、缺乏领域语义约束导致幻觉和语义偏差问题。

Method: 提出AdaptaGen框架，通过跨模态适配机制和智能内容合成保持核心主题，同时设计两阶段标题语义转换以保证语义连贯和视觉多样性。

Result: 在40个类别的多样本数据集上，仅用每类16张图像，显著提升图像质量、多样性和语义一致性。

Conclusion: AdaptaGen有效整合语义和视觉信息，解决领域特定图像生成的关键问题，实现高质量和语义准确的图像生成。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [27] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一个用于复合图像检索的新方法，通过分割和双重聚焦映射减少噪声影响，利用文本引导对焦点进行自适应调整，提高检索效果。


<details>
  <summary>Details</summary>
Motivation: 当前复合图像检索方法忽视了视觉数据中主要与噪声部分的不同以及文本在图像修改中的优先作用，导致查询特征退化和视觉聚焦偏差。

Method: 提出了基于聚焦映射的特征提取器，包括主要部分分割和双重聚焦映射模块，识别图像中的主要部分，减少噪声影响；并设计文本引导的聚焦修正模块，根据文本修改需求自适应调整参考图像上的聚焦。整体构建了基于分割的聚焦偏移修正网络（OFFSET）。

Result: 在四个基准数据集上的全面实验显示，所提方法在复合图像检索任务中表现优越。

Conclusion: 通过聚焦映射和文本引导的聚焦修正，有效提升了复合图像检索的性能，验证了方法的有效性和优越性。

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [28] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种基于复数轮廓波变换的结构知识引导复数扩散模型，用于极化合成孔径雷达图像分类，提升了边缘细节保护和区域一致性。


<details>
  <summary>Details</summary>
Motivation: 传统实值扩散模型难以捕捉极化SAR数据的复数相位信息，且难以保持细结构细节。

Method: 利用复数轮廓波变换将数据分解为低高频分量，设计知识引导的复数扩散网络建模低频统计特征，同时用高频结构特征引导扩散过程，多尺度多方向学习高频特征提升分类精度。

Result: 在三个真实极化SAR数据集上，所提方法显著优于现有技术，尤其在边缘细节保护和复杂地形区域的同质性维护方面表现突出。

Conclusion: 结合复数轮廓波变换与结构知识引导的复数扩散模型有效提升了极化SAR图像分类的性能，特别是细节保持和结构信息利用方面。

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [29] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种动态秩自适应（DRA）方法，通过根据特征重要性动态分配适应秩，提升视觉-语言模型在新类上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对大规模视觉-语言模型的微调方法难以保持对新类别的强泛化能力，原因在于对所有编码器token一视同仁，导致过拟合不重要的特征。

Method: DRA利用序列注意力进行token重要性分组，根据重要性动态分配高低秩适应，并设计通道响应机制优先保留和适应最信息量大的特征通道，同时引入L1正则化稳定训练。

Result: 大量实验表明，DRA在多个基准测试，包括基础-新类分类、跨数据集评估和领域泛化等任务上均优于现有方法，尤其提升了新类性能。

Conclusion: DRA通过动态调整适应秩和通道响应机制，有效防止过拟合，提高了视觉-语言模型对新类别的泛化能力，具有广泛应用前景。

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [30] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种结合RGB摄像头与事件传感器的飞行器姿态估计方法，通过传感器融合克服单一传感器的不足，在复杂光照条件下实现更准确的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB摄像头的姿态估计在恶劣光照条件下面临成像伪影问题，而事件传感器虽在动态范围上表现更好，但空间分辨率低且在低运动时信噪比下降。为了克服各自的局限，提出传感器融合方法。

Method: 采用光学分束棱镜实现RGB和事件传感器的光学与时间对齐，利用基于RANSAC的算法融合两种传感器数据实现姿态估计，同时引入dropout不确定性估计检测极端影响条件。

Result: 在实验室条件下采集了多种复杂光照下的卫星姿态数据，实验证明了事件-RGB融合方法的有效性，提升了姿态估计的鲁棒性与准确度。

Conclusion: 事件传感器与RGB传感器的融合显著提升了航天器姿态估计的性能，支持事件传感器在该领域的应用，并公开了数据集以促进相关研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [31] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 提出基于扩散模型的脑损伤分析与逆转框架，实现核心病灶分离及损伤前脑组织重建，提升病灶分割与表征准确性。


<details>
  <summary>Details</summary>
Motivation: 现有脑损伤分割方法未区分不可逆损伤组织和因病灶生长导致变形的组织，缺乏有效的病灶过程模型及逆向分析方法。

Method: 采用扩散模型框架，首先分割异常区域，接着估计并逆转组织变形，将移位组织恢复至原位，分离出初始损伤核心区域，最后对核心病灶部位进行修复以重建损伤前的健康脑组织。

Result: 该方法在病灶分割、特征表征及脑区标注方面较传统方法表现出更高准确性，通过合成多张模拟病灶脑图像验证逆向模拟效果。

Conclusion: 该框架有效解决了脑损伤不可逆损伤与组织变形的区分问题，且实现了对病灶进展过程的逆转，具备重要的临床及科研价值。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [32] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: 本文提出了R-VLM，一种利用放大区域提议和IoU感知目标函数的GUI元素精确定位方法，显著提升了不同GUI平台上的元素定位和导航任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的GUI代理方法需要处理大量无关信息，且采用的交叉熵损失函数无法有效反映元素定位质量，导致定位精度不足。

Method: 提出R-VLM方法，通过放大关注区域进行更精确的元素定位，同时设计了基于IoU的目标函数，使模型能够更好地学习高重合度的定位结果。

Result: 在ScreenSpot和AgentStudio基准测试中，R-VLM使元素定位准确率提高了13%；在AITW和Mind2Web导航任务中，准确率提升3.2%至9.7%。

Conclusion: R-VLM成功结合了视觉语言模型与目标检测技术，显著提升了GUI元素定位的准确性和自动化导航任务的性能。

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [33] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 本文提出了首个大规模、多样且带有丰富字幕的医疗视频生成数据集MedVideoCap-55K，及基于该数据集开发的医疗视频生成模型MedGen，实现了视觉质量和医疗准确性的领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗视频生成研究缺乏大规模高质量数据，导致生成结果不真实或错误，影响临床训练和教育等应用。

Method: 构建包含55,000多个医疗场景视频剪辑的MedVideoCap-55K数据集，训练和开发MedGen模型，提升医疗视频的生成质量与准确性。

Result: MedGen在多个基准测试中在视觉质量和医疗准确性上表现优异，达到开源模型领先水平，媲美商业系统。

Conclusion: 该数据集与模型为医疗视频生成领域提供了重要资源，推动了相关研究发展。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [34] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了集成结构提示（ISP）方法，通过建模多模态间提示与令牌的结构关系，提升预训练视觉语言模型在迁移学习中的表现，同时动态调整样本损失系数以增强新类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法未考虑提示和令牌之间的结构关系，且难以平衡基础类别与新类别的性能表现。

Method: 提出ISP，包含自我结构提示模块和跨结构提示模块，分别建模同模态与跨模态的结构关系；设计样本探测模块根据样本难度动态调整损失系数，防止模型对简单样本过拟合。

Result: 在基础到新类别泛化、跨数据集评估和领域泛化三大设置中，ISP展现出与最先进方法相媲美的竞争性性能。

Conclusion: ISP有效增强了视觉语言模型中多模态信息的交互和传递，改善了模型对新类别的泛化能力，具有显著的实用价值。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [35] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为LiON-LoRA的新框架，用于改进视频扩散模型中LoRA的融合，实现对摄像机轨迹和物体运动的精准控制。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法难以同时精准控制视频中的摄像机轨迹和物体运动，原因在于融合不稳定和非线性扩展问题。

Method: LiON-LoRA通过线性扩展性、正交性和范数一致性三大原则重构LoRA融合，利用浅层VDM特征正交性实现解耦控制，并在扩散变换器中引入可控token和自注意力机制以线性调节运动幅度。同时扩展到时序生成，实现空间和时间上的可控性统一。

Result: LiON-LoRA在轨迹控制准确性和运动强度调整方面优于现有方法，且在极少训练数据下表现出更好的泛化能力。

Conclusion: LiON-LoRA有效解决了视频扩散模型中LoRA融合的不稳定和非线性扩展问题，实现了对摄像机与物体运动的精确解耦控制，推动了视频生成中的空间与时序可控性发展。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [36] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 该论文综述了高光谱异常检测的多种方法，并对17个数据集上的性能进行了比较，发现深度学习模型在检测准确率方面表现最佳，而统计模型运算速度最快。


<details>
  <summary>Details</summary>
Motivation: 高光谱异常检测在多个领域有重要应用，但现有方法仍面临计算复杂度高、对噪声敏感及泛化能力不足等挑战。

Method: 对统计模型、基于表示的方法、传统机器学习和深度学习四类高光谱异常检测技术进行归类和综合比较，使用17个基准数据集及多种性能指标评估其效果。

Result: 深度学习模型在检测准确率方面表现出色，统计模型在计算速度上具有优势，不同方法各有优缺点。

Conclusion: 本文为高光谱异常检测领域的研究人员提供了全面的性能评估和未来研究方向的见解，促进技术进步和实际应用。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [37] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 本文提出了SenseShift6D，一个涵盖多种传感器曝光、增益、深度模式和光照条件的RGB-D数据集，用于评估6D物体位姿估计在真实环境变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有6D物体位姿估计数据集缺乏对真实环境中传感器设置和光照变化影响的考察，限制了模型在实际应用中的适应能力。

Method: 收集了13种RGB曝光、9种RGB增益、自动曝光、4种深度捕获模式及5种光照强度下的三种物体共101.9k RGB和10k深度图像。采用测试时传感器控制方法，动态调整传感器参数以提升位姿估计性能。

Result: 在SenseShift6D数据集上，测试时传感器控制相较于数字数据增强带来更大性能提升，且单独适应RGB或深度传感器均有效，联合适应效果最佳。性能提升可媲美或超过通过增加真实训练数据量和多样性获得的效果。

Conclusion: SenseShift6D推动6D位姿估计评测从数据中心向传感器感知鲁棒性转变，为自适应、自调节感知系统在复杂真实环境中实现鲁棒操作奠定基础。

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [38] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: 提出了一种用于显微镜图像的全自动白平衡算法，实验验证其优于传统白平衡方法。


<details>
  <summary>Details</summary>
Motivation: 显微镜中获取颜色准确、平衡的图像具有挑战性，传统白平衡算法不足以满足显微图像的需求。

Method: 设计了一种自动白平衡机制，专门用于显微镜图像的颜色校正，并在200幅显微图像上进行了实验验证。

Result: 算法在病理形态学常用显微样本以及免疫组化染色图像上表现优于传统数码摄影白平衡算法。

Conclusion: 该自动白平衡算法能够更有效地纠正显微镜图像颜色，提升图像质量，适合用于显微图像的颜色校正。

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [39] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt框架从单视角图像生成高保真的可交互关节物体，实现了部分分解和关节运动建模，提升了生成质量和应用范围。


<details>
  <summary>Details</summary>
Motivation: 当前图像到3D的方法忽略了部件分解与关节建模，神经重建方法依赖密集多视角数据，限制了扩展性，因此需要一种基于单视角图像的高品质可关节物体生成方法。

Method: DreamArt采用三阶段流程：1）结合图像到3D生成、带掩模提示的3D分割和部分非完整补全，实现带部件分割和完整的3D网格重建；2）微调视频扩散模型，利用移动部件掩模和非完整图像捕获部件级关节先验；3）优化双四元数表示的关节运动，并进行全局纹理细化和重绘保证纹理一致性。

Result: 实验表明DreamArt生成的关节物体具备准确部件形状、高保真外观和合理关节运动，效果优异且具备良好可扩展性。

Conclusion: DreamArt提供了一种有效的单视角图像驱动高质量、可交互关节物体生成解决方案，推动了关节资产生成技术的发展。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [40] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本指令的多功能虚拟试衣助手TalkFashion，实现全身服装更换和局部编辑，提升了灵活性和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法多为端到端单任务网络，缺乏多功能性和灵活性，用户体验受限。

Method: 利用大型语言模型理解用户文本指令，自动选择执行任务的处理流程；引入基于指令的局部重新绘制模型，结合多模态模型实现无人工遮罩的局部编辑。

Result: 实验表明TalkFashion在语义一致性和视觉质量上优于现有方法。

Conclusion: 通过结合大型语言模型和多模态模型，TalkFashion实现了灵活且自动化的多功能虚拟试衣，显著提升用户体验。

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [41] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 提出了SPADE框架，通过结合反演引导的微调和空间感知关系图变换器，显著提升开集场景图生成中的空间关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉语言模型的方法在空间关系推理方面存在局限，难以准确区分物体的相对位置，导致关系预测效果不佳。

Method: 采用扩散模型的反演过程保持空间结构，设计两步法：1) 反演引导的轻量微调，将通用扩散模型校准为适用于PSG的去噪网络；2) 空间感知关系图变换器，捕获局部及长距离上下文信息，提升关系查询质量。

Result: 在PSG和Visual Genome数据集上，SPADE在封闭集和开放集任务中均优于现有最先进方法，尤其在空间关系预测上表现突出。

Conclusion: SPADE有效结合扩散模型的空间结构保留能力和空间感知推理机制，显著改善了开集场景图生成中的空间关系理解与预测。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [42] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为DREAM的端到端自回归模型，用于文档重建，解决了现有方法中的误差传播和布局信息缺失问题，并引入了新的评估指标和数据集，实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档重建方法多为多阶段，存在误差传播导致性能下降的问题，且生成式模型虽端到端但忽视了元素布局信息，影响重建效果。

Method: 提出DREAM模型，将文本图像转化为包含丰富元素信息的文档重建序列，采用端到端自回归方式；并定义标准化重建任务，引入DSM指标和DocRec1K数据集进行评测。

Result: 实验证明DREAM在文档重建任务中表现优异，在布局分析、文本识别、表格结构识别、公式识别及阅读顺序检测等子任务中均显示出竞争力和兼容性。

Conclusion: DREAM模型有效克服了传统方法的不足，实现全面高效的文档重建，具备良好的多任务适应能力，为文档分析领域提供了新的技术手段。

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [43] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 本论文通过引入太阳高度角作为全局条件变量，实现了白天光照变化的合成摄像头传感器数据，解决了人工标注困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中高质量自主驾驶传感器数据获取困难，特别是白天光照变化的合成数据缺乏研究，原因是标注数据稀缺。

Method: 将太阳高度角作为全局条件变量计算，无需人工标注，同时设计了针对光照敏感性的归一化方法，在扩散模型中精准捕捉光照特性和光照相关噪声。

Result: 该方法成功模拟了光照特性及基于光照的图像噪声，提升了合成数据的真实性和多样性。

Conclusion: 引入太阳高度角条件变量和归一化策略有效解决了白天光照变化模拟的难题，为合成自主驾驶传感器数据提供了新的解决方案。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [44] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 提出了一种生成3D桥梁数据的系统框架，解决了真实数据不完整性问题，提高点云分割和补全性能。


<details>
  <summary>Details</summary>
Motivation: 传统桥梁检测人工效率低，3D点云数据因缺失标注和扫描遮挡导致应用受限。

Method: 自动生成带组件级实例标注、高保真颜色和法向量的完整点云，并模拟不完整点云以训练分割和补全网络。

Result: PointNet++在实际桥梁语义分割中取得84.2% mIoU，微调KT-Net在组件补全任务表现优越。

Conclusion: 提出的框架和数据集为桥梁3D视觉分析提供创新方法，有助于基础设施自动化管理与维护。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [45] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出了一种创新的“2D-3D-2D”框架，通过将二维图像中的对象提升为三维表示，在三维空间中进行编辑后再投影回二维图像，实现了高一致性和身份保持的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽在二维图像编辑中表现出色，但由于其基于像素操作的特性，难以保持编辑的一致性和对象身份完整性。

Method: 提出将二维对象提升至受刚性约束的三维环境中进行编辑，再将编辑后的三维对象重新投影并无缝修复回二维图像，从而实现更精准的图像编辑。

Result: 实验表明，该框架在整体性能上优于现有的二维编辑方法，如 DragGAN 和 DragDiffusion，能够实现高度一致且稳定的对象身份保持。

Conclusion: 通过引入三维建模和编辑过程，该方法有效克服了二维像素编辑的局限，大幅提升了图像编辑的一致性和真实性。

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [46] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出一种结合视觉基础模型与大型语言模型的框架，实现视频的高级因果推理和未来预测，表现优越且具备零-shot泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在高阶认知任务如因果推理和未来预测方面表现不足，缺乏常识性世界知识。

Method: 设计一种基于Q-Former架构的融合模块，将复杂的视觉特征转化为语言对齐表示，并采用两阶段训练策略（大规模对齐预训练及针对推理任务的微调）。

Result: 模型在多个复杂基准测试上达到最先进性能，具备显著的零-shot泛化能力，并且消融实验证明各模块的重要性。

Conclusion: 研究推动了机器感知从简单识别向真正认知理解的发展，为更智能的AI系统奠定基础。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [47] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: 本文针对少样本语义分割中的两个关键难点，提出了I$^2$R方法，通过高阶语义特征和定向掩码策略提升分割准确率。


<details>
  <summary>Details</summary>
Motivation: 少样本语义分割受支持图像与查询图像的语义差异以及视觉上相似但语义不同区域的影响，导致分割效果不佳。

Method: 提出I$^2$R方法：1）利用类别特定的高级语义表示聚合支持与查询图像的全局语义线索，实现更准确的区域定位；2）采用定向掩码策略抑制支持与查询像素对中的不一致特征，减少误判。

Result: 在PASCAL-5$^i$和COCO-20$^i$数据集的1-shot设置中，I$^2$R分别提升了1.9%和2.1%的mIoU，优于现有最先进方法。

Conclusion: I$^2$R有效解决了少样本语义分割中图像间语义差异及图像内视觉误差问题，显著提升了分割性能。

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [48] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为USIGAN的免疫组化虚拟染色方法，通过去除弱配对条件下的空间异质影响，提高了生成结果的语义一致性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 传统免疫组化虚拟染色在弱配对条件下，邻近切片的空间异质性导致生成结果在形态结构与染色模式之间的一对多映射不准确，影响病理语义一致性。

Method: 提出了无平衡自信息特征传输(USIGAN)，去除弱配对条件的联合边际分布中的弱配对项，设计了无平衡最优传输一致性(UOT-CTM)机制和病理自我对应(PC-SCM)机制，构建形态和染色图像的相关矩阵。

Result: 在两个公开数据集上的实验表明，该方法在多项临床重要指标（如IoD和Pearson-R相关系数）上表现优越，临床相关性更强。

Conclusion: USIGAN有效解决了弱配对条件下空间异质性问题，提升了虚拟免疫组化染色图像的内容和病理语义一致性，为病理分析提供了一种高效且经济的解决方案。

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [49] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为DFYP的动态融合框架，用于基于遥感数据的作物产量预测，通过增强光谱表示和边缘敏感的空间建模，提高了跨作物类型和年份的预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于遥感的作物产量预测方法面临空间模式复杂、光谱特征异质和农业条件动态变化，导致空间建模能力有限且跨作物和年份泛化能力弱。

Method: DFYP包括三个关键模块：（1）分辨率感知通道注意力（RCA）模块，适应性重加权输入光谱通道；（2）自适应算子学习网络（AOL-Net），动态选择卷积核算子以增强边缘敏感的空间特征提取；（3）双分支架构和可学习融合机制，联合建模局部空间细节和全局上下文信息，实现跨分辨率和跨作物泛化。

Result: 在多年的MODIS数据集和多作物Sentinel-2数据集上的大量实验表明，DFYP在RMSE、MAE和R2指标上均优于当前最先进方法，适用于不同空间分辨率、作物类型和时间段。

Conclusion: DFYP通过光谱通道关注和动态卷积算子选择，有效提升了遥感作物产量预测的准确性和泛化能力，增强了模型在多样化农业场景中的实用性和鲁棒性。

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [50] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为D-FCGS的前馈压缩框架，用于动态高斯点云序列的高效编码，实现了快速压缩和高保真度重建。


<details>
  <summary>Details</summary>
Motivation: 现有的动态3D高斯点云压缩方法通常依赖于场景重建和优化编码，限制了泛化能力和实时性。

Method: 设计了基于I-P帧组结构和稀疏控制点的运动提取方法，结合双重先验的熵模型进行前馈式压缩，并采用控制点引导的运动补偿及细化网络提升视角一致性重建效果。

Result: 该方法无需针对单个场景优化，能在两秒内实现超过40倍的压缩比，且在率失真性能上与基于优化的方法相当，保持了跨视角的视觉质量。

Conclusion: D-FCGS有效提升了动态3D高斯点云的压缩效率和适用性，为沉浸式自由视点视频的传输和存储提供了可扩展的解决方案。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [51] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: 提出了GeoMag框架，提升遥感图像多粒度解析能力，支持像素级任务并减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言模型（RS-VLMs）多局限于图像和区域级任务，像素级任务能力不足，且处理高分辨率图像时计算资源消耗大，限制实际应用。

Method: 提出GeoMag框架，采用任务驱动多粒度分辨率调整（TMRA）和提示引导语义感知裁剪（PSC），动态聚焦任务相关区域，减少任务无关区域分辨率，提高关键区域表征，降低计算成本。

Result: 在10个基准测试中，GeoMag在像素级任务表现突出，同时在其它粒度任务上也表现优异，优于现有RS-VLMs。

Conclusion: GeoMag有效解决了RS-VLMs在像素级任务和计算资源方面的不足，具备广泛的多粒度遥感图像理解能力。

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [52] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 本文首次系统研究了时序缺失多模态数据对视觉追踪性能的影响，提出了一种灵活的异构专家混合融合框架和视频级掩码策略，实现了对不同时序缺失率和场景复杂度的适应。


<details>
  <summary>Details</summary>
Motivation: 现有多模态追踪器因架构刚性，难以应对视频中传感器同步问题导致的模态缺失，性能显著下降，该问题尚未被充分研究。

Method: 提出一种动态激活计算单元的异构专家混合融合机制，结合视频级掩码策略，实现对缺失数据率和场景复杂度的自适应，保证时序一致性和空间完整性。

Result: 该方法在9个基准测试中均达到或超过了现有最先进水平，在完整模态和缺失模态场景下表现优越。

Conclusion: 本文提出的灵活融合框架有效提升了时序不完整多模态视觉追踪的鲁棒性，且适应不同缺失率和场景复杂度，具有广泛应用潜力。

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [53] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本文针对遥感图像场景分类中可解释人工智能方法及其评估指标进行了系统分析，揭示了现有方法和指标的局限性，并提出了选择指导。


<details>
  <summary>Details</summary>
Motivation: 现有xAI方法和评估指标多由计算机视觉领域自然图像发展而来，直接应用于遥感领域存在适用性问题。

Method: 系统分析了五类十种评估指标，结合五种特征归因方法，在三个遥感数据集上进行了详尽的实验验证。

Result: 发现扰动方法对基线和空间特征敏感，梯度方法多标签时表现差，相关传播偏差大；评估指标中忠实度指标与扰动方法问题类似，定位和复杂度指标对大空间类不可靠，鲁棒性和随机化指标表现稳定。

Conclusion: 基于发现的问题，提出了遥感图像场景分类中解释方法、评估指标和超参数选择的指导方案。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [54] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多轮对话的多模态大模型视觉定位强化学习框架MGPO，通过自动裁剪关键视觉区域，提升模型对高分辨率图像的处理能力，无需额外标注即获得强大定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型处理高分辨率图像时生成大量无关视觉token，导致效率低且定位能力不足。

Method: 提出MGPO框架，基于多轮对话自动裁剪图像，通过强化学习训练模型利用二元奖励实现视觉定位能力，并采用多轮对话模版解决冷启动问题。

Result: MGPO在不使用定位标注的视觉问答数据上训练后，较GRPO提升5.4%（MME-Realworld）和5.2%（OOD V* Bench）性能，且在Qwen2.5-VL-7B上超越OpenAI的o1和GPT-4o模型。

Conclusion: MGPO无需额外定位标注即可有效增强大模型视觉定位能力，对处理高分辨率多模态输入有显著提升，具有广泛应用前景。

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [55] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: 本文通过引入几何信息（单目深度估计）提升视频实例分割（VIS）的鲁棒性，提出了三种集成范式，实验证明扩展深度通道（EDC）和共享ViT（SV）方法显著提升了性能，在OVIS基准上取得了56.2 AP的新纪录。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割在处理对象遮挡、运动模糊和外观变化时存在挑战，现有方法难以实现稳定的时序关联。引入几何信息有望解决这些问题。

Method: 提出三种结合单目深度估计与视频实例分割的方法：扩展深度通道（EDC）将深度图作为输入通道；共享ViT骨干网络（SV）实现深度估计与分割任务共享同一网络；深度监督（DS）作为辅助训练引导。

Result: 实验证明ECD和SV显著提升了VIS的鲁棒性，其中EDC方法采用Swin-L骨干网络在OVIS数据集上达到56.2的AP分数，刷新了最先进成绩。DS方法效果有限。

Conclusion: 深度信息是提升视频实例分割鲁棒性的关键因素，合理融合深度估计能显著增强视频理解能力。

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [56] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 本文提出了一种稀疏表示方法，显著提升神经表面重建的分辨率和内存效率，实现了比传统密集体素表示更高精度的3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统的密集3D特征体表示虽然有效，但在提高体素分辨率时内存消耗剧增，限制了重建质量。

Method: 采用两阶段方法：先训练网络预测体素占据率，再仅在占据率高的体素中计算特征和体积渲染，并设计了高效采样和查询的自定义算法以支持稀疏体积。

Result: 在公共数据集上实验显示，该方法在不降低性能的情况下，存储需求降低50倍以上，支持512^3分辨率重建，优于传统128^3分辨率和现有方法。

Conclusion: 提出的基于稀疏表示的神经表面重建方法显著提升了重建分辨率和效率，突破了内存瓶颈，实现了更精确的重建效果。

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [57] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 本文提出了Tora2，一种改进的扩散变换器模型，实现了多实体外观和运动的同时定制。


<details>
  <summary>Details</summary>
Motivation: 现有的运动引导视频生成方法在外观和运动的多样化定制能力上存在限制，难以精细保留视觉细节和处理多模态信息融合。

Method: 引入解耦个性化提取器获取多实体的个性化嵌入，设计门控自注意力机制融合轨迹、文本和视觉信息，采用对比损失优化轨迹动态和实体一致性。

Result: Tora2在多实体外观和运动定制能力上表现突出，相较现有方法具有更好的视觉细节保持和运动控制效果。

Conclusion: Tora2实现了视频生成领域多实体外观与运动的同时定制，推动了多条件视频生成技术的发展。

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [58] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: 本文提出了一种名为T-LoRA的扩散模型微调框架，针对单张图像的模型个性化，解决小样本条件下过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型微调在样本有限时易过拟合，影响泛化和多样性，尤其是单图像自定义需求强烈。

Method: 引入时间步骤依赖的低秩适应方法，采用动态微调策略和正交初始化保证参数组件独立。

Result: 实验显示T-LoRA优于标准LoRA及其他个性化方法，兼顾了概念还原度和文本对齐度。

Conclusion: T-LoRA在数据和资源受限条件下拥有较大应用潜力，提升了扩散模型的个性化效果。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [59] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一个自动生成合成三元组数据集CIRHS和一个新的图文检索框架CoAlign，实现了基于合成数据集的零样本和监督训练下的高效图像检索。


<details>
  <summary>Details</summary>
Motivation: 现有的图文组合检索方法依赖昂贵的手工标注三元组，限制了其扩展性和零样本能力。

Method: 利用大语言模型生成多样提示，控制文本到图像生成模型产出包含相同元素的图像对，从而自动生成合成三元组，构建CIRHS数据集。提出混合上下文对齐(CoAlign)框架实现全局对齐和局部推理。

Result: 基于合成数据集训练的CoAlign在三大基准零样本任务上表现优异，验证了合成训练的可行性。监督训练下优于所有现有的监督方法。

Conclusion: 自动合成数据和新的检索框架为图文组合图像检索提供了高效可扩展的解决方案，提升了零样本和有监督性能。

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [60] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SCINet的部分多标签学习框架，旨在通过捕捉标签与实例之间的共现模式，提升部分标签数据的学习效果。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习面临的核心挑战是准确识别标签与实例之间的模糊关系，现有方法未充分利用标签与实例的共现关系。

Method: 提出了SCINet模型，包括双主导提示模块（使用多模态模型捕获文本-图像相关性）、跨模态融合模块（联合建模标签间、实例间及实例-标签共现关系）、以及内在语义增强策略（通过多样图像变换强化语义理解）。

Result: 在四个广泛使用的基准数据集上的大量实验结果表明，SCINet优于当前最先进的部分多标签学习方法。

Conclusion: SCINet通过有效利用标签与实例的语义共现模式，实现了部分多标签学习的性能提升，验证了其模型设计和增强策略的有效性。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [61] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: 本文提出了一种基于集成的方法来提升深度伪造检测模型在不同数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测模型虽在基准数据集上表现优异，但在分布外数据上表现显著下降，亟需提升泛化能力。

Method: 基于近期开源基准，结合多个顶尖异构模型的预测概率进行集成，实现跨数据集的稳健性能。

Result: 在两个不同的分布外数据集上实验表明，单一模型表现不稳定，集成方法则提供了更稳定可靠的性能。

Conclusion: 异构模型集成为现实应用中缺乏伪造类型及质量先验知识的深度伪造检测提供了一种稳健且易扩展的解决方案。

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [62] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种在无GNSS信号的城市环境中，通过与卫星图像对齐来实现激光雷达点云精准地理配准和空间校正的方法。


<details>
  <summary>Details</summary>
Motivation: 城市高楼和桥梁遮挡导致GNSS信号不可用，传统依赖GNSS和IMU数据的方法定位不准确。

Method: 利用预训练的Point Transformer模型分割道路点，提取路网骨架和交叉点，与目标地图对齐；先全局刚性对齐，再局部RBF插值校正；基于SRTM数据进行高度修正。

Result: 在KITTI和珀斯数据集上，定位精度分别提升55.3%和77.4%，高度相关性分别提升30.5%和50.4%。

Conclusion: 该方法有效提升了无GNSS环境下激光雷达点云的空间定位和高度精度，支持城市大尺度3D地图重建。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [63] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 本论文提出了一种名为GCDA的新型扩散模型框架，用以提升文本到图像生成中生成文字的可读性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽然在生成逼真多样图像方面表现出色，但无法生成可读、准确拼写的文字，限制了实际应用。

Method: GCDA框架包含三部分：双流文本编码器（融合语义和字形信息）、字符感知注意力机制（配合注意力分离损失减少注意力混淆）以及内嵌OCR的微调阶段（通过文本感知损失优化字符准确性）。

Result: GCDA在MARIO-10M和T2I-CompBench等数据集上实现了文本生成相关指标的新高，字符错误率从0.21降至0.08，词错误率从0.25降至0.15，同时保持了图像生成的高质量。

Conclusion: GCDA有效解决了文本到图像生成中字符识别难题，显著提升了文字的可读性和准确性，推动此类模型在广告、教育和设计等实用领域的应用。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [64] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出VisualSpeaker方法，利用光度真实的可微分渲染和视觉语音识别监督，提升3D面部动画的真实感和准确度。


<details>
  <summary>Details</summary>
Motivation: 现有3D面部动画方法依赖网格模型，难以充分利用2D视觉技术的快速发展，限制了动画质量提升。

Method: 采用基于光度真实3D高斯点渲染的可微分渲染技术，结合视觉语音识别模型形成感知式唇读损失函数，监督训练3D面部动画生成。

Result: 在MEAD数据集上，VisualSpeaker在唇部顶点误差指标上提升56.1%，同时生成动画的感知质量显著提高，并保持了网格驱动动画的可控性。

Conclusion: VisualSpeaker在提升高保真3D面部动画质量的同时，为手语动画中相似手势的区分提供了准确的口型信息，展示了良好应用前景。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [65] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出MEDTalk框架，实现基于音频的细粒度动态情感3D面部动画，提升表情自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多使用静态预定义情感标签，导致表情单一且不自然。

Method: 通过跨重构过程分离内容和情感嵌入，实现唇动和表情的独立控制，并融合音频与文本信息动态调整情感强度，多模态输入增强个性化控制。

Result: 生成的3D面部动画表现出生动且同步的唇动和情感变化，可集成于MetaHuman工业生产流程。

Conclusion: MEDTalk有效提升了音频驱动表情动画的自然性和多样性，为动态情感表达提供了灵活控件及多模态交互支持。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [66] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态因果分析模型（MCAM），通过构建视觉和语言模态间的潜在因果结构，实现了驾驶行为识别及因果推理的新突破。


<details>
  <summary>Details</summary>
Motivation: 现有方法挖掘因果关系浅显，无法有效处理模态间的伪相关，也忽视了自车级别的因果建模，限制了自动驾驶视频理解效果。

Method: 设计多层次特征提取器捕捉长距离依赖，构建驾驶状态的有向无环图实现因果分析模块，利用视觉语言变换器对关键视觉特征和对应语言表达进行对齐。

Result: 在BDD-X和CoVLA数据集上，MCAM实现了视觉语言因果关系学习的SOTA性能，并展示出对视频序列中因果特征的优秀捕捉能力。

Conclusion: MCAM有效突破了现有方法局限，提升了自动驾驶视频中的因果关系建模能力，具备良好的实际应用潜力。

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [67] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的法线积分方法，能够显式处理深度不连续性，并适用于通用中心相机模型。


<details>
  <summary>Details</summary>
Motivation: 现有法线积分方法多隐式处理深度不连续性且仅限于正交或理想针孔相机，限制了其应用范围。

Method: 通过局部平面假设，建立表面法线与光线方向之间的约束，显式建模深度不连续性，并支持通用中心相机。

Result: 在标准法线积分基准测试中，本文方法达到最先进的性能，并首次支持通用中心相机模型。

Conclusion: 该方法更精确地近似了深度与表面法线的关系，拓宽了法线积分的应用边界，提升了重建的准确性。

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [68] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新型自然无约束对抗样本生成方法ScoreAdv，能够生成高质量且具有强攻击性的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击方法依赖于ℓ_p范数扰动，与人类感知能力不符，现有GAN方法图像质量差，扩散模型尚未充分利用其去噪能力，急需一种更自然且有效的攻击手段。

Method: 提出了ScoreAdv方法，通过可解释的对抗引导机制逐步调整采样分布，以及利用显著性图将参考图像的视觉信息注入对抗样本生成过程，实现自然无约束对抗样本的高质量生成。

Result: 在ImageNet和CelebA数据集的十个目标模型上，ScoreAdv在黑盒和白盒攻击中均表现出领先的攻击成功率和图像质量，同时在防御措施下保持较强的鲁棒性。

Conclusion: ScoreAdv有效结合扩散模型的去噪能力和对抗引导策略，成功生成自然且强攻击性的对抗样本，可攻击分类和检索模型，推动了自然无约束对抗样本领域的发展。

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [69] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: 提出了一套无接触多模态远程生理情感识别数据库CAST-Phys，包含面部视频与多种生理信号，提升真实场景下的情感识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别依赖接触式设备，可能影响被试情绪真实性，且缺乏高质量多模态数据集，限制了情感识别系统准确性和发展。

Method: 构建了包含光电容积描记(PPG)、皮肤电活动(EDA)、呼吸频率(RR)等多种生理信号及高分辨率无损面部视频的CAST-Phys数据集，用于远程多模态情感识别。

Result: 分析表明生理信号在实际情境下补充面部表情信息关键，融合多模态数据能显著提升远程无接触情感识别性能。

Conclusion: CAST-Phys数据库的建立和多模态远程情感识别方法的实验验证推动了无接触情感识别技术的发展，有利于实现更真实、准确的情感计算应用。

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [70] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer的多物种植物识别方案，在PlantCLEF 2025挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 针对植被象限图像中的多物种植物识别问题，提升识别准确率和模型泛化能力。

Method: 采用微调后的ViT模型进行图像分块推理，结合4x4切分策略匹配感受野，通过PaCMAP和K-Means进行视觉聚类及地理位置过滤，最后用投票和贝叶斯先验加权聚合预测结果。

Result: 该方法在未额外训练的情况下获得私有榜单宏平均F1值0.348，排名第二。

Conclusion: 结合视觉变换器、切分策略及域适应聚类对多物种植物识别效果显著，且方法代码开源保障了复现性。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [71] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 本文提出通过构建合成文化数据集CulTwin并微调CLIP模型，解决视觉语言模型在文化细节识别上的不足，实现了更细粒度的文化差异识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉语言模型难以区分视觉相似但文化背景不同的概念，主要由于缺乏高质量的文化特定数据集、缺少上下文知识融合及缺乏突出微妙差异的负样本。

Method: 设计数据构建流程，利用开源视觉语言模型和文本生成图像扩散模型制作包含概念-标题-图像三元组的合成文化数据集CulTwin；在此基础上微调CLIP，采用定制对比学习方法加强文化概念与上下文增强标题及合成图像的对齐。

Result: CultureCLIP在多个文化相关基准测试中，相较原CLIP模型，在细粒度文化概念识别上最高提升5.49%，且保持了原有的泛化能力。

Conclusion: 通过合成数据集与定制训练策略有效提升了视觉语言模型识别细微文化差异的能力，验证了该方法在文化特征捕捉上的先进性。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [72] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock提出了一种基于3D Gaussian Splatting的几何感知反射建模框架，解决了现有新视角合成方法在反射表面建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和3D Gaussian Splatting在反射场景中通常将反射误判为物理几何，导致重建质量下降，且现有几何约束不完善，难以处理复杂几何反射，产生模糊和表面伪影。

Method: 提出了一个基于3D Gaussian Splatting的Ref-Unlock框架，通过双分支表示和高阶球面谐波捕捉高频反射细节，同时引入反射去除模块和伪深度图，以及几何感知的双边平滑约束，以实现传输和反射成分的显式分离和稳定的几何一致性。

Result: 实验表明，Ref-Unlock在反射建模上显著优于传统GS方法，并在与NeRF模型比较中表现竞争力，同时支持灵活的视觉基础模型进行反射编辑。

Conclusion: Ref-Unlock 提供了一种高效且通用的解决方案，能实现真实感的反射场景渲染，并促进了反射处理和视图合成领域的发展。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [73] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: 该论文提出了Omni-Video，一个统一的视频理解、生成及编辑框架，通过将多模态大语言模型与扩散解码器相结合，实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要专注于图像处理，缺乏统一的视频理解和生成模型，推动视频多任务统一建模需求。

Method: 设计轻量级架构，给多模态大语言模型顶部添加视觉头，扩散解码器输入前加适配器；采用多阶段训练，提升模型连接效率。

Result: 模型在视频生成、编辑及理解任务中表现出良好的泛化能力，能高效生成高质量视频。

Conclusion: 通过结合多模态语言模型和扩散解码器，Omni-Video实现了统一且高效的视频多任务处理，弥补了视频领域基础模型的不足。

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [74] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种无提示条件扩散框架，用于多物体图像增强，结合局部-全局语义融合和计数损失，提高了生成图像的多样性和类别一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多物体图像生成方法依赖文本导致类别偏差，或依赖原图导致多样性不足，限制了下游任务的效果。

Method: 引入局部-全局语义融合替代文本条件，采用LoRA注入知识缓解类别偏差，设计基于计数的奖励模型损失辅助训练以约束类别数量，提升生成图像多样性和数量一致性。

Result: 实验结果表明该方法优于多种先进基线，在下游任务性能和跨域泛化能力上表现突出。

Conclusion: 该框架有效解决了多物体生成中的类别偏差与多样性不足问题，提高了图像增强的实用性和广泛适应性。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [75] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 提出了一种名为SoftReMish的新激活函数，在MNIST数据集上的标准CNN架构中对比了ReLU、Tanh和Mish，结果显示SoftReMish在训练损失和验证准确率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了提升卷积神经网络在图像分类任务中的性能，设计一种新的激活函数。

Method: 在标准CNN架构中将所有可训练层的激活函数替换为SoftReMish，并与ReLU、Tanh、Mish进行对比实验，使用MNIST数据集评价模型表现。

Result: SoftReMish实现了最低训练损失（3.14e-8）和最高验证准确率（99.41%），优于其他激活函数。

Conclusion: SoftReMish在收敛速度和泛化能力方面表现优异，是视觉识别任务中有潜力的新型激活函数。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [76] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: 本文提出了一种基于相似度矩阵归一化的新型光滑算子，用于在无结构域上实现类拉普拉斯热扩散的信号平滑。


<details>
  <summary>Details</summary>
Motivation: 在无明确定义结构的非规则数据域（如点云、稀疏体素网格）上，传统拉普拉斯算子难以构建，现有方法对边界存在偏差。

Method: 利用对称化的Sinkhorn算法对一般相似或邻接矩阵进行归一化，构造出继承拉普拉斯性质的扩散类算子。

Result: 新算子不仅逼近热扩散过程，还保留了拉普拉斯算子的谱特征，在形状分析和匹配等应用中表现良好。

Conclusion: 该方法拓展了热扩散光滑在无结构数据上的应用，有助于提升非欧几里得数据处理的理论保障和效果。

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [77] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: OmniPart是一种新的3D对象生成框架，专注于生成可编辑的部件结构，实现了语义上高度独立且结构上紧密的组件分离。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法通常生成单一整体形状，缺乏清晰且可编辑的部件结构，限制了其在交互应用中的实用性。

Method: OmniPart将任务分为两个阶段：首先，通过自回归结构规划模块生成可控的3D部件边界盒序列，利用灵活的2D部件掩码实现对部件分解的直观控制，无需语义标签；其次，使用基于空间条件的修正流模型，同时合成所有3D部件，确保整体结构协调。

Result: 实验结果表明，OmniPart在生成可解释、可编辑且多样化的3D内容方面达到最先进水平。

Conclusion: 该方法成功实现了用户定义的部件粒度和精准定位，推动了更具解释性和可编辑性的3D资产创建，拓宽了其应用前景。

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [78] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: 该论文针对科学图表的视觉问答任务，提出了结合提示优化、连锁推理和模型集成的方法，实现了较高的性能指标。


<details>
  <summary>Details</summary>
Motivation: 科学技术报告中大量重要信息以半结构化图表形式存在，传统视觉问答方法难以精准理解科学数据中的数值和多步推理需求。

Method: 使用参数规模为5B至8B的视觉语言模型，结合提示优化、连锁推理（chain-of-thought）以及多模型集成策略，提升对科学图表问答的表现。

Result: 单模型InternVL3在SciVQA测试集上达到了ROUGE-1和ROUGE-L F1分别为0.740和0.983，集成模型表现优于大多数单模型，但InternVL3仍是最优单模型。

Conclusion: 通过提示优化、连锁推理和模型集成，可以有效提升视觉问答模型在科学图表问题上的表现，尤其是对数值推理和多步推理的处理能力。

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [79] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督的语义场景完成方法SceneDINO，基于多视图一致性的自监督学习，从单张图像推断3D几何和语义特征。


<details>
  <summary>Details</summary>
Motivation: 现有的语义场景完成方法严重依赖昂贵的带注释数据，急需一种无需语义和几何标注的无监督方法。

Method: 引入SceneDINO，结合自监督表示学习与2D无监督场景理解，利用多视图一致性自监督训练，采用新颖的3D特征蒸馏获得无监督3D语义。

Result: SceneDINO在3D和2D无监督场景理解任务中达到最新的分割精度，线性探测其3D特征的分割准确率与现有监督方法相当。

Conclusion: SceneDINO展示了出色的领域泛化和多视图一致性，为单张图像的3D场景理解奠定了坚实基础。

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [80] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出了RSRefSeg 2，一种基于视觉-语言协同的遥感图像细化分割框架，采用双阶段的粗定位和细分割策略，提升了复杂语义关系的解析能力和分割精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言的遥感图像分割方法因目标定位与边界细化耦合处理，导致语义模糊时误差传播大，泛化能力和解释性不足。

Method: 设计RSRefSeg 2，通过CLIP进行目标粗定位，利用级联二阶提示器分解文本语义，再用SAM生成像素级精细掩膜，实现任务解耦和跨模态协作。

Result: 实验证明RSRefSeg 2在多个数据集上的分割精度提升约3%（gIoU指标），且在复杂语义解析任务中表现优于现有方法。

Conclusion: RSRefSeg 2有效解决了遥感图像语义模糊导致的误差传播问题，增强了模型的泛化性和解释性，为视觉语言遥感图像分割提供了新思路。

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [81] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种利用SMPL模型自动生成伪标签数据的点跟踪训练管道AnthroTAP，实现了高效且准确的人体点跟踪。


<details>
  <summary>Details</summary>
Motivation: 人类运动复杂且具有丰富的监督信息，但人工标注训练数据耗时费力，难以获取大量数据。

Method: 通过SMPL模型拟合视频中的人体，将3D网格顶点投影到2D图像平面生成伪轨迹，采用射线投射处理遮挡，利用光流一致性过滤不可靠轨迹，从而自动生成带标签的训练数据。

Result: 基于AnthroTAP数据集训练的点跟踪模型在TAP-Vid基准测试中达到最先进性能，使用的数据量比真实视频少10000倍，且训练资源远低于最新方法。

Conclusion: AnthroTAP有效减少了训练数据和计算资源需求，显著提升了人体点跟踪的性能和实用性。

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [82] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 提出了TokenShapley，一种结合Shapley值和KNN检索的新颖token级数据归因方法，有效提升了大语言模型生成内容中特定关键词的归因准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的句子级归因方法无法满足用户对生成结果中特定关键词（如数字、年份、姓名）归因的需求。

Method: 提出结合Shapley值数据归因与KNN辅助检索的TokenShapley方法，通过预计算的数据存储和上下文检索，细粒度评估token的重要性。

Result: 在四个基准测试中，TokenShapley在token级归因准确率上较现有最优方法提升了11%至23%。

Conclusion: TokenShapley有效提升了对大语言模型生成内容中关键token的归因能力，促进生成内容的可验证性。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [83] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文提出用用户行为预测替代知识检索和推理任务来衡量大语言模型的泛化能力，并在推荐系统数据集上验证了该方法，结果显示GPT-4o优于其他模型但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 由于数据污染，传统的知识检索和推理任务不适合评估大语言模型的泛化能力，因此需要一种理论上合理且可扩展的替代方法。

Method: 提出基于用户行为预测的框架，并在电影和音乐推荐数据集上对GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct进行了测试。

Result: 测试结果与框架预测一致，GPT-4o表现优于GPT-4o-mini和Llama，所有模型均有较大提升空间，尤其是Llama。

Conclusion: 用户行为预测是衡量大语言模型泛化能力的有效替代方案，能够更好地反映模型在个性化场景下的性能。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [84] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出了ASCEND方法，通过自适应阈值对比学习显著提升隐性性别歧视检测效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上隐性性别歧视内容难以被传统检测方法识别，需要更有效的检测框架。

Method: 引入基于阈值的对比学习，通过学习的相似度阈值选择正样本对，结合词级注意力和情感、情绪、毒性特征进行联合优化。

Result: 在EXIST2021和MLSC数据集上，ASCEND在多个任务中Macro F1值平均提升9.86%、29.63%和32.51%。

Conclusion: ASCEND方法有效捕捉隐性性别歧视的细微语义差异，显著优于现有检测技术。

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [85] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 本文提出了一种结合情感分析和多模态信息融合的AI框架，有效预测远程学习中的学生辍学风险，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型难以捕捉学生互动中复杂的情感与上下文因素，导致远程学习辍学问题难以有效预测和干预。

Method: 采用基于检索增强生成(RAG)的BERT模型进行领域特定情感分析，通过提示工程识别学业压力指标，利用跨模态注意力融合文本、行为和社会人口统计数据，构建全面的风险画像。

Result: 在包含4423名学生的纵向数据集上，该框架实现了89%的准确率和0.88的F1分数，较传统模型提升了7%，假阴性率降低21%。

Conclusion: 该系统不仅提升了辍学风险预测精度，还能生成针对性干预措施，推动预测分析与教育实践的融合，为全球教育系统降低辍学率提供了可扩展方案。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [86] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: 本文提出了LCDS系统，通过逻辑控制和来源映射解决大语言模型生成出院小结时的幻觉和溯源难题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成出院小结时存在内容不准确和虚构信息的问题，且电子病历数据冗长，难以准确溯源。

Method: 通过计算电子病历与出院小结的文本相似度构建来源映射表，利用逻辑规则约束生成内容范围，支持内容来源归因，并记录专家修正以用于模型增量微调。

Result: LCDS生成了更可靠的出院小结，可针对不同临床领域定制，支持专家高效审核和反馈。

Conclusion: LCDS有效缓解了大语言模型在出院小结生成中的幻觉问题和溯源难题，提升了生成内容的准确性和可追溯性，有助于临床应用。

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [87] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: MindFlow是首个面向电商的多模态大语言模型代理，基于CoALA框架，整合记忆、决策和动作模块，提高了复杂查询处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂多模态电商场景下能力有限，难以满足客户服务需求。

Method: 基于CoALA框架构建，采用模块化“MLLM-as-Tool”策略，结合记忆、决策和动作模块，实现视觉-文本推理。

Result: 通过在线AB测试和仿真消融实验，MindFlow在处理复杂查询、提升用户满意度及降低运营成本方面表现优异，实际部署中相对提升达93.53%。

Conclusion: MindFlow显著增强了电商多模态客户服务的能力，提升了服务效率和用户体验，具备广泛应用前景。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [88] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoRA-Augmented Generation (LAG)的方法，用于高效选择和组合大量任务特定的LoRA适配器，在无需额外训练或数据访问下提升知识密集型任务的性能。


<details>
  <summary>Details</summary>
Motivation: 面对大量针对特定任务和领域微调的语言模型专家，迫切需要高效的选择和组合方法以更好地利用这些模型。

Method: 提出LAG方法，通过在每个token和层级上筛选、检索并应用专家模型，且无须额外训练或访问数据，实现模型的高效组合。

Result: 在多个知识密集型任务上，LAG的性能优于现有无数据方法，并且在有额外数据时能与如检索增强生成（RAG）等方法兼容。

Conclusion: LAG是一种高效、灵活的专家模型组合方法，能提升任务性能且无需额外训练，适用于多种知识密集型应用场景。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [89] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型中推理流程的策略，发现训练中使用较长且结构合理的推理路径比使用最优路径更有助于模型泛化。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型解决复杂问题时的推理能力，探索计算资源分配与推理路径结构对性能的影响。

Method: 构建基于分层图最短路径任务的控制实验，训练解码器型Transformer模型，比较训练于最优动态规划路径与更长且含回溯的有效路径的效果。

Result: 在相同训练资源下，训练于较长、系统性且局部递增的推理路径的模型，在未见图结构上的泛化能力更强。简单增加冗余路径不能提升效果。

Conclusion: 有效的训练推理路径应保证长度、连贯性和递增性，这有助于提升模型的信心和优化效率，从而促进泛化能力。

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [90] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

TL;DR: EduCoder是一款专门用于教育对话逐句标注的工具，支持复杂教学特征的编码及多标注者比较，提升数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有通用文本标注工具难以满足教育对话中多样师生互动及复杂教学特征的标注需求。

Method: EduCoder支持复杂代码本定义、分类与开放式标注、上下文材料支持以及多标注者并列比较，便于研究者和领域专家协作标注。

Result: 实现了一个开源平台，能够提高教育对话标注的准确性和一致性，支持协同标注和数据校准。

Conclusion: EduCoder有效解决了教育对话标注中的复杂挑战，促进了教育领域对话数据的高质量标注和研究。

Abstract: We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [91] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

TL;DR: 本文提出InfoRidge框架，揭示Transformer模型中间层在信息流动和泛化中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在自然语言生成任务中表现优异，但其层间的任务相关信息传播机制尚不清楚。

Method: 通过信息论方法估计隐藏表示与目标输出间的互信息，并引入可训练的残差缩放系数，分析信息在各层的变化及权重。

Result: 发现预测信息在中上层形成峰值，表现非单调趋势，且分布迁移时模型更依赖中间层，显示其泛化能力。

Conclusion: 中间层在Transformer中起关键的泛化作用，揭示其重要性，有助于理解模型内部机制及提升模型鲁棒性。

Abstract: Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [92] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: 本文提出通过隐私配置文件控制大语言模型查询隐私，使用本地模型改写查询以保护敏感信息，在公开模型和用户隐私间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过商业API访问时存在用户数据暴露风险，用户希望对数据隐私有更多控制。

Method: 设计本地模型利用自然语言隐私配置文件改写用户查询，仅隐藏用户定义的敏感信息，然后发送给外部模型；构建多语言PEEP数据集，包含带隐私标注的真实查询和合成隐私配置文件。

Result: 轻量级大语言模型能够部分遵守隐私指令，但仍面临挑战，表明需要更好理解和执行用户隐私偏好的模型。

Conclusion: 采用隐私配置文件进行查询改写是平衡隐私和性能的有效途径，但现有模型能力有限，未来需改进隐私指令的理解与执行能力。

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [93] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: 本文提出GeoFact-X多语言地理事实推理基准和BRIDGE训练方法，提高大语言模型多语言推理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在低资源语言中的推理能力较弱，存在偏向高资源语言的问题，影响准确性和可解释性。

Method: 设计包含五种语言注释推理轨迹的GeoFact-X基准，提出BRIDGE训练方法结合有监督微调和带语言一致性奖励的强化学习，并开发自动评测协议。

Result: BRIDGE显著提升了多语言推理的语言一致性和推理质量，实现更强的跨语言泛化能力。

Conclusion: 基于推理意识的多语言强化学习对增强大语言模型跨语言推理鲁棒性至关重要。

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [94] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出了CoPE框架，系统衡量大型语言模型在多语言环境中对上下文知识和参数知识的利用，发现模型存在对后文信息忽视的偏差。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何优先及整合上下文知识与参数知识，填补该领域研究空白。

Method: 构建MultiWikiAtomic多语言数据集，通过CoPE框架评估模型在开放式问答中的知识整合和信息利用，分析模型的位置信息偏差及推理能力的作用。

Result: 发现模型存在“lost-in-the-later”现象，推理模型及使用链式思考提示的模型反而更少利用上下文，且CoT提示降低信息召回率和回答长度。设计基于提示的方法提升上下文利用效率。

Conclusion: CK知识引导的提示方法可增强模型的事实性基础，减少幻觉现象，表明合理利用上下文知识对提升模型效果有显著帮助。

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [95] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

TL;DR: 美国最高法院在Dobbs案件中的裁决引发了围绕堕胎权利的分歧，研究通过分析约1千万条与堕胎相关的X平台帖子，揭示了性别和地区政治环境如何交互影响公众言论。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨性别和地方社会政治背景如何共同影响公众对堕胎的态度及情绪表达，填补性别和地区互动影响方面的研究空白。

Method: 基于X平台近一千万条与堕胎相关的帖子，结合用户推测的性别、意识形态和地理位置，对其言论态度及情绪表达进行分析。

Result: 发现性别显著调节堕胎态度和情绪表达，尤其在保守地区更为显著，形成了加剧的性别差距；Dobbs草案的泄露加剧了网络上的参与度，特别是激发了保守地区支持堕胎的女性的动员。

Conclusion: 堕胎话语不仅意识形态对立明显，还受到性别和地域因素深刻影响，强调身份认同在制度变革时期政治表达中的核心作用。

Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [96] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: 本文提出了一种名为PhoniTale的跨语言助记词生成系统，通过基于语音相似性的L1关键词检索结合大语言模型生成助记词，有效帮助第二语言学习者词汇习得。


<details>
  <summary>Details</summary>
Motivation: 词汇习得对学习 typologically不同语言的第二语言学习者具有挑战性，尤其是英语和韩语之间存在语音和结构差异，影响词汇学习效果。现有研究多集中在以英语为母语学习其他语言的情形，缺乏反向研究。

Method: 设计了PhoniTale系统，通过检索与目标单词语音相似的母语（L1）关键词序列，并利用大语言模型生成助记词。系统输出通过自动指标和人工评估与真人及现有自动化方法生成的助记词进行比较。

Result: 实验和短期记忆测试显示，PhoniTale生成的助记词在质量上与人类原创助记词相当，证明了其有效性。

Conclusion: PhoniTale在帮助跨语言词汇习得方面表现出色，但仍需在助记词质量和生成方法上进一步改进。研究为跨语言助记策略提供了新思路。

Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [97] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: 本文探讨大语言模型（LLMs）如ChatGPT在词汇和句子层面的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前对于LLMs是否真正理解语言存在争议，本文意在通过细化至语义层面来探究这一问题。

Method: 通过分析LLMs的内部机制及其语言表示，并结合Frege和Russell的经典语义理论，进行深入研究。

Result: 获得了LLMs在语义能力方面更细致的理解，展示了其潜在的语义处理能力。

Conclusion: LLMs在语义理解上具有一定潜力，但仍需更精准的评估和理论支持，才能全面评价其语义能力。

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [98] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: 本论文提出了MODELCITIZENS数据集，包含多元身份群体的毒性语言标注，并引入对话情境以提升毒性检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有毒性语言检测模型忽视了不同社区规范和语境对毒性的多样化认知，且通常将多样注释合并为单一真值，导致重要的情境特异性毒性概念被忽略。

Method: 构建包含6.8K社交媒体帖子及4万条标注的MODELCITIZENS数据集，涵盖多身份群体视角，利用大语言模型生成对话情境以增强数据，基于该数据集微调LLaMA和Gemma模型，提升检测效果。

Result: 现有主流毒性检测工具如OpenAI Moderation API和GPT-4-mini在MODELCITIZENS数据集及其对话情境增强版上表现较差，而微调后的LLAMACITIZEN-8B和GEMMACITIZEN-12B模型在样本内评测中性能提升了5.5%。

Conclusion: 社区视角驱动的注释和建模对于构建包容性的内容审核系统至关重要，能够有效提升毒性语言检测的准确性和适应性。

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [99] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文针对护士口述报告的结构化表格生成和医生-患者会诊中的医疗指令提取这两个临床NLP高难任务，评估了多种大型语言模型的表现，并提出了生成真实非敏感护士口述的自主管道，同时发布了首个公开的数据集以支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 这两个任务由于数据稀缺和敏感性，研究较少，但其解决方案能够显著减轻医疗工作者的文档负担，提高患者护理质量。

Method: 利用私有和开源临床数据集，评估开放权重和闭源大型语言模型在这两任务中的表现，分析优缺点；提出基于智能代理的生成管道，以生成真实且非敏感的护士口述，实现临床观察的结构化提取。

Result: 评测展示了不同LLM在这两任务中的性能与局限性；成功开发了生成管道用于生成非敏感护士口述；同时发布了首个护士观测提取和医疗指令提取的开放数据集SYNUR和SIMORD。

Conclusion: 通过多模型评估和新技术方法，本研究推动了临床NLP中两个未充分探索的重要任务的研究，为实际医疗应用减负提供了重要资源与技术基础。

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [100] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

TL;DR: 本文提出了R2-LLMs，一种无需蒸馏链式思维训练数据的新型分层检索增强推理框架，通过双层检索结合蒙特卡洛树搜索提升大语言模型的推理性能，在多个复杂推理数据集上效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在测试时扩展推理能力依赖复杂的链式思维训练数据，且性能提升受限，急需一种无须蒸馏更高级模型数据且能增强推理能力的新方法。

Method: R2-LLMs采用双层检索策略：粗层级提取抽象模板并检索相似题目与答案促进高级上下文学习，细层级在蒙特卡洛树搜索中检索相似中间求解步骤，辅以过程奖励模型优化候选生成与决策。

Result: 在MATH500、GSM8K及OlympiadBench-TO数据集上，基于LLaMA-3.1-8B模型，R2-LLMs相比基线方法最高提升16%的准确率，证明其在复杂推理任务中的有效性。

Conclusion: R2-LLMs为提升大语言模型测试时推理能力提供了有效且灵活的分层检索增强方案，显著改善了上下文推理及步进决策过程，推动了复杂推理任务的性能提升。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [101] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: 本文提出了Re5，一种自我评估与修订框架，旨在提升大语言模型的指令遵循能力，同时保持生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用强大模型生成高质量数据，但复杂指令下生成效果有限，且多次修订成本高昂；利用开源模型自评能力有限，过度修订又影响质量。

Method: Re5通过提取任务及约束，进行结构化评估防止错误累积，再基于细粒度约束做内容评估和选择性修订，最后用高质量输出进行调优，实现高效的迭代优化。

Result: 实验显示，Re5在少量数据下，指令遵循表现可与基于GPT-4o-mini生成数据训练的模型媲美，同时保持回答质量，修订后回答胜率达64.24%。

Conclusion: Re5是一种资源高效且有效的方案，能在最小外部监督下显著提升大语言模型的指令遵循能力及输出质量。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [102] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一种倒置知识蒸馏范式，即让大型语言模型从小型模型学习，通过LoRA实现编码器-解码器架构，采用边缘感知对比学习提升文本匹配任务的性能，在金融和医疗领域验证有效并已在线部署。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏是由大型模型向小型模型迁移知识，但在文本匹配任务中，经过微调的小型模型能够更好地捕捉领域特定的语义信息。为了结合两者优势，提出让大型模型向小型模型学习的新方法。

Method: 采用LoRA将解码器结构的大型模型转化为编码器-解码器架构，由编码器生成压缩表示，解码器映射输出。训练时通过编码器产生的表示及其相似度，与教师模型的小型模型输出相似度进行对齐，设计了边缘感知对比学习（MCL）方法，准确处理正负样本的相似度信息。

Result: 新范式使大型模型能够利用表现良好的小型模型知识提升性能。通过金融和医疗领域的多个基准测试及实际应用验证了该方法的有效性，模型已成功部署上线。

Conclusion: 倒置知识蒸馏结合了小型模型的领域优势和大型模型的深层语义理解，有效提升文本匹配等任务性能，具有良好的实用价值和推广前景。

Abstract: Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [103] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: SARA框架通过结合自然语言片段和语义压缩向量，在有限上下文预算下提升检索增强生成模型的答案准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成模型面临上下文长度限制和冗余信息问题，纯压缩方法又会丢失重要细节，影响事实准确性。

Method: SARA采用双层表示策略，将细粒度文本与压缩向量结合，通过迭代证据选择模块动态重排序上下文，提升信息利用效率。

Result: 在9个数据集和5个开源大模型上，SARA显著提高答案相关性（+17.71）、答案正确性（+13.72）和语义相似度（+15.53）。

Conclusion: 结合文本细节和压缩表达的融合方法能够实现更稳健、更高效的检索增强生成。

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [104] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文提出了ECom-Bench，一个用于评估具有多模态能力的电商客服大语言模型代理的基准框架。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对电商客服场景中多模态大语言模型代理的评测基准，难以衡量其实际应用效果。

Method: 构建基于真实电商客服交互的用户画像动态模拟和真实电商对话任务数据集，涵盖多种复杂业务场景，设计具有高度挑战性的任务。

Result: 在该基准上，即使是GPT-4o等先进模型的通过率仅为10-20%，说明复杂电商场景的巨大挑战。

Conclusion: ECom-Bench为多模态电商客服大语言模型的评测提供了首个实际且高度挑战性的基准，对推动该领域研究具有重要价值。代码和数据将开源以促进后续研究。

Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [105] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出了Smoothie-Qwen，一种轻量级后处理方法，通过调整token输出概率减少多语言大模型的语言混淆问题，显著降低了不期望语言输出，提高模型语言可控性。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型在生成响应时容易出现语言混淆，即无视提示语言而生成支配语言，影响模型的多语言适用性和准确性。

Method: Smoothie-Qwen方法通过选择性调整token级别的输出概率，抑制不期望的语言生成，无需重新训练模型，简单高效。

Result: 应用于Qwen模型后，不期望的中文输出降低了95%以上，同时多语言基准任务的准确性得以保持。

Conclusion: Smoothie-Qwen为提升多语言大模型的语言可控性提供了实用、高效的解决方案，增强了模型在全球应用中的可靠性。

Abstract: Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [106] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: 本文提出了DualDistill微调框架，通过多教师模型蒸馏互补推理策略，训练出了Agentic-R1模型，能够针对不同查询动态选择推理策略，提高数学和逻辑问题的解题准确率。


<details>
  <summary>Details</summary>
Motivation: 现有长链推理模型虽然在数学推理方面表现优异，但依赖缓慢且易出错的自然语言线索。工具增强型代理虽然能通过代码执行解决算术问题，但在复杂逻辑任务上表现不佳。

Method: 提出DualDistill微调框架，将多位教师模型的互补推理策略蒸馏到一个统一的学生模型中。通过训练Agentic-R1模型，实现根据查询动态选取最优推理策略，算术和算法问题调用工具，抽象问题采用基于文本的推理。

Result: 该方法提升了多个任务的准确率，涵盖计算密集型和标准基准测试，验证了多策略蒸馏在实现稳健高效推理中的有效性。

Conclusion: 多策略蒸馏通过整合不同推理方法，可显著提高模型解决数学与逻辑复杂问题的能力，使推理更高效和准确。

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [107] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文提出了DRAGON，一个针对俄语的动态Retrieval-Augmented Generation（RAG）基准，基于不断更新的新闻语料库，支持全面评估检索和生成模块，且自动生成多类型问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准多为英文，其他语言如俄语的资源稀缺且静态，无法反映真实环境中的动态变化需求。

Method: 构建动态更新的俄语新闻及公开文档语料库，基于知识图谱自动生成与子图模式对应的四类核心问题，提供自动问题生成及评测脚本。

Result: 发布了完整评测框架和数据，支持检索器与生成器的综合评测，适用于多语言设置，并创建了公开排行榜促进社区交流。

Conclusion: DRAGON填补了俄语RAG动态评测资源的空白，有助于推动多语言RAG系统的实用性和准确性提升。

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [108] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种针对检索增强生成模型（RAG）的层次思维指令微调方法（HIRAG），通过多级渐进链式思维提升模型处理复杂知识检索和推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型在面对实时信息和领域问题时，存在文档质量不一和检索系统缺陷，且缺乏对模型具体能力的深入研究及链式思维的充分利用。

Method: 提出RAG模型应具备过滤、组合和特定推理三层次能力，引入多级渐进链式思维的"先思考再回答"策略，设计HIRAG指令微调方法以提升模型能力。

Result: HIRAG策略显著提升了模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的表现。

Conclusion: 层次思维指令微调有效增强了RAG模型的开放书本考试能力，改善了信息选择、语义合成和基于内外知识的推理能力。

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [109] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: 本文提出了Omni-router Transformer，通过在不同MoE层共享路由器，提升了语音识别模型中专家间的协作和专业化，实现了更低的训练损失和更优的识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法中各层路由器独立选择专家，导致不同层间专家选择相关性弱，限制了专家间的协作和专业化。

Method: 设计了一个跨层共享路由器的Omni-router Transformer模型，增强了不同MoE层间专家的合作。

Result: 在大规模伪标签数据集和10个多样化领域外语音识别测试中，Omni-router Transformer较稠密模型和Switch Transformer分别降低了11.2%和8.2%的词错误率，训练损失更低且专家使用更有结构性。

Conclusion: 通过共享路由器机制，Omni-router Transformer实现了专家间更好合作和专业化，提升了自动语音识别的性能和鲁棒性。

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [110] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
*Yujia Hu,Tuan-Phong Nguyen,Shrestha Ghosh,Moritz Müller,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文介绍了GPTKB v1.5，一个由GPT-4.1构建的包含1亿三元组的知识库，实现对大规模语言模型知识的系统化材料化和分析。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的事实知识尚不清楚，且难以进行即席浏览和大规模统计分析，因此需要一个高效的知识库来系统地展现和分析LLM知识。

Method: 利用GPTKB方法和大量递归技术，从GPT-4.1生成1亿条三元组，构建一个高度互联的知识库。

Result: 实现了三大应用场景：基于链接遍历的知识探索、基于SPARQL的结构化查询、以及对LLM知识优劣对比的探索，证明了知识材料化的有效性。

Conclusion: 大规模递归语言模型知识材料化为LLM知识的系统性分析和自动知识库构建提供了突破性机遇，GPTKB工具已上线开放访问。

Abstract: Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

</details>


### [111] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
*Jing Yang Lee,Hamed Bonab,Nasser Zalmout,Ming Zeng,Sanket Lokegaonkar,Colin Lockard,Binxuan Huang,Ritesh Sarkhel,Haodong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种通过将多篇相关文档合成为多轮多主题对话数据的预训练方法，以提升大语言模型在多轮会话任务中的上下文记忆与理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的预训练数据多为连贯文本，缺乏多轮对话结构，导致模型在多轮会话中的表现受限。

Method: 设计一个管道，将多个相关文档聚合转化为信息检索式的多轮多主题对话，基于维基百科文章构建了包含73万轮对话的DocTalk数据集，用于预训练。

Result: 利用DocTalk进行预训练，可在不影响基础性能的前提下，在上下文记忆和理解能力上提升多达40%。

Conclusion: 合成的多轮对话预训练数据有助于增强大语言模型的多轮会话能力，DocTalk数据集公开可用。

Abstract: Large Language Models (LLMs) are increasingly employed in multi-turn
conversational tasks, yet their pre-training data predominantly consists of
continuous prose, creating a potential mismatch between required capabilities
and training paradigms. We introduce a novel approach to address this
discrepancy by synthesizing conversational data from existing text corpora. We
present a pipeline that transforms a cluster of multiple related documents into
an extended multi-turn, multi-topic information-seeking dialogue. Applying our
pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training
dialogue corpus consisting of over 730k long conversations. We hypothesize that
exposure to such synthesized conversational structures during pre-training can
enhance the fundamental multi-turn capabilities of LLMs, such as context memory
and understanding. Empirically, we show that incorporating DocTalk during
pre-training results in up to 40% gain in context memory and understanding,
without compromising base performance. DocTalk is available at
https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [112] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
*Anand A. Rajasekar,Praveen Tangarajan,Anjali Nainani,Amogh Batwal,Vinay Rao Dandin,Anusua Trivedi,Ozan Ersoy*

Main category: cs.CL

TL;DR: Flippi是一个基于大语言模型的端到端电商对话助手，通过自然语言交互提升用户产品发现和购物体验。


<details>
  <summary>Details</summary>
Motivation: 面对庞大复杂的电商产品，用户难以高效找到合适商品，需个性化、智能的购物助手。

Method: 利用查询重构、意图识别、检索增强生成、命名实体识别和上下文缩减等先进NLP技术，实现精准商品信息获取和个性化推荐。

Result: Flippi能识别并展示最优优惠，支持产品特征和价格的对比分析，提高用户决策效率和满意度。

Conclusion: Flippi优化了电商购物体验，提升用户参与度和转化率，具备跨平台集成能力，代表数字市场客户服务新标准。

Abstract: The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

</details>


### [113] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
*Amane Watahiki,Tomoki Doi,Taiga Shinozaki,Satoshi Nishida,Takuya Niikawa,Katsunori Miyahara,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文构建了一个基于基本形式本体的多模态完成基准，评估大型视觉-语言模型（LVLMs）对无形完成文本推理能力，发现不同模型在某些物体类型上的表现差异显著，且在日语提示下部分模型表现异常。


<details>
  <summary>Details</summary>
Motivation: 旨在填补LVLMs在无形完成（amodal completion）文本推理能力方面的研究空白，评估其是否能够理解和推断被遮挡物体的信息。

Method: 构建一个基于基本形式本体的无形完成系统分类基准，使用该基准系统评估多个LVLM模型在处理相关文本和视觉信息的表现。

Result: 发现多数LVLM整体表现可与人类相媲美，但在某些物体类型的无形完成任务中表现不一。尤其是LLaVA-NeXT部分变体和Claude 3.5 Sonnet在日语提示下对原始图像的准确度低于空白无视觉内容的刺激。

Conclusion: LVLM具备一定的无形完成推理能力，但存在语言特异性缺陷，特别是在日语处理能力方面表现不足，提示模型多语言能力亟需加强。

Abstract: One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.

</details>


### [114] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
*Tanvina Patel,Wiebke Hutiri,Aaron Yi Ding,Odette Scharenborg*

Main category: cs.CL

TL;DR: 本文研究了自动语音识别（ASR）系统中存在的针对不同说话人群体（如性别、年龄、口音）的偏见问题，比较了不同性能和偏见测量方法，提出了评估和缓解策略，并给出更全面的性能报告建议。


<details>
  <summary>Details</summary>
Motivation: ASR系统存在针对不同说话人群体的偏见，现有研究主要集中在检测和减缓偏见，但如何有效衡量系统性能和偏见仍是未解问题。

Method: 本文比较了多种现有及新提出的性能和偏见衡量指标，利用多种偏见缓解策略，评估荷兰语端到端ASR系统。

Result: 实验结果表明，传统的平均错误率指标不足以全面反映系统性能，需辅以其他偏见衡量指标。

Conclusion: 建议在报告ASR系统性能和偏见时，应结合多种指标，以更全面体现系统在不同说话人群体上的表现和整体偏见水平。

Abstract: There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

</details>


### [115] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一个利用大型语言模型（LLMs）模拟虚拟受访者，以生成和验证心理测量调查题目有效性的框架。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLMs进行心理测量调查时，确保生成题目真正测量预期特质（构念效度）需大量昂贵的人类数据采集，效率低下。

Method: 通过模拟具有不同中介变量的虚拟受访者，利用LLMs从特质定义生成合理的中介变量，并模拟受访者行为，识别稳健测量预期特质的调查题目。

Result: 在Big5、Schwartz和VIA三种心理特质量表上的实验表明，该框架和中介变量生成方法能有效鉴别高效度题目。LLMs展现了生成合适中介变量及模拟受访者行为能力。

Conclusion: 该方法为成本效益高的测验题目开发和理解LLMs模拟人类行为提供了新方向，并将公开数据集和代码支持后续研究。

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [116] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
*Teodor-George Marchitan,Claudiu Creanga,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 本文介绍了Unibuc - NLP团队在2025年SemEval任务11中使用大型语言模型进行文本情感检测的实验，最终在多标签情感检测中取得不同语言子集的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决文本情感检测中的跨语言难题，提升不同语言子集上的情感检测效果。

Method: 采用大型语言模型（Gemini, Qwen, DeepSeek），结合少样本提示和微调方法进行多标签情感检测。

Result: 在多标签情感检测轨道A中，英文子集F1-macro为0.7546（排名26/96），葡萄牙语子集为0.1727（排名35/36），Emakhuwa语子集为0.325（排名第一）。

Conclusion: 通过大型语言模型及相应训练策略，团队在复杂多语言文本情感检测任务中取得了阶段性成果，特别是在低资源语言Emakhuwa上表现突出。

Abstract: This paper describes the approach of the Unibuc - NLP team in tackling the
SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion
Detection. We mainly focused on experiments using large language models
(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With
our final system, for the multi-label emotion detection track (track A), we got
an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36
teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams)
for the Emakhuwa subset.

</details>


### [117] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
*Sebastian Pohl,Max Ploner,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了知识编辑模型评估方法的鲁棒性和公平性，发现不同评估指标和方法会导致模型排名变化，并指出字符串匹配评估存在误判问题。


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑领域缺乏对评估方法的鲁棒性研究，且现有评估方法可能对部分编辑器存在偏向，同时对编辑对模型整体能力影响了解不足。

Method: 通过比较不同评估指标、方法及编辑批量大小对知识编辑器排名的影响，结合手工评估字符串匹配方法的准确性，分析评估方法的偏差。

Result: 发现不同评估策略会显著影响知识编辑器排名，同时字符串匹配评估方法易产生误判，且编辑对通用语言理解任务的影响也被揭示。

Conclusion: 评价方法选择和参数设定对模型编辑结果排名有显著影响，现有字符串匹配评估存在误判问题，未来需要设计更鲁棒和公平的评估体系。

Abstract: Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

</details>


### [118] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的持续多模态错误信息检测方法DAEDCMD，以应对在线数据流中持续涌现的新事件，解决遗忘旧知识和适应未来环境变化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态错误信息检测方法依赖离线数据训练，无法适应不断出现的新事件，导致模型过时且效果下降。

Method: 通过采用基于狄利克雷过程的专家混合结构隔离事件特定参数，防止旧知识遗忘；并利用连续时间动力学模型预测环境分布变化，提高对未来数据的泛化能力。

Result: 实验表明，DAEDCMD在持续检测多模态错误信息任务中，显著优于包括六个基准MMD方法和三个持续学习方法在内的对比方法。

Conclusion: DAEDCMD有效解决了持续多模态错误信息检测中旧知识遗忘和环境变化适应问题，提升了模型的持续学习能力和泛化性能。

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


### [119] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
*Sandeep Mishra,Anubhab Mandal,Bishal Santra,Tushar Abhishek,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文研究了聊天场景下的文本自动补全（Ghosting）问题，评估了多种方法的性能，并提出了动态早停策略。


<details>
  <summary>Details</summary>
Motivation: 随着聊天系统（如ChatGPT）的普及，预测用户意图输入（Ghosting）变得愈发重要，但该问题尚缺乏统一基准与性能分析。

Method: 利用四个公开对话数据集，比较了trie、n-gram统计模型与包括T5、Phi-2在内的深度学习模型的效果，同时提出基于熵的动态早停策略。

Result: n-gram模型和trie在已见前缀上表现和推理效率优于深度模型，神经网络模型在未见查询上表现更佳，加入对话上下文显著提升性能。

Conclusion: 统计方法与深度学习方法各有优势，结合对话上下文可提升自动补全质量，研究代码和数据公开促进未来工作。

Abstract: Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

</details>


### [120] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文介绍了开源实现OpenFActScore框架，用于评估大型语言模型生成文本的真实性，通过事实原子生成和验证实现，支持任意兼容Hugging Face的模型，性能接近闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有FActScore依赖闭源商业模型，缺乏开放性和可重复性，需构建开源实现以支持广泛模型并提高透明性和成本效益。

Method: 设计并实现OpenFActScore，使之支持任意Hugging Face模型进行事实原子生成（AFG）和验证（AFV），改进设计并在标准基准上进行性能评测。

Result: 多款开源模型在AFG和AFV任务中表现良好，Gemma模型表现最佳，整体结果与原始FActScore实验达0.99的皮尔逊相关系数。

Conclusion: OpenFActScore成功实现了基于开源模型的文本真实性评估框架，具有高度透明性和可重复性，为研究和实际应用提供了成本效益高的评估工具。

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [121] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
*Ehud Reiter*

Main category: cs.CL

TL;DR: ACL领域的论文中极少有关注NLP系统实际影响的评估，主要集中于指标评估，缺乏对真实世界影响的深入理解。


<details>
  <summary>Details</summary>
Motivation: NLP技术的实际应用和推广需要更深入、系统的真实影响评估，但目前ACL社区对此关注甚少。

Method: 通过对ACL论文集的系统性调查，统计分析涉及实际影响评估的论文数量及质量。

Result: 发现约0.1%的论文包含真实影响评估，且大多数仅进行简单的评估，主要关注指标评估。

Conclusion: 建议NLP领域应更多关注技术的真实世界影响评估，以促进技术的实际应用和推广。

Abstract: The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [122] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文介绍了针对新加坡多语言环境的安全评估基准RabakBench，涵盖新加坡土语、中文、马来语和泰米尔语，构建了5000多个多语言安全标注样本。实验结果显示现有安全分类器在该基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和安全分类器在资源匮乏语言上的表现受限，缺少针对新加坡独特语言环境的安全评估数据集。

Method: 构建了一个三阶段可扩展流程：生成（通过大语言模型对新加坡土语网络内容进行对抗样本生成），标注（使用多数投票的半自动多标签安全注释），翻译（保持语言细节与毒性特征的高保真翻译跨多语言）。

Result: 最终数据集包含5000多个包含严重等级的数据显示安全类别样本，涵盖4种语言和6个细化安全类别。对11种热门开源和封闭源分类器的评测显示性能显著下降。

Conclusion: RabakBench为东南亚多语言安全评估提供了实证基础，并提出了可复制的构建本土化安全数据集的框架，数据集及评测工具已公开。

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [123] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
*Minghang Zhu,Shen Gao,Zhengliang Shi,Jiabao Fang,Pengjie Ren,Zhaochun Ren,Zhumin Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 提出一种语言模型自我进化方法，通过多层原则生成和基于原则的实例生成，有效扩展数据集，提升小型模型性能并减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有利用大规模语言模型扩展数据集的方法面临高碳排放和数据泄露风险。

Method: 先用大规模模型总结任务完成原则，再用小型模型基于这些原则生成大量训练数据进行模型训练。

Result: 方法显著提升了小型模型性能，同时大幅降低了训练过程中的碳排放。

Conclusion: 利用原则指导的数据生成策略不仅提升了模型表现，还解决了传统方法在碳排放和数据安全上的问题。

Abstract: A common training approach for language models involves using a large-scale
language model to expand a human-provided dataset, which is subsequently used
for model training.This method significantly reduces training costs by
eliminating the need for extensive human data annotation. However, it still
faces challenges such as high carbon emissions during data augmentation and the
risk of data leakage when we use closed-source LLMs. To address these issues,
we propose a self-evolution method for language models. First, we introduce the
Multi-level Principle Generation, which enables a large-scale model to
summarize task-completion principles based on a small amount of task data.
Then, we propose the Principle-based Instance Generation, in which a
smaller-scale language model uses these task principles to generate a large
amount of data. This data is then used for model training. Experimental results
show that our proposed method significantly improves model performance compared
to directly using a smaller-scale language model to generate data.
Additionally, since we only use the large-scale language model to generate the
task-completion principles, the carbon emissions associated with training the
model are greatly reduced.

</details>


### [124] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
*Nicholas Popovič,Ashish Kangen,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大语言模型的自动合成数据生成和上下文学习管道，用于文档级实体和关系抽取，解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前文档级实体和关系抽取在零样本或少样本设置下缺乏高质量的标注语料。

Method: 提出结合合成数据生成和基于检索的上下文学习，利用推理优化的大语言模型动态检索示例，避免人工标注。

Result: 生成了包含约5千个维基百科摘要、5.9万个实体和3万个关系三元组的合成数据集；在DocIE共享任务中测试了零样本上下文学习性能。

Conclusion: 即使对于最先进的大语言模型，文档级联合实体和关系抽取在零样本设置下依然具有挑战性。

Abstract: Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

</details>


### [125] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
*Youmna Farag,Svetlana Stoyanchev,Mohan Li,Simon Keizer,Rama Doddipatla*

Main category: cs.CL

TL;DR: 本文提出了一种基于零样本链式提示的多阶段条件故障恢复框架，显著提升了实体代理执行复杂任务时的故障恢复能力。


<details>
  <summary>Details</summary>
Motivation: 实体代理在执行复杂任务时易遭遇执行失败，亟需高效的故障恢复机制。

Method: 设计了包含四个错误处理阶段的多阶段恢复框架，利用大型语言模型的推理能力在任务执行和事后反思阶段分析执行环境中的挑战并制定解决策略。

Result: 在TEACH数据集的TfD基准测试中，该方法比无错误恢复基线提高11.5%，超越现有最强模型19%。

Conclusion: 提出的多阶段故障恢复框架有效增强了实体代理的执行鲁棒性，提升了复杂任务的完成率。

Abstract: Embodied agents performing complex tasks are susceptible to execution
failures, motivating the need for effective failure recovery mechanisms. In
this work, we introduce a conditional multistage failure recovery framework
that employs zero-shot chain prompting. The framework is structured into four
error-handling stages, with three operating during task execution and one
functioning as a post-execution reflection phase. Our approach utilises the
reasoning capabilities of LLMs to analyse execution challenges within their
environmental context and devise strategic solutions. We evaluate our method on
the TfD benchmark of the TEACH dataset and achieve state-of-the-art
performance, outperforming a baseline without error recovery by 11.5% and
surpassing the strongest existing model by 19%.

</details>


### [126] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: 本文研究大语言模型中训练数据的记忆难度，提出了熵-记忆法则，发现数据熵与记忆分数线性相关。


<details>
  <summary>Details</summary>
Motivation: 探讨如何刻画大语言模型中训练数据的记忆难度，这是一个基础但尚未充分研究的问题。

Method: 通过对OLMo模型系列的实验，提出并验证了熵-记忆法则；进一步分析了高随机性字符串的经验熵，采用该法则区分训练数据与测试数据，实现数据集推断。

Result: 发现数据熵与记忆分数呈线性关系；高随机性的“无意义字符串”表现出相对较低的经验熵；利用该现象成功区分训练和测试数据，实现数据集推断。

Conclusion: 熵-记忆法则为理解LLM中数据记忆机制提供了新视角，并可应用于训练和测试数据的区分，具备实际应用价值。

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [127] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: 本文综述了提示调优技术，通过添加可训练的连续向量且保持模型不变，实现对语言模型的高效适配。


<details>
  <summary>Details</summary>
Motivation: 为提高语言模型适配的参数效率，减少全部模型微调的成本与复杂度。

Method: 将提示调优方法分为直接提示学习和迁移学习两类，进一步细分多种具体技术，并详细分析设计、创新、优缺点。

Result: 归纳和比较了不同提示调优框架，指出了计算效率和训练稳定性方面的挑战。

Conclusion: 强调需提升训练鲁棒性和扩展提示调优应用范围，指明未来研究方向。

Abstract: This survey reviews prompt tuning, a parameter-efficient approach for
adapting language models by prepending trainable continuous vectors while
keeping the model frozen. We classify existing approaches into two categories:
direct prompt learning and transfer learning. Direct prompt learning methods
include: general optimization approaches, encoder-based methods, decomposition
strategies, and mixture-of-experts frameworks. Transfer learning methods
consist of: general transfer approaches, encoder-based methods, and
decomposition strategies. For each method, we analyze method designs,
innovations, insights, advantages, and disadvantages, with illustrative
visualizations comparing different frameworks. We identify challenges in
computational efficiency and training stability, and discuss future directions
in improving training robustness and broadening application scope.

</details>


### [128] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: NeoBabel是一个支持六种语言的多语言文本生成图像框架，提升了生成质量和效率，并推广了多语言评测标准。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像方法主要面向英文，存在语义漂移、计算负担和文化不匹配问题，限制了非英语用户的使用体验和数字公平。

Method: NeoBabel结合大规模多语言预训练与高分辨率指令微调训练，支持英语、中文、荷兰语、法语、印地语和波斯语六种语言，并扩展相关评测基准为多语言版本。

Result: NeoBabel在多语言基准测试中超过了现有多语言基模型，在保持英文能力的同时，多语言性能分别提升0.11和0.09，模型体积却是英文模型的1/2到1/4。

Conclusion: 多语言能力不仅不是性能折中，而是提升生成模型鲁棒性、效率和文化贴合度的关键，NeoBabel为促进包容性AI研究提供了完整工具和数据集。

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [129] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了Code Triangle框架，从编辑分析、代码实现和测试用例生成三个维度系统评估大规模语言模型（LLMs）的编程能力，并发现模型在多样性和鲁棒性上仍不及人类。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型虽然在代码生成上取得了显著进展，但其真实编程能力和表现尚未被系统全面评估，特别是在多维度能力和与人类专家的差异方面。

Method: 构建Code Triangle框架，从编辑分析、代码实现和测试用例生成三方面评估LLMs，进行大规模竞赛编程基准测试，并分析模型认知与人类技能的分布差异。

Result: 发现LLMs虽然能在三维度间构建自洽系统，但其解法缺乏人类的多样性和鲁棒性，错误多因训练数据偏差和推理迁移能力有限。加入人类编辑、解法和多样化测试用例及模型混合能显著提升性能和鲁棒性。

Conclusion: 研究揭示了LLMs认知的一致性与不一致性，这为模型的自我反思和自我提升提供了潜在方向，有助于开发更强大的编程模型。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [130] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
*Wei Shen,Jiangbo Pei,Yi Peng,Xuchen Song,Yang Liu,Jian Peng,Haofeng Sun,Yunzhuo Hao,Peiyu Wang,Yahui Zhou*

Main category: cs.CL

TL;DR: Skywork-R1V3是一种先进的开源视觉语言模型，通过后训练的强化学习框架，有效提升视觉推理能力，实现了领先的多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 将文本大语言模型的推理能力迁移到视觉任务中，提升视觉语言模型的推理表现。

Method: 采用精心设计的后训练强化学习框架，激活并增强模型推理能力，利用连接模块实现跨模态对齐，并引入关键推理token的熵作为训练过程中模型优选指标。

Result: Skywork-R1V3在MMMU任务上表现达76.0%，显著超过之前的64.3%，达到接近入门级人类水平。大规模模型经强化学习后可媲美顶级闭源视觉语言模型。

Conclusion: 通过强化学习的后训练策略，Skywork-R1V3实现了多模态推理能力的重大突破，为开源视觉语言模型的发展提供了强大动力。

Abstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.

</details>


### [131] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
*Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge Zhang*

Main category: cs.CL

TL;DR: 本文提出了CriticLean框架，通过强化学习提升对数学命题自然语言转形式代码的语义评估能力，构建了CriticLeanGPT评估模型和CriticLeanBench基准测试，并发布了含28.5万问题的FineLeanCorpus数据集。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注生成和编译成功率，忽视了生成的形式化表达是否真正反映了原问题的语义意图，评估阶段较为薄弱。

Method: 提出CriticLean框架，将评价者从被动验证者转变为主动学习组件；训练CriticLeanGPT模型进行语义忠实度评估；设计CriticLeanBench用于测评模型识别语义正误的能力。

Result: CriticLeanGPT显著优于现有强基线模型；FineLeanCorpus数据集具有领域丰富、难度广泛、高正确率特点，促进模型训练和评估。

Conclusion: 强化评价阶段对高质量形式化表达至关重要，CriticLean为未来形式数学推理研究提供了重要方法和资源。

Abstract: Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

</details>


### [132] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: 这篇论文研究了利用迁移学习和风格化数据增强提升英语新闻文本中主观性检测的效果，提出了基于GPT-4o的风格化增强方法，并取得了较好的结果。


<details>
  <summary>Details</summary>
Motivation: 提升英语新闻文本中主观和客观句子的分类效果，弥补现有方法在主观性检测上的不足。

Method: 对比了预训练编码器的微调与在相关任务上微调后转移学习的方法，引入GPT-4o进行受控的风格化数据增强，并使用同一模型进行生成数据的标签和风格一致性校正。

Result: 迁移学习的特定编码器优于通用编码器的微调，风格化数据增强显著提升模型鲁棒性，尤其在主观内容检测上效果突出。官方提交在24个参赛队伍中排名第16。

Conclusion: 结合编码器专业化和标签一致的数据增强是提升主观性检测效果的有效策略。

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [133] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
*Maximilian Heil,Aleksandar Pramov*

Main category: cs.CL

TL;DR: 本文针对涉及数量、比较和时间引用的数值声明，评估了自动事实核查中真实性预测的建模策略。


<details>
  <summary>Details</summary>
Motivation: 数值声明因涉及复杂的数量和时间信息，对自动事实核查系统构成独特挑战，需要探索有效的建模方法提升准确率。

Method: 使用QuanTemp数据集和自建证据检索管道，研究现代BERT在不同证据数量、右到左(R2L)分词及其组合对分类性能的影响。

Result: R2L分词和更长输入上下文窗口均未提升性能，强调证据质量为关键瓶颈。最佳系统在CheckThat!2025任务中获得宏平均F1得分0.57，排名前四。

Conclusion: 提高自动事实核查中数值声明的性能，需更多关注证据质量而非仅提升上下文长度或采用R2L分词。

Abstract: Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [134] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: 本文介绍了UQLM，一个利用不确定性量化技术检测大型语言模型幻觉的Python工具包。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成虚假或误导性内容（幻觉）影响下游应用的安全性和可信度。

Method: 采用最新的不确定性量化技术，通过多个基于不确定性的评分器计算回应级置信度得分（0至1）。

Result: 提供了一套现成的基于不确定性的幻觉检测工具，能够有效评估模型生成内容的可信度。

Conclusion: UQLM工具包可以轻松集成到各种应用中，提高大型语言模型输出的可靠性，缓解幻觉问题。

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [135] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
*Rui-Jie Zhu,Tianhao Peng,Tianhao Cheng,Xingwei Qu,Jinfa Huang,Dawei Zhu,Hao Wang,Kaiwen Xue,Xuanliang Zhang,Yong Shan,Tianle Cai,Taylor Kergan,Assel Kembay,Andrew Smith,Chenghua Lin,Binh Nguyen,Yuqi Pan,Yuhong Chou,Zefan Cai,Zhenhe Wu,Yongchi Zhao,Tianyu Liu,Jian Yang,Wangchunshu Zhou,Chujie Zheng,Chongxuan Li,Yuyin Zhou,Zhoujun Li,Zhaoxiang Zhang,Jiaheng Liu,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文综述了潜在推理技术，这是通过模型连续隐藏状态实现的多步推理方法，避免了对自然语言的依赖。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）虽提升了大语言模型的推理能力和解释性，但受限于自然语言表达，降低了模型的表达能力，潜在推理试图打破这一瓶颈。

Method: 论文回顾了潜在推理的基础神经网络层作用、多种实现方法（激活循环、隐藏状态传播、微调策略）及先进范式（如通过掩码扩散模型实现无限深度推理）。

Result: 综合分析了当前潜在推理的主要途径和技术特点，揭示其如何支持复杂、全局一致的推理过程。

Conclusion: 该综述明确了潜在推理的概念框架，推动该领域研究发展，并提供了最新资源的GitHub链接以促进社区交流。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, especially when guided by explicit chain-of-thought (CoT)
reasoning that verbalizes intermediate steps. While CoT improves both
interpretability and accuracy, its dependence on natural language reasoning
limits the model's expressive bandwidth. Latent reasoning tackles this
bottleneck by performing multi-step inference entirely in the model's
continuous hidden state, eliminating token-level supervision. To advance latent
reasoning research, this survey provides a comprehensive overview of the
emerging field of latent reasoning. We begin by examining the foundational role
of neural network layers as the computational substrate for reasoning,
highlighting how hierarchical representations support complex transformations.
Next, we explore diverse latent reasoning methodologies, including
activation-based recurrence, hidden state propagation, and fine-tuning
strategies that compress or internalize explicit reasoning traces. Finally, we
discuss advanced paradigms such as infinite-depth latent reasoning via masked
diffusion models, which enable globally consistent and reversible reasoning
processes. By unifying these perspectives, we aim to clarify the conceptual
landscape of latent reasoning and chart future directions for research at the
frontier of LLM cognition. An associated GitHub repository collecting the
latest papers and repos is available at:
https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [136] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
*Ayush Parikh,Hoang Thanh Thanh Truong,Jeanette Schofield,Maximilian Heil*

Main category: cs.CL

TL;DR: 本文针对CLEF 2025 CheckThat!任务4a的科学网络话语检测，提出了三种模型方法，团队排名第七，成绩优于基线。


<details>
  <summary>Details</summary>
Motivation: 需要识别推文中是否包含科学论断、科学研究引用或科学实体，以提升科学网络话语的自动检测能力。

Method: 采用三种方法：变换器微调、少量样本提示的大型语言模型(LLM)和基于前期实验设计的集成模型。

Result: 团队在比赛中获得第七名，宏平均F1分数达0.8611，高于基线的0.8375。

Conclusion: 多方法结合的模型在多分类科学话语检测任务中表现优异，具有实际应用潜力。

Abstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [137] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: 本文提出了针对大型语言模型（LLM）重排序任务的效率评估新指标E²R-FLOPs，解决了现有效率评估指标受硬件和运行环境影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM重排序效率评估指标依赖硬件和运行时间设置，难以准确反映模型大小及效率-效果的权衡，阻碍实用部署。

Method: 提出了基于计算量（FLOPs）的效率指标RPP（单位计算相关性指标）和QPP（单位计算吞吐量指标），并设计了无需实验即可预测FLOPs的估算器。

Result: 通过广泛实验验证了该指标在不同LLM架构上的有效性，揭示了效率与效果之间的权衡关系。

Conclusion: E²R-FLOPs指标为LLM重排序任务提供了硬件无关、可解释的效率评估方法，有助于推动该领域的实用性研究与优化。

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [138] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 引入了Agent KB，一个分层经验框架，通过Reason-Retrieve-Refine流程提升语言代理在复杂任务中的错误纠正和经验复用能力。


<details>
  <summary>Details</summary>
Motivation: 语言代理在处理复杂任务时难以有效纠错及跨域复用经验，且不能互相学习彼此的经验。

Method: 提出Agent KB框架，通过捕捉高层策略和详细执行日志形成共享知识库，实现跨代理知识转移，使用Reason-Retrieve-Refine管道进行问题解决。

Result: 在GAIA基准测试中，Agent KB将成功率提升最多16.28个百分点，Claude-3和GPT-4在不同任务上均显著提升表现，在SWE-bench代码修复任务中也有明显改进。

Conclusion: Agent KB为语言代理提供了一个模块化、框架无关的基础设施，促进代理从过往经验学习并将成功策略推广到新任务。

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation](https://arxiv.org/abs/2507.05331)
*TRI LBM Team,Jose Barreiros,Andrew Beaulieu,Aditya Bhat,Rick Cory,Eric Cousineau,Hongkai Dai,Ching-Hsin Fang,Kunimatsu Hashimoto,Muhammad Zubair Irshad,Masha Itkina,Naveen Kuppuswamy,Kuan-Hui Lee,Katherine Liu,Dale McConachie,Ian McMahon,Haruki Nishimura,Calder Phillips-Grafflin,Charles Richter,Paarth Shah,Krishnan Srinivasan,Blake Wulfe,Chen Xu,Mengchao Zhang,Alex Alspach,Maya Angeles,Kushal Arora,Vitor Campagnolo Guizilini,Alejandro Castro,Dian Chen,Ting-Sheng Chu,Sam Creasey,Sean Curtis,Richard Denitto,Emma Dixon,Eric Dusel,Matthew Ferreira,Aimee Goncalves,Grant Gould,Damrong Guoy,Swati Gupta,Xuchen Han,Kyle Hatch,Brendan Hathaway,Allison Henry,Hillel Hochsztein,Phoebe Horgan,Shun Iwase,Donovon Jackson,Siddharth Karamcheti,Sedrick Keh,Joseph Masterjohn,Jean Mercat,Patrick Miller,Paul Mitiguy,Tony Nguyen,Jeremy Nimmer,Yuki Noguchi,Reko Ong,Aykut Onol,Owen Pfannenstiehl,Richard Poyner,Leticia Priebe Mendes Rocha,Gordon Richardson,Christopher Rodriguez,Derick Seale,Michael Sherman,Mariah Smith-Jones,David Tago,Pavel Tokmakov,Matthew Tran,Basile Van Hoorick,Igor Vasiljevic,Sergey Zakharov,Mark Zolotas,Rares Ambrus,Kerri Fetzer-Borelli,Benjamin Burchfiel,Hadas Kress-Gazit,Siyuan Feng,Stacie Ford,Russ Tedrake*

Main category: cs.RO

TL;DR: 本文评估了多任务机器人操作策略（大型行为模型LBMs），通过扩展扩散策略范式，在仿真和真实机器人数据上进行了严格测试。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人模仿学习和大规模基础模型发展迅速，现实中的性能评估仍然困难，限制了对当前能力的深入理解和进一步发展。

Method: 提出并验证了一个评价流程，采用盲测和随机试验，在仿真及真实环境中对多任务和单任务模型进行对比分析。

Result: 多任务预训练提高了策略的成功率和鲁棒性，能更快速学习复杂新任务，性能随着预训练数据量和多样性增长而提升。

Conclusion: 多任务预训练的机器人操作基础模型在效率和表现上优于单任务模型，拓展了机器人操作的能力和应用潜力。

Abstract: Robot manipulation has seen tremendous progress in recent years, with
imitation learning policies enabling successful performance of dexterous and
hard-to-model tasks. Concurrently, scaling data and model size has led to the
development of capable language and vision foundation models, motivating
large-scale efforts to create general-purpose robot foundation models. While
these models have garnered significant enthusiasm and investment, meaningful
evaluation of real-world performance remains a challenge, limiting both the
pace of development and inhibiting a nuanced understanding of current
capabilities. In this paper, we rigorously evaluate multitask robot
manipulation policies, referred to as Large Behavior Models (LBMs), by
extending the Diffusion Policy paradigm across a corpus of simulated and
real-world robot data. We propose and validate an evaluation pipeline to
rigorously analyze the capabilities of these models with statistical
confidence. We compare against single-task baselines through blind, randomized
trials in a controlled setting, using both simulation and real-world
experiments. We find that multi-task pretraining makes the policies more
successful and robust, and enables teaching complex new tasks more quickly,
using a fraction of the data when compared to single-task baselines. Moreover,
performance predictably increases as pretraining scale and diversity grows.
Project page: https://toyotaresearchinstitute.github.io/lbm1/

</details>


### [140] [Feature Geometry for Stereo Sidescan and Forward-looking Sonar](https://arxiv.org/abs/2507.05410)
*Kalin Norman,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 本文提出了一种基于几何的方法，实现了用于海洋机器人中前视声纳和侧扫声纳的立体声声纳数据融合。


<details>
  <summary>Details</summary>
Motivation: 为了实现海洋机器人中不同类型声纳数据的融合，提高特征匹配和三维信息恢复能力。

Method: 借鉴立体相机的极几何，利用两声纳之间的相对姿态，将一个声纳观测到的特征投影到另一个声纳图像中，同时分析特征相对位置和声纳相对姿态对投影的影响。

Result: 通过仿真结果识别出了适合于现场机器人应用的立体声纳配置，有助于特征对应和三维信息恢复。

Conclusion: 该方法为海洋机器人中双模态声纳的特征对应和三维信息提取提供了一种有效的几何投影方案。

Abstract: In this paper, we address stereo acoustic data fusion for marine robotics and
propose a geometry-based method for projecting observed features from one sonar
to another for a cross-modal stereo sonar setup that consists of both a
forward-looking and a sidescan sonar. Our acoustic geometry for sidescan and
forward-looking sonar is inspired by the epipolar geometry for stereo cameras,
and we leverage relative pose information to project where an observed feature
in one sonar image will be found in the image of another sonar. Additionally,
we analyze how both the feature location relative to the sonar and the relative
pose between the two sonars impact the projection. From simulated results, we
identify desirable stereo configurations for applications in field robotics
like feature correspondence and recovery of the 3D information of the feature.

</details>


### [141] [CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning](https://arxiv.org/abs/2507.05458)
*Yi-Shiuan Tung,Bradley Hayes,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文提出了一种名为CRED的轨迹生成方法，通过联合优化环境设计和轨迹选择，提高了主动偏好学习中人类奖励函数的估计效果，尤其适用于长时间任务。


<details>
  <summary>Details</summary>
Motivation: 机器人在实际应用中需根据人类偏好调整行为，而现有主动偏好学习方法难以全面探索轨迹空间，且难以获取有效的查询，影响奖励函数的准确学习。

Method: CRED通过环境设计想象新场景，并利用反事实推理，从当前奖励信念中采样，生成多样且信息丰富的轨迹供人类评选，从而改善奖励估计。

Result: 在GridWorld和基于OpenStreetMap的数据的真实导航实验中，CRED表现出更好的奖励学习效果和更强的跨环境泛化能力。

Conclusion: CRED有效提升了主动偏好学习的奖励函数学习与泛化能力，为机器人在多样环境中适应人类偏好提供了有力方法支持。

Abstract: For effective real-world deployment, robots should adapt to human
preferences, such as balancing distance, time, and safety in delivery routing.
Active preference learning (APL) learns human reward functions by presenting
trajectories for ranking. However, existing methods often struggle to explore
the full trajectory space and fail to identify informative queries,
particularly in long-horizon tasks. We propose CRED, a trajectory generation
method for APL that improves reward estimation by jointly optimizing
environment design and trajectory selection. CRED "imagines" new scenarios
through environment design and uses counterfactual reasoning -- by sampling
rewards from its current belief and asking "What if this reward were the true
preference?" -- to generate a diverse and informative set of trajectories for
ranking. Experiments in GridWorld and real-world navigation using OpenStreetMap
data show that CRED improves reward learning and generalizes effectively across
different environments.

</details>


### [142] [Gaussian Process-Based Active Exploration Strategies in Vision and Touch](https://arxiv.org/abs/2507.05522)
*Ho Jin Choi,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种融合视觉与触觉数据的统一高斯过程距离场模型，以实现机器人对物体形状和表面属性的主动感知，提升复杂环境中的物体操作能力。


<details>
  <summary>Details</summary>
Motivation: 机器人在非结构化环境中由于先验知识有限，难以理解物体的形状、材料及语义，限制了其操作能力。人类通过多传感器交互探索学习这些属性，启发了该研究融合视觉与触觉的思路。

Method: 采用高斯过程距离场（GPDF）编码点云的有符号距离、解析梯度和海森矩阵及表面不确定性，结合差分渲染迭代整合视觉与触觉数据，量化多传感器不确定性并规划探索动作，提升物体几何估计精度。

Result: 通过在真实机器人（Franka Research 3与定制DIGIT触觉传感器、Intel Realsense D435）上实验，展示了该方法在主动探索和精确恢复物体三维结构及表面属性上的有效性，同时提出了高斯过程诱导点方法以提升可扩展性。

Conclusion: 该方法实现了多模态融合的概率建模，支持机器人主动感知复杂对象几何及表面特性，具备扩展到更多属性的潜力，推动了机器人在复杂非结构化环境中的自主操作能力。

Abstract: Robots struggle to understand object properties like shape, material, and
semantics due to limited prior knowledge, hindering manipulation in
unstructured environments. In contrast, humans learn these properties through
interactive multi-sensor exploration. This work proposes fusing visual and
tactile observations into a unified Gaussian Process Distance Field (GPDF)
representation for active perception of object properties. While primarily
focusing on geometry, this approach also demonstrates potential for modeling
surface properties beyond geometry. The GPDF encodes signed distance using
point cloud, analytic gradient and Hessian, and surface uncertainty estimates,
which are attributes that common neural network shape representation lack. By
utilizing a point cloud to construct a distance function, GPDF does not need
extensive pretraining on large datasets and can incorporate observations by
aggregation. Starting with an initial visual shape estimate, the framework
iteratively refines the geometry by integrating dense vision measurements using
differentiable rendering and tactile measurements at uncertain surface regions.
By quantifying multi-sensor uncertainties, it plans exploratory motions to
maximize information gain for recovering precise 3D structures. For the
real-world robot experiment, we utilize the Franka Research 3 robot
manipulator, which is fixed on a table and has a customized DIGIT tactile
sensor and an Intel Realsense D435 RGBD camera mounted on the end-effector. In
these experiments, the robot explores the shape and properties of objects
assumed to be static and placed on the table. To improve scalability, we
investigate approximation methods like inducing point method for Gaussian
Processes. This probabilistic multi-modal fusion enables active exploration and
mapping of complex object geometries, extending potentially beyond geometry.

</details>


### [143] [PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs](https://arxiv.org/abs/2507.05555)
*Obin Kwon,Sankalp Yamsani,Noboru Myers,Sean Taylor,Jooyoung Hong,Kyungseo Park,Alex Alspach,Joohyung Kim*

Main category: cs.RO

TL;DR: PAPRLE是一个模块化机器人肢体环境，支持灵活配置和多种输入设备控制，实现双向力反馈，促进遥控操作和AI研究。


<details>
  <summary>Details</summary>
Motivation: 为应对不同任务需求，提升遥控机器人肢体的灵活性和适应性，开发一个可插拔、模块化的机器人肢体控制生态系统。

Method: 设计并实现PAPRLE系统，支持多种控制输入（傀儡装置、游戏控制器、VR接口），引入可插拔傀儡装置，实现关节空间和任务空间控制及实时力反馈，支持多种空间肢体排列。

Result: PAPRLE在多种实际场景中验证了其灵活性和多设备、多机器人组合的适配性，实现了高效双向遥控，促进了数据采集和学习控制研究。

Conclusion: PAPRLE作为开源硬件和软件平台，将推动机器人肢体遥控系统的广泛应用和社区扩展，促进体现式AI及机器人控制领域进展。

Abstract: We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular
ecosystem that enables flexible placement and control of robotic limbs. With
PAPRLE, a user can change the arrangement of the robotic limbs, and control
them using a variety of input devices, including puppeteers, gaming
controllers, and VR-based interfaces. This versatility supports a wide range of
teleoperation scenarios and promotes adaptability to different task
requirements. To further enhance configurability, we introduce a pluggable
puppeteer device that can be easily mounted and adapted to match the target
robot configurations. PAPRLE supports bilateral teleoperation through these
puppeteer devices, agnostic to the type or configuration of the follower robot.
By supporting both joint-space and task-space control, the system provides
real-time force feedback, improving user fidelity and physical interaction
awareness. The modular design of PAPRLE facilitates novel spatial arrangements
of the limbs and enables scalable data collection, thereby advancing research
in embodied AI and learning-based control. We validate PAPRLE in various
real-world settings, demonstrating its versatility across diverse combinations
of leader devices and follower robots. The system will be released as open
source, including both hardware and software components, to support broader
adoption and community-driven extension. Additional resources and
demonstrations are available at the project website:
https://uiuckimlab.github.io/paprle-pages

</details>


### [144] [Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube](https://arxiv.org/abs/2507.05607)
*Chongshan Fan,Shenghai Yuan*

Main category: cs.RO

TL;DR: Auto-RubikAI是一种结合知识库、视觉语言模型和大语言模型的机器人自主规划系统，可以在无需大量演示数据的情况下，解算魔方复原任务，实现多步解释性控制。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统依赖预定义脚本，或现代方法依赖大规模训练和演示，难以实现少数据、高解释性的结构化操作任务。

Method: 系统结合symbolic知识库解决符号推理，视觉语言模型解析三维场景，大语言模型生成结构化指令，形成三模块架构，支持空间不确定性环境下的高效作业。

Result: 在仿真及实物7自由度机械臂上测试，任务成功率达79%，相较其他方法减少解算步骤且保持可解释性和安全性。

Conclusion: Auto-RubikAI为智能制造及机器人自主执行任务提供一种数据低需求、高效且模块化的规划框架，具备良好实用前景。

Abstract: This paper presents Auto-RubikAI, a modular autonomous planning framework
that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM),
and a large language model (LLM) to solve structured manipulation tasks
exemplified by Rubik's Cube restoration. Unlike traditional robot systems based
on predefined scripts, or modern approaches relying on pretrained networks and
large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step
task execution with minimal data requirements and no prior demonstrations. The
proposed system employs a KB module to solve group-theoretic restoration steps,
overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to
construct a semantic 3D scene representation, while the LLM generates
structured robotic control code via prompt chaining. This tri-module
architecture enables robust performance under spatial uncertainty. We deploy
Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic
arm, demonstrating effective Sim-to-Real adaptation without retraining.
Experiments show a 79% end-to-end task success rate across randomized
configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our
KB-enhanced method reduces average solution steps while maintaining
interpretability and safety. Auto-RubikAI provides a cost-efficient, modular
foundation for embodied task planning in smart manufacturing, robotics
education, and autonomous execution scenarios. Code, prompts, and hardware
modules will be released upon publication.

</details>


### [145] [DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation](https://arxiv.org/abs/2507.05627)
*Young Hun Kim,Seungyeon Kim,Yonghyeon Lee,Frank Chongwoo Park*

Main category: cs.RO

TL;DR: DreamGrasp利用大规模预训练图像生成模型的想象能力，实现部分视角下复杂多物体场景的鲁棒3D重建。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，由于遮挡和稀疏视角，传统基于对称性或监督学习的3D识别方法难以普适，且缺乏可靠深度信息，亟需新的解决方案。

Method: DreamGrasp结合粗糙3D重建、对比学习实现实例分割，并利用文本引导对单个实例进行细化，从而推断场景未观察部分。

Result: 实验表明，DreamGrasp能够准确恢复物体几何形状，在复杂多物体环境中实现高成功率的连续整理和目标检索。

Conclusion: DreamGrasp克服了现有方法的局限性，实现了对部分视角场景中多物体的高效3D重建和应用，具有广泛的实际应用价值。

Abstract: Partial-view 3D recognition -- reconstructing 3D geometry and identifying
object instances from a few sparse RGB images -- is an exceptionally
challenging yet practically essential task, particularly in cluttered, occluded
real-world settings where full-view or reliable depth data are often
unavailable. Existing methods, whether based on strong symmetry priors or
supervised learning on curated datasets, fail to generalize to such scenarios.
In this work, we introduce DreamGrasp, a framework that leverages the
imagination capability of large-scale pre-trained image generative models to
infer the unobserved parts of a scene. By combining coarse 3D reconstruction,
instance segmentation via contrastive learning, and text-guided instance-wise
refinement, DreamGrasp circumvents limitations of prior methods and enables
robust 3D reconstruction in complex, multi-object environments. Our experiments
show that DreamGrasp not only recovers accurate object geometry but also
supports downstream tasks like sequential decluttering and target retrieval
with high success rates.

</details>


### [146] [A Physics-Based Continuum Model for Versatile, Scalable, and Fast Terramechanics Simulation](https://arxiv.org/abs/2507.05643)
*Huzaifa Unjhawala,Luning Bakke,Harry Zhang,Michael Taylor,Ganesh Arivoli,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: 本文提出了Chrono的连续表示模型（Chrono::CRM），一种基于物理的高效通用地形力学仿真方法，能够模拟复杂任务如挖掘和车辆轮胎与地形的相互作用，具有可扩展性和GPU加速优势，实现大规模高保真仿真。


<details>
  <summary>Details</summary>
Motivation: 传统地形力学方法如半经验模型在应对复杂地形交互和任务时存在局限，需要一个物理基础更强、适用范围更广泛且高效的仿真模型。

Method: 基于Chrono的平滑粒子流体动力学（SPH）框架，结合动力学引擎实现可模拟刚性及柔性部件交互，采用GPU加速和“活跃域”技术提升计算效率，实现大规模地形仿真。

Result: 该模型通过三项物理实验（包括NASA MGRU3漫游车测试）验证，并与高精度离散元方法（DEM）仿真对比，展示出高准确性和高效性，能够处理长达10公里、含1亿粒子的地形。

Conclusion: Chrono::CRM是一个开源的、可扩展且高效的地形力学仿真方案，支持复杂机械与地形交互，适用于大规模离线或近实时仿真，推动地形力学研究与应用的发展。

Abstract: This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.

</details>


### [147] [3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting](https://arxiv.org/abs/2507.05661)
*Haitao Lu,Haijier Chen,Haoze Liu,Shoujian Zhang,Bo Xu,Ziao Liu*

Main category: cs.RO

TL;DR: 提出了一种基于3D高斯点云渲染的单目RGB图像定位框架3DGS-LSR，实现了厘米级精度的城市环境自主机器人定位。


<details>
  <summary>Details</summary>
Motivation: 传统GNSS定位在复杂城市环境中易受信号遮挡和多路径效应影响，导致定位不可靠；同时传统地图构建方法存储和计算开销大，不适合资源受限的机器人平台。

Method: 结合多传感器数据构建高精度3DGS地图，机器人端只需单目RGB图像。利用SuperPoint和SuperGlue进行特征提取和匹配，采用迭代优化策略通过逐步渲染细化定位结果，支持实时导航。

Result: 在KITTI数据集的多种城市道路环境中，3DGS-LSR实现了0.026m、0.029m和0.081m的平均定位精度，显著优于其他代表性方法，且只需单目RGB输入。

Conclusion: 3DGS-LSR提供了一种高效、精确且适用于实际复杂城市环境的自主机器人定位方案，能够在GNSS失效情况下保持可靠定位能力。

Abstract: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

</details>


### [148] [Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains](https://arxiv.org/abs/2507.05663)
*Neelay Joglekar,Fei Liu,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 本文提出了一种稳定的远端运动中心（RCM）操纵器控制方法，解决了传统电缆驱动手术机器人中不可视关节误差的问题，提升了机器人自主手术的可行性。


<details>
  <summary>Details</summary>
Motivation: RCM手术机器人在微创手术中精确操作至关重要，但电缆驱动导致的关节测量误差影响控制准确性，尤其是插入点之前不可见的误差无法通过视觉追踪纠正。

Method: 设计了一种可证明稳定的闭环跟踪控制器，专门针对不可视的臂链部分；并将其集成到双层控制方案中，以控制整个运动链。

Result: 在模拟和真实环境中严格验证了该方法的理论稳定性和有效性。

Conclusion: 该研究为实现从遥操作到自主手术迈出了关键一步，提供了针对RCM手术机器人误差控制的新思路。

Abstract: Remote Center of Motion (RCM) robotic manipulators have revolutionized
Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation
within the patient's body cavity without disturbing the insertion point on the
patient. Accurate RCM tool control is vital for incorporating autonomous
subtasks like suturing, blood suction, and tumor resection into robotic
surgical procedures, reducing surgeon fatigue and improving patient outcomes.
However, these cable-driven systems are subject to significant joint reading
errors, corrupting the kinematics computation necessary to perform control.
Although visual tracking with endoscopic cameras can correct errors on in-view
joints, errors in the kinematic chain prior to the insertion point are
irreparable because they remain out of view. No prior work has characterized
the stability of control under these conditions. We fill this gap by designing
a provably stable tracking-in-the-loop controller for the out-of-view portion
of the RCM manipulator kinematic chain. We additionally incorporate this
controller into a bilevel control scheme for the full kinematic chain. We
rigorously benchmark our method in simulated and real world settings to verify
our theoretical findings. Our work provides key insights into the next steps
required for the transition from teleoperated to autonomous surgery.

</details>


### [149] [Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control](https://arxiv.org/abs/2507.05674)
*Xinyao Qin,Xiaoteng Ma,Yang Qi,Qihan Liu,Chuanyi Xue,Ning Gui,Qinyu Dong,Jun Yang,Bin Liang*

Main category: cs.RO

TL;DR: 提出了基于扩散模型的四足机器人多任务预训练与在线强化学习微调框架DMLoco，实现语言条件控制和稳定任务切换。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型的多任务泛化和语言条件优势，解决四足机器人运动中稳定性差和任务切换困难的问题。

Method: 基于Denoising Diffusion Implicit Models先进行多任务数据预训练，再通过在线PPO算法在仿真中微调，实现稳定性和任务转换。使用TensorRT优化部署，保证50Hz实时运行。

Result: DMLoco支持语言引导的多技能执行，适应性强，任务切换稳定，适用于资源受限的机器人平台，实现高效快速的在线控制。

Conclusion: 结合扩散模型与强化学习的多任务预训练与微调策略，有效提升四足机器人语言条件控制和任务切换能力，具备良好实用价值。

Abstract: Recent research has highlighted the powerful capabilities of imitation
learning in robotics. Leveraging generative models, particularly diffusion
models, these approaches offer notable advantages such as strong multi-task
generalization, effective language conditioning, and high sample efficiency.
While their application has been successful in manipulation tasks, their use in
legged locomotion remains relatively underexplored, mainly due to compounding
errors that affect stability and difficulties in task transition under limited
data. Online reinforcement learning (RL) has demonstrated promising results in
legged robot control in the past years, providing valuable insights to address
these challenges. In this work, we propose DMLoco, a diffusion-based framework
for quadruped robots that integrates multi-task pretraining with online PPO
finetuning to enable language-conditioned control and robust task transitions.
Our approach first pretrains the policy on a diverse multi-task dataset using
diffusion models, enabling language-guided execution of various skills. Then,
it finetunes the policy in simulation to ensure robustness and stable task
transition during real-world deployment. By utilizing Denoising Diffusion
Implicit Models (DDIM) for efficient sampling and TensorRT for optimized
deployment, our policy runs onboard at 50Hz, offering a scalable and efficient
solution for adaptive, language-guided locomotion on resource-constrained
robotic platforms.

</details>


### [150] [Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning](https://arxiv.org/abs/2507.05695)
*Xiatao Sun,Yuxuan Wang,Shuo Yang,Yinxing Chen,Daniel Rakita*

Main category: cs.RO

TL;DR: 本文提出了hPGA-DP，一种结合射影几何代数（PGA）和扩散策略的机器人运动生成方法，通过融合几何归纳偏置提高训练效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习中的扩散策略每次训练需重新学习空间变换（如平移、旋转），效率较低。引入几何归纳偏置能减少冗余，提高训练效率。

Method: 将射影几何代数（PGA）框架引入扩散策略，设计P-GATr架构利用其E(3)-等变性，采用混合架构以P-GATr作为编码解码器，结合U-Net或Transformer模块完成去噪。

Result: 实验证明hPGA-DP在模拟与真实环境中较传统方法展现出更优的任务表现和训练效率，并通过混合架构实现更快收敛。

Conclusion: 通过引入PGA几何偏置和混合架构，hPGA-DP有效提升了机器人运动生成任务的性能与训练速度，具备较强实用价值。

Abstract: Diffusion policies have become increasingly popular in robot learning due to
their reliable convergence in motion generation tasks. At a high level, these
policies learn to transform noisy action trajectories into effective ones,
conditioned on observations. However, each time such a model is trained in a
robotics context, the network must relearn fundamental spatial representations
and operations, such as translations and rotations, from scratch in order to
ground itself and operate effectively in a 3D environment. Incorporating
geometric inductive biases directly into the network can alleviate this
redundancy and substantially improve training efficiency. In this paper, we
introduce hPGA-DP, a diffusion policy approach that integrates a mathematical
framework called Projective Geometric Algebra (PGA) to embed strong geometric
inductive biases. PGA is particularly well-suited for this purpose as it
provides a unified algebraic framework that naturally encodes geometric
primitives, such as points, directions, and rotations, enabling neural networks
to reason about spatial structure through interpretable and composable
operations. Specifically, we propose a novel diffusion policy architecture that
incorporates the Projective Geometric Algebra Transformer (P-GATr), leveraging
its E(3)-equivariant properties established in prior work. Our approach adopts
a hybrid architecture strategy, using P-GATr as both a state encoder and action
decoder, while employing U-Net or Transformer-based modules for the denoising
process. Several experiments and ablation studies in both simulated and
real-world environments demonstrate that hPGA-DP not only improves task
performance and training efficiency through the geometric bias of P-GATr, but
also achieves substantially faster convergence through its hybrid model
compared to architectures that rely solely on P-GATr.

</details>


### [151] [DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving](https://arxiv.org/abs/2507.05710)
*Hyeongchan Ham,Heejin Ahn*

Main category: cs.RO

TL;DR: 本文提出了一种基于分布鲁棒优化(DRO)和证据深度学习(EDL)的自动驾驶运动规划安全框架，通过动态调整不确定性约束，在感知置信度不同的情况下实现安全与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖神经网络感知，但基于不确定感知结果的控制决策存在显著安全风险，因此需要一种方法有效处理感知中的不确定性。

Method: 提出结合EDL的DRO框架，引入基于证据分布的新型模糊集，动态调整保守性，整合进模型预测控制(MPC)，形成计算可行的DRO-EDL-MPC算法。

Result: 在CARLA仿真环境中验证，方法在高感知置信度下保持效率，在低置信度下保证约束保守，提升了安全性。

Conclusion: 该方法有效兼顾感知不确定性与控制安全性，实现了自动驾驶运动规划的鲁棒性和效率的动态平衡。

Abstract: Safety is a critical concern in motion planning for autonomous vehicles.
Modern autonomous vehicles rely on neural network-based perception, but making
control decisions based on these inference results poses significant safety
risks due to inherent uncertainties. To address this challenge, we present a
distributionally robust optimization (DRO) framework that accounts for both
aleatoric and epistemic perception uncertainties using evidential deep learning
(EDL). Our approach introduces a novel ambiguity set formulation based on
evidential distributions that dynamically adjusts the conservativeness
according to perception confidence levels. We integrate this uncertainty-aware
constraint into model predictive control (MPC), proposing the DRO-EDL-MPC
algorithm with computational tractability for autonomous driving applications.
Validation in the CARLA simulator demonstrates that our approach maintains
efficiency under high perception confidence while enforcing conservative
constraints under low confidence.

</details>


### [152] [Simultaneous Triggering and Synchronization of Sensors and Onboard Computers](https://arxiv.org/abs/2507.05717)
*Morten Nissov,Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文介绍了一种基于常见组件和已确立同步方法的实时低成本同步系统，实现高低速传感器触发及同步。


<details>
  <summary>Details</summary>
Motivation: 机器人高精度估计算法需要准确的数据时间戳，但时间戳的不准确性影响线上估计性能，目前缺乏经济高效的实时时间同步方案。

Method: 提出一种利用常见组件和同步方法的多功能系统，实现多种传感器的同步和触发。

Result: 系统成功展示了对高低速传感器的同步和触发能力。

Conclusion: 该系统为实时且低成本的时间戳同步提供了有效解决方案，适用于机器人等需要高精度时间同步的场景。

Abstract: High fidelity estimation algorithms for robotics require accurate data.
However, timestamping of sensor data is a key issue that rarely receives the
attention it deserves. Inaccurate timestamping can be compensated for in
post-processing but is imperative for online estimation. Simultaneously, even
online mitigation of timing issues can be achieved through a relaxation of the
tuning parameters from their otherwise more performative optimal values, but at
a detriment to performance. To address the need for real-time, low-cost
timestamping, a versatile system which utilizes readily-available components
and established methods for synchronization is introduced. The synchronization
and triggering (of both high- and low-rate sensors) capabilities of the system
are demonstrated.

</details>


### [153] [A Learning-based Planning and Control Framework for Inertia Drift Vehicles](https://arxiv.org/abs/2507.05748)
*Bei Zhou,Zhouheng Li,Lei Xie,Hongye Su,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化的学习规划与控制框架，实现了惯性漂移的平滑过渡和精准路径跟踪。


<details>
  <summary>Details</summary>
Motivation: 惯性漂移作为连续漂移阶段之间的过渡动作，能帮助自动驾驶赛车应对连续急弯，但因车辆动力学复杂和环境变化使得控制困难。

Method: 利用贝叶斯优化进行规划逻辑设计，确保惯性漂移与持续漂移阶段的平滑过渡和最小速度损失；并通过性能驱动的控制策略学习缓解模型误差。

Result: 仿真结果表明，提出的方法能在8字形轨迹上实现平稳、稳定的惯性漂移。

Conclusion: 所提框架有效解决了惯性漂移快速切换过程中的控制难题，提升了自动赛车的漂移能力和路径跟踪精度。

Abstract: Inertia drift is a transitional maneuver between two sustained drift stages
in opposite directions, which provides valuable insights for navigating
consecutive sharp corners for autonomous racing.However, this can be a
challenging scenario for the drift controller to handle rapid transitions
between opposing sideslip angles while maintaining accurate path tracking.
Moreover, accurate drift control depends on a high-fidelity vehicle model to
derive drift equilibrium points and predict vehicle states, but this is often
compromised by the strongly coupled longitudinal-lateral drift dynamics and
unpredictable environmental variations. To address these challenges, this paper
proposes a learning-based planning and control framework utilizing Bayesian
optimization (BO), which develops a planning logic to ensure a smooth
transition and minimal velocity loss between inertia and sustained drift
phases. BO is further employed to learn a performance-driven control policy
that mitigates modeling errors for enhanced system performance. Simulation
results on an 8-shape reference path demonstrate that the proposed framework
can achieve smooth and stable inertia drift through sharp corners.

</details>


### [154] [LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving](https://arxiv.org/abs/2507.05754)
*Yuhang Zhang,Jiaqi Liu,Chengkai Xu,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出了一种结合模仿学习端到端框架与大语言模型增强的城市自动驾驶架构LeAD，提升复杂场景的理解与决策能力。


<details>
  <summary>Details</summary>
Motivation: 当前城市自动驾驶系统难以有效理解复杂交通语义信息和判断其他参与者意图，导致决策与人类驾驶员推理不一致。

Method: 提出双速率架构LeAD，高频端到端系统保证实时感知规划控制，低频大语言模型通过多模态感知融合和链式推理提升场景理解与决策优化。

Result: 在CARLA模拟器上，LeAD优于现有方法，异常场景表现优异，Leaderboard V1 得分71，路线完成率达93%。

Conclusion: LeAD通过结合端到端学习与大语言模型增强，有效提升城市自动驾驶在复杂和边缘场景下的性能，实现更接近人类驾驶的决策能力。

Abstract: A principal barrier to large-scale deployment of urban autonomous driving
systems lies in the prevalence of complex scenarios and edge cases. Existing
systems fail to effectively interpret semantic information within traffic
contexts and discern intentions of other participants, consequently generating
decisions misaligned with skilled drivers' reasoning patterns. We present LeAD,
a dual-rate autonomous driving architecture integrating imitation
learning-based end-to-end (E2E) frameworks with large language model (LLM)
augmentation. The high-frequency E2E subsystem maintains real-time
perception-planning-control cycles, while the low-frequency LLM module enhances
scenario comprehension through multi-modal perception fusion with HD maps and
derives optimal decisions via chain-of-thought (CoT) reasoning when baseline
planners encounter capability limitations. Our experimental evaluation in the
CARLA Simulator demonstrates LeAD's superior handling of unconventional
scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route
completion of 93%.

</details>


### [155] [Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments](https://arxiv.org/abs/2507.05861)
*Woonsang Kang,Joohyung Lee,Seungjun Kim,Jungchan Cho,Yoonseon Oh*

Main category: cs.RO

TL;DR: 本文提出了一种模块化的联邦学习框架，针对抓取姿态检测中的通信开销问题，提高了通信效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 抓取姿态检测虽然是机器人自主能力的基础，但依赖大规模数据集带来了数据隐私和中心化难题。传统联邦学习通信开销大，限制了资源受限机器人的应用。

Method: 通过分析模型各功能模块的学习动态，识别收敛较慢的模块，采用两阶段训练：先全模型训练，再仅针对收敛慢模块进行高效通信和聚合，减少通信负担。

Result: 在GraspNet-1B数据集和真实机器人实验中，该方法在通信预算相同时，准确率高于FedAvg及其他基线，且在复杂环境下抓取成功率更高。

Conclusion: 该模块化联邦学习框架有效提升了抓取姿态检测的通信效率和模型表现，实现了分散训练中通信代价与性能的良好平衡。

Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy,
but its reliance on large, diverse datasets creates significant data privacy
and centralization challenges. Federated Learning (FL) offers a
privacy-preserving solution, but its application to GPD is hindered by the
substantial communication overhead of large models, a key issue for
resource-constrained robots. To address this, we propose a novel module-wise FL
framework that begins by analyzing the learning dynamics of the GPD model's
functional components. This analysis identifies slower-converging modules, to
which our framework then allocates additional communication effort. This is
realized through a two-phase process: a standard full-model training phase is
followed by a communication-efficient phase where only the identified subset of
slower-converging modules is trained and their partial updates are aggregated.
Extensive experiments on the GraspNet-1B dataset demonstrate that our method
outperforms standard FedAvg and other baselines, achieving higher accuracy for
a given communication budget. Furthermore, real-world experiments on a physical
robot validate our approach, showing a superior grasp success rate compared to
baseline methods in cluttered scenes. Our work presents a
communication-efficient framework for training robust, generalized GPD models
in a decentralized manner, effectively improving the trade-off between
communication cost and model performance.

</details>


### [156] [Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data](https://arxiv.org/abs/2507.05884)
*Chang Liu,Zhexiong Xue,Tamas Sziranyi*

Main category: cs.RO

TL;DR: 本文比较评估了多种路径规划算法在基于高分辨率卫星影像和机载LiDAR数据生成的加权像素级路网上的表现，结果显示Dijkstra算法在2D和3D路径规划中表现最稳定高效。


<details>
  <summary>Details</summary>
Motivation: 无人驾驶车辆在非结构化环境（如森林和山区）导航面临地形复杂和道路状况多变的挑战，需评估不同路径规划算法在高精度地理信息数据上的性能。

Method: 采用A*、Dijkstra、RRT*及新改进蚁群算法（NIACO）在2D卫星影像数据上测试路径规划性能；在3D机载LiDAR数据上测试3D A*、3D Dijkstra、RRT-Connect和NIACO的表现，比较路径代价、计算时间和内存消耗。

Result: Dijkstra算法在2D和3D两种情况下均表现出最高的稳定性和效率，特别是在密集的像素级地理路网上表现优越。

Conclusion: 基于Dijkstra的路径规划算法在静态地形导航中可靠性高，为未来复杂环境动态路径规划的研究奠定基础。

Abstract: Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.

</details>


### [157] [FineGrasp: Towards Robust Grasping for Delicate Objects](https://arxiv.org/abs/2507.05978)
*Yun Du,Mengao Zhao,Tianwei Lin,Yiwei Jin,Chaodong Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: 提出了一种针对小物体和精细部件抓取的新方法FineGrasp，通过网络改进、标签归一化和混合仿真训练有效提升抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有抓取方法难以对小物体或精细部件生成可行的抓取姿态，导致抓取失败。

Method: 提出FineGrasp方法，包括网络结构改进以处理精细区域，标签归一化策略解决标签不平衡问题，并引入新的仿真抓取数据集，采用混合仿真到真实训练。

Result: 实验结果显示FineGrasp在抓取小型物体方面有显著提升，验证了在语义抓取中的有效性。

Conclusion: FineGrasp有效提升了机器人对小物体和精细部件的抓取能力，推动了语义驱动机器人抓取的发展。

Abstract: Recent advancements in robotic grasping have led to its integration as a core
module in many manipulation systems. For instance, language-driven semantic
segmentation enables the grasping of any designated object or object part.
However, existing methods often struggle to generate feasible grasp poses for
small objects or delicate components, potentially causing the entire pipeline
to fail. To address this issue, we propose a novel grasping method, FineGrasp,
which introduces improvements in three key aspects. First, we introduce
multiple network modifications to enhance the ability of to handle delicate
regions. Second, we address the issue of label imbalance and propose a refined
graspness label normalization strategy. Third, we introduce a new simulated
grasp dataset and show that mixed sim-to-real training further improves grasp
performance. Experimental results show significant improvements, especially in
grasping small objects, and confirm the effectiveness of our system in semantic
grasping.

</details>


### [158] [AURA-CVC: Autonomous Ultrasound-guided Robotic Assistance for Central Venous Catheterization](https://arxiv.org/abs/2507.05979)
*Deepak Raina,Lidia Al-Zogbi,Brian Teixeira,Vivek Singh,Ankur Kapoor,Thorsten Fleiter,Muyinatu A. Lediju Bell,Vinciya Pandian,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一套基于机器人和超声引导的中心静脉穿刺(CVC)全流程系统，实现从扫描初始化到针头插入的自主操作，并在高精度仿真模型上验证了其高成功率和精准度。


<details>
  <summary>Details</summary>
Motivation: 中心静脉穿刺操作复杂，需持续的超声引导，受解剖变异和操作者技术影响大，且针头误插可能导致严重并发症。现有机器人系统尚未实现完全自主操作。

Method: 使用深度学习模型从患者颈部深度图像中定位解剖标志，自动确定扫描区域及路径，结合机器人运动规划进行血管扫描、分割和重建，定位最佳穿刺点，最终结合超声引导和操作者反馈完成针头插入。

Result: 系统在10个模拟临床场景中首次尝试均成功穿刺，血管重建误差平均2.15毫米，自主针头插入误差接近1毫米。

Conclusion: 这是首个在高保真仿真模型上展示的集规划、扫描和插入于一体的机器人中心静脉穿刺系统，实验结果表明其具备临床应用潜力。

Abstract: Purpose: Central venous catheterization (CVC) is a critical medical procedure
for vascular access, hemodynamic monitoring, and life-saving interventions. Its
success remains challenging due to the need for continuous ultrasound-guided
visualization of a target vessel and approaching needle, which is further
complicated by anatomical variability and operator dependency. Errors in needle
placement can lead to life-threatening complications. While robotic systems
offer a potential solution, achieving full autonomy remains challenging. In
this work, we propose an end-to-end robotic-ultrasound-guided CVC pipeline,
from scan initialization to needle insertion. Methods: We introduce a
deep-learning model to identify clinically relevant anatomical landmarks from a
depth image of the patient's neck, obtained using RGB-D camera, to autonomously
define the scanning region and paths. Then, a robot motion planning framework
is proposed to scan, segment, reconstruct, and localize vessels (veins and
arteries), followed by the identification of the optimal insertion zone.
Finally, a needle guidance module plans the insertion under ultrasound guidance
with operator's feedback. This pipeline was validated on a high-fidelity
commercial phantom across 10 simulated clinical scenarios. Results: The
proposed pipeline achieved 10 out of 10 successful needle placements on the
first attempt. Vessels were reconstructed with a mean error of 2.15
\textit{mm}, and autonomous needle insertion was performed with an error less
than or close to 1 \textit{mm}. Conclusion: To our knowledge, this is the first
robotic CVC system demonstrated on a high-fidelity phantom with integrated
planning, scanning, and insertion. Experimental results show its potential for
clinical translation.

</details>


### [159] [Robust Speech-Workload Estimation for Intelligent Human-Robot Systems](https://arxiv.org/abs/2507.05985)
*Julian Fortune,Julie A. Adams,Jamison Heard*

Main category: cs.RO

TL;DR: 本文提出了一种实时估计语音工作负荷的算法，以改善高需求任务环境中的操作性能。


<details>
  <summary>Details</summary>
Motivation: 高需求任务环境中，操作员的工作负荷状态波动影响任务表现，需实时监测不同工作负荷成分以适应系统交互。

Method: 设计并实现了一种用于实时估计语音工作负荷的算法，分析其准确性并验证其在不同个体和人机合作范式中的通用性。

Result: 算法准确有效，能够实时估计语音工作负荷，适用于多种个体和人机团队环境。

Conclusion: 实时语音工作负荷估计是实现适应性人机系统的关键，有助于提高任务执行效率和准确性。

Abstract: Demanding task environments (e.g., supervising a remotely piloted aircraft)
require performing tasks quickly and accurately; however, periods of low and
high operator workload can decrease task performance. Intelligent modulation of
the system's demands and interaction modality in response to changes in
operator workload state may increase performance by avoiding undesirable
workload states. This system requires real-time estimation of each workload
component (i.e., cognitive, physical, visual, speech, and auditory) to adapt
the correct modality. Existing workload systems estimate multiple workload
components post-hoc, but few estimate speech workload, or function in
real-time. An algorithm to estimate speech workload and mitigate undesirable
workload states in real-time is presented. An analysis of the algorithm's
accuracy is presented, along with the results demonstrating the algorithm's
generalizability across individuals and human-machine teaming paradigms.
Real-time speech workload estimation is a crucial element towards developing
adaptive human-machine systems.

</details>


### [160] [SCCRUB: Surface Cleaning Compliant Robot Utilizing Bristles](https://arxiv.org/abs/2507.06053)
*Jakub F. Kowalewski,Keeyon Hajjafar,Alyssa Ugent,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 本文展示了软机器人臂通过扭矩和压力清洁表面粘附残留物的方法，实现了高效安全的擦洗任务。


<details>
  <summary>Details</summary>
Motivation: 传统硬式机器人虽然能施加足够的擦洗力，但因安全性限制需在隔离环境工作；软机器人安全性高，但难以产生持续扭矩和横向力以完成擦洗任务。

Method: 训练神经网络学习软臂的逆运动学和弹性，实现开环力和位置控制，使软臂能够施加连续的扭矩和压力进行擦洗。

Result: 软臂成功去除盘子上的烧焦食物残渣和马桶座上的粘性果酱，平均除污率达99.7%。

Conclusion: 软机器人能够安全有效地施加持续扭矩，用于复杂表面污染的擦洗，展示了软机器人在实际清洁应用中的潜力。

Abstract: Scrubbing surfaces is a physically demanding and time-intensive task.
Removing adhered contamination requires substantial friction generated through
pressure and torque or high lateral forces. Rigid robotic manipulators, while
capable of exerting these forces, are usually confined to structured
environments isolated from humans due to safety risks. In contrast, soft robot
arms can safely work around humans and adapt to environmental uncertainty, but
typically struggle to transmit the continuous torques or lateral forces
necessary for scrubbing. Here, we demonstrate a soft robotic arm scrubbing
adhered residues using torque and pressure, a task traditionally challenging
for soft robots. We train a neural network to learn the arm's inverse
kinematics and elasticity, which enables open-loop force and position control.
Using this learned model, the robot successfully scrubbed burnt food residue
from a plate and sticky fruit preserve from a toilet seat, removing an average
of 99.7% of contamination. This work demonstrates how soft robots, capable of
exerting continuous torque, can effectively and safely scrub challenging
contamination from surfaces.

</details>


### [161] [Learning-Augmented Model-Based Multi-Robot Planning for Time-Critical Search and Inspection Under Uncertainty](https://arxiv.org/abs/2507.06129)
*Abhish Khanal,Joseph Prince Mathew,Cameron Nowzari,Gregory J. Stein*

Main category: cs.RO

TL;DR: 本文提出了一种基于图神经网络和模型规划的多机器人协同搜索框架，用于灾害响应中优先检查可能需要紧急处理的位置，提高了搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 灾害响应和监控中快速识别需要紧急关注的区域非常关键，但部署响应团队到每个地点效率低且常不可行，因此需要多机器人高效协同，优先检查潜在重点区域，减少旅行时间。

Method: 利用图神经网络从噪声传感器数据中估计兴趣点(POIs)需要关注的可能性，并基于此预测指导多机器人模型规划器，生成成本效益高的检查路线。

Result: 仿真实验显示相较于传统和学习基线，所提规划方法在1、3、5机器人配置下分别提升性能16.3%、26.7%和26.2%；此外还在四旋翼无人机平台上进行了真实验证。

Conclusion: 该研究展示了结合图神经网络与多机器人模型规划能显著提升灾害响应中多机器人搜索与检测的效率和效果，具备实际应用潜力。

Abstract: In disaster response or surveillance operations, quickly identifying areas
needing urgent attention is critical, but deploying response teams to every
location is inefficient or often impossible. Effective performance in this
domain requires coordinating a multi-robot inspection team to prioritize
inspecting locations more likely to need immediate response, while also
minimizing travel time. This is particularly challenging because robots must
directly observe the locations to determine which ones require additional
attention. This work introduces a multi-robot planning framework for
coordinated time-critical multi-robot search under uncertainty. Our approach
uses a graph neural network to estimate the likelihood of PoIs needing
attention from noisy sensor data and then uses those predictions to guide a
multi-robot model-based planner to determine the cost-effective plan. Simulated
experiments demonstrate that our planner improves performance at least by
16.3\%, 26.7\%, and 26.2\% for 1, 3, and 5 robots, respectively, compared to
non-learned and learned baselines. We also validate our approach on real-world
platforms using quad-copters.

</details>


### [162] [Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling](https://arxiv.org/abs/2507.06149)
*Charles Champagne Cossette,Taylor Scott Clawson,Andrew Feit*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯分布轨迹的动态物体碰撞概率估计算法，具有快速且误差低的优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略了碰撞概率的时间相关性，导致碰撞概率被高估，因此需要提出一种能够有效考虑时间相关性的算法。

Method: 提出了一种自适应sigma点采样方案，利用动态物体的位置序列和高斯分布，快速估计碰撞概率，显著降低计算误差和时间复杂度。

Result: 算法在Intel Xeon Gold 6226R上实现中位误差为3.5%，中位运行时长为0.21毫秒，并在400个6秒自动驾驶日志片段的真实场景中验证了准确性与实时性。

Conclusion: 该算法简单高效，能够准确考虑碰撞概率的时间依赖性，适用于实际自动驾驶等领域中动态物体碰撞概率的估计。

Abstract: A novel algorithm is presented for the estimation of collision probabilities
between dynamic objects with uncertain trajectories, where the trajectories are
given as a sequence of poses with Gaussian distributions. We propose an
adaptive sigma-point sampling scheme, which ultimately produces a fast, simple
algorithm capable of estimating the collision probability with a median error
of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold
6226R Processor. Importantly, the algorithm explicitly accounts for the
collision probability's temporal dependence, which is often neglected in prior
work and otherwise leads to an overestimation of the collision probability.
Finally, the method is tested on a diverse set of relevant real-world
scenarios, consisting of 400 6-second snippets of autonomous vehicle logs,
where the accuracy and latency is rigorously evaluated.

</details>


### [163] [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
*William Li,Lei Hamilton,Kaise Al-natour,Sanjeev Mohindra*

Main category: cs.RO

TL;DR: 本文评估了大型语言模型在Meta PARTNER基准上的机器人任务解决能力，发现推理型模型表现优于非推理型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在具身机器人任务中的表现，以推动具身机器人领域的发展。

Method: 利用Meta PARTNER基准中的随机厨房场景进行实验，比较了多种前沿模型在不同观测配置下的表现。

Result: 推理模型OpenAI o3-mini在所有测试配置中均优于非推理模型GPT-4o和Llama 3。

Conclusion: 推理型大型语言模型在具身机器人任务中表现更佳，显示了推动该领域研究的潜力。

Abstract: This paper focuses on evaluating the effectiveness of Large Language Models
at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR
provides simplified environments and robotic interactions within randomized
indoor kitchen scenes. Each randomized kitchen scene is given a task where two
robotic agents cooperatively work together to solve the task. We evaluated
multiple frontier models on Meta PARTNER environments. Our results indicate
that reasoning models like OpenAI o3-mini outperform non-reasoning models like
OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied
environments. o3-mini displayed outperform across centralized, decentralized,
full observability, and partial observability configurations. This provides a
promising avenue of research for embodied robotic development.

</details>


### [164] [Learning Agile Tensile Perching for Aerial Robots from Demonstrations](https://arxiv.org/abs/2507.06172)
*Kangle Yuan,Atar Babgei,Luca Romanello,Hai-Nguyen Nguyen,Ronald Clark,Mirko Kovac,Sophie F. Armanini,Basaran Bahadir Kocer*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的轨迹控制框架，实现无人机通过带有绳索的张力附着机制准确附着在多种结构上。


<details>
  <summary>Details</summary>
Motivation: 无人机通过附着在结构上延长续航时间，但传统张力附着机制建模复杂，需精确控制飞行动力学和绳索动力。

Method: 采用带示范的软行为者评论家算法（SACfD），结合最佳与次优示范，提高训练效率，实现位置和速度的精确控制，从而准确锁定绳索特定位段并完成缠绕固定。

Result: 通过大量仿真和实地实验验证，框架能够实现灵活且可靠的轨迹生成，确保张力附着的成功。

Conclusion: 所提轨迹框架有效解决了张力附着的复杂动力学问题，提升了无人机附着的操控精准性和稳定性。

Abstract: Perching on structures such as trees, beams, and ledges is essential for
extending the endurance of aerial robots by enabling energy conservation in
standby or observation modes. A tethered tensile perching mechanism offers a
simple, adaptable solution that can be retrofitted to existing robots and
accommodates a variety of structure sizes and shapes. However, tethered tensile
perching introduces significant modelling challenges which require precise
management of aerial robot dynamics, including the cases of tether slack &
tension, and momentum transfer. Achieving smooth wrapping and secure anchoring
by targeting a specific tether segment adds further complexity. In this work,
we present a novel trajectory framework for tethered tensile perching,
utilizing reinforcement learning (RL) through the Soft Actor-Critic from
Demonstrations (SACfD) algorithm. By incorporating both optimal and suboptimal
demonstrations, our approach enhances training efficiency and responsiveness,
achieving precise control over position and velocity. This framework enables
the aerial robot to accurately target specific tether segments, facilitating
reliable wrapping and secure anchoring. We validate our framework through
extensive simulation and real-world experiments, and demonstrate effectiveness
in achieving agile and reliable trajectory generation for tensile perching.

</details>


### [165] [Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model](https://arxiv.org/abs/2507.06174)
*Koki Yamane,Yunhan Li,Masashi Konosu,Koki Inami,Junji Oaki,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本论文提出利用4通道双边控制实现具备力反馈的低成本机械臂快速远程操作，并通过集成非线性补偿、速度及外力估计和可变增益技术提升性能。同时，结合力反馈信息的模仿学习策略表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有远程操作多依赖单边控制，缺乏力反馈，难以胜任快速和接触丰富的任务。本文旨在克服这一局限，实现低成本机械臂的高性能远程操作。

Method: 基于准确的机械臂动力学辨识，采用4通道双边控制，集成非线性项补偿、速度与外力估计及与惯量变化对应的可变增益，并将力反馈信息纳入模仿学习策略的输入和输出。

Result: 证明了即使在无力传感器的低成本机械臂上，利用4通道双边控制也能实现带力反馈的快速远程操作；模仿学习中加入力反馈显著提升策略表现。

Conclusion: 本方法实验证明了结合精确动力学和力反馈的4通道双边控制具备实际应用价值，可以在经济实惠的硬件上实现高保真远程操控和示教数据采集。

Abstract: In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.

</details>


### [166] [Is Diversity All You Need for Scalable Robotic Manipulation?](https://arxiv.org/abs/2507.06219)
*Modi Shi,Li Chen,Jin Chen,Yuxiang Lu,Chiming Liu,Guanghui Ren,Ping Luo,Di Huang,Maoqing Yao,Hongyang Li*

Main category: cs.RO

TL;DR: 本论文研究了数据多样性在机器人操作学习中影响数据扩展效果的作用，挑战了“多样性越多越好”的传统观点。


<details>
  <summary>Details</summary>
Motivation: 尽管在NLP和CV领域数据扩展带来了成功，但机器人操作领域中数据扩展原理尚未被充分理解，特别是任务、机器人类型和示范者多样性三方面的影响。

Method: 通过在不同机器人平台上进行大量实验，分析任务多样性、机器人多样性及专家多样性对模型迁移和学习性能的影响，并提出了基于速度模态去偏方法以缓解学习中的速度歧义问题。

Result: 发现任务多样性比单任务示范数更关键，单一高质量机器人示范数据能更高效地实现跨机器人平台迁移，多样化专家示范存在干扰，针对速度多模态的去偏方法提升性能15%，相当于使用2.5倍预训练数据。

Conclusion: 本研究揭示了机器人操作中有效数据规模化的关键因素，为数据采集和模型训练提供了新的视角和实践指导。

Abstract: Data scaling has driven remarkable success in foundation models for Natural
Language Processing (NLP) and Computer Vision (CV), yet the principles of
effective data scaling in robotic manipulation remain insufficiently
understood. In this work, we investigate the nuanced role of data diversity in
robot learning by examining three critical dimensions-task (what to do),
embodiment (which robot to use), and expert (who demonstrates)-challenging the
conventional intuition of "more diverse is better". Throughout extensive
experiments on various robot platforms, we reveal that (1) task diversity
proves more critical than per-task demonstration quantity, benefiting transfer
from diverse pre-training tasks to novel downstream scenarios; (2)
multi-embodiment pre-training data is optional for cross-embodiment
transfer-models trained on high-quality single-embodiment data can efficiently
transfer to different platforms, showing more desirable scaling property during
fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,
arising from individual operational preferences and stochastic variations in
human demonstrations, can be confounding to policy learning, with velocity
multimodality emerging as a key contributing factor. Based on this insight, we
propose a distribution debiasing method to mitigate velocity ambiguity, the
yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to
using 2.5 times pre-training data. Collectively, these findings provide new
perspectives and offer practical guidance on how to scale robotic manipulation
datasets effectively.

</details>


### [167] [EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://arxiv.org/abs/2507.06224)
*Yixiang Chen,Peiyan Li,Yan Huang,Jiabing Yang,Kehan Chen,Liang Wang*

Main category: cs.RO

TL;DR: EC-Flow通过利用机器人本体运动学从无标签视频中学习操作，实现对变形物体、遮挡和非位移任务的强大泛化能力，在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动的机器人操作系统依赖低层动作标注数据，且受限于刚性物体场景，难以处理复杂操作如变形物体和遮挡问题。

Method: 提出基于本体运动学的EC-Flow框架，预测本体中心流，结合目标对齐模块通过运动一致性和目标图像预测实现语言指令与物体交互的连接，并利用标准URDF文件将预测转换为机器人执行动作。

Result: 在模拟和真实任务中，EC-Flow分别在遮挡物体操作、变形物体操作和非位移任务上比现有方法提升62%、45%和80%。

Conclusion: EC-Flow有效利用机器人本体运动学提升了无标注视频学习的泛化能力，拓展了语言驱动机器人操作的应用场景，具备实际可用性和显著性能优势。

Abstract: Current language-guided robotic manipulation systems often require low-level
action-labeled datasets for imitation learning. While object-centric flow
prediction methods mitigate this issue, they remain limited to scenarios
involving rigid objects with clear displacement and minimal occlusion. In this
work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly
learns manipulation from action-unlabeled videos by predicting
embodiment-centric flow. Our key insight is that incorporating the embodiment's
inherent kinematics significantly enhances generalization to versatile
manipulation scenarios, including deformable object handling, occlusions, and
non-object-displacement tasks. To connect the EC-Flow with language
instructions and object interactions, we further introduce a goal-alignment
module by jointly optimizing movement consistency and goal-image prediction.
Moreover, translating EC-Flow to executable robot actions only requires a
standard robot URDF (Unified Robot Description Format) file to specify
kinematic constraints across joints, which makes it easy to use in practice. We
validate EC-Flow on both simulation (Meta-World) and real-world tasks,
demonstrating its state-of-the-art performance in occluded object handling (62%
improvement), deformable object manipulation (45% improvement), and
non-object-displacement tasks (80% improvement) than prior state-of-the-art
object-centric flow methods. For more information, see our project website at
https://ec-flow1.github.io .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 本文通过基于二进制决策图的符号搜索方法，利用单核CPU和大内存，生成了89.6GB的四子棋查找表，实现了强解。


<details>
  <summary>Details</summary>
Motivation: 虽然四子棋已通过搜索方法数学求解，但实现强解（查找表形式）被认为不可行，本文旨在突破这一限制。

Method: 采用基于二进制决策图的符号搜索，实现了在标准7×6棋盘上的高效强解生成，包含胜负平评估和alpha-beta搜索以确定最快胜利或最慢失败的走法。

Result: 在单核CPU和128GB内存支持下，47小时内成功生成89.6GB的查找表，开放源代码实现相关功能。

Conclusion: 本文证明了基于符号搜索的二进制决策图方法能够生成实用的强解查找表，推动了四子棋AI的发展。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [169] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出了Chat2SPaT方法，利用大语言模型将模糊的交通信号控制计划描述转换为精确的信号阶段和时序结果，实现结构化信号控制计划管理。


<details>
  <summary>Details</summary>
Motivation: 预设时段的交通信号控制方案创建和更新耗费大量人工，且同一路口需多方案管理，工作重复繁琐，亟需简化信号控制方案管理流程。

Method: 设计Chat2SPaT流程，通过精心设计的提示词，运用大语言模型理解用户半结构化描述，输出json格式的相位序列和属性。结合Python脚本处理交通信号控制细节，生成完整控制方案，并支持交互式多轮编辑。

Result: 在包含300多个信号控制方案描述的测试集中，Chat2SPaT在中英文方案生成准确率均超过94%。

Conclusion: Chat2SPaT为交通信号方案管理提供了首个基于大语言模型的评测基准和易用工具，推动智能交通系统领域语言模型的更精准多样化应用。源代码和数据集均已开源。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [170] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 本文证明了在给定条件下模糊分类聚合函数必须为加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 研究多对象多类型模糊分类聚合函数的性质，以确定满足特定条件的函数形态。

Method: 利用数学证明方法，针对连续个体分类，分析满足最优性、独立性和零一致性的聚合函数形式。

Result: 证明了当对象数量大于等于3，类型数量在2到对象数量之间时，任何满足条件的模糊分类聚合函数必定是加权算术平均。

Conclusion: 在多对象多类型的模糊分类聚合问题中，加权算术平均是唯一满足给定优化与一致性条件的聚合函数形式。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [171] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++是一种针对市政和跨司法管辖区法律规则建模的泊义逻辑图扩展，支持更丰富的节点和边类型，增强了规则的表达能力与推理功能。


<details>
  <summary>Details</summary>
Motivation: 现有法律知识图模型在表达法律义务、例外和层级关系时存在局限，需要一个更丰富且结构化的模型来支持复杂法律规则的语义建模和推理。

Method: 提出了OLG++扩展，增加空间、时间、群体、防御性和逻辑分组等节点与边类型，支持带上下文条件、优先级和复杂触发规则的结构化推理，结合属性图查询实现法律问答。

Result: 通过食品商业法规示例展示了OLG++的表达能力，支持复杂法律义务和例外的建模及查询，且相较LegalRuleML和先前模型，具有更高的表达力和原生支持多类语义结构。

Conclusion: OLG++显著提升了法律法规的图模型表达能力和推理性能，是法律知识表达领域优于现有图模型的有效工具。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [172] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 本文提出了Deep Research Comparator平台，用于评估和比较深度研究代理，通过展示报告及中间步骤，结合细粒度人类反馈，提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 目前评估自动深度研究代理的挑战在于难以有效评估长报告及其中间步骤，缺乏细致反馈机制。

Method: 构建一个集成多个深度研究代理的比较平台，支持报告和中间步骤的并排展示，收集详细的人类反馈，并开发Simple Deepresearch作为基础代理框架。

Result: 平台成功收集了17名标注者对三种深度研究代理的真实用户偏好数据，证明其在代理评估和开发中的实用性。

Conclusion: Deep Research Comparator提供了一个全面的框架，显著改善了深度研究代理的评估方式，促进了相关代理的发展和优化。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [173] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文提出了一个针对增强现实(AR)培训的视觉语言模型(VLM)数据集，并评估了九种先进VLM，发现它们在细粒度装配任务上的表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在增强现实培训应用中未得到充分探索，尤其是在细粒度视觉语言理解方面表现不足。

Method: 构建一个系统化的AR培训视觉语言任务数据集，并对九种先进VLM进行评测。

Result: 即使是先进模型如GPT-4o，在状态检测任务中的最高F1分数仅为40.54%，揭示了模型在细粒度视觉语言对齐上的不足。

Conclusion: 该研究强调需要更好的数据集和基准，并推动技术进步，以提升细粒度视觉语言模型性能，同时促进视障用户的公平学习机会。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [174] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 本文提出了使用答案集编程(ASP)中的默认否定和强否定来优雅地表达规范模态逻辑中的模态算子，并采用ASP的全局约束表示义务和禁止，成功解决了规范模态逻辑中的各种悖论。


<details>
  <summary>Details</summary>
Motivation: 规范模态逻辑的实现存在困难，尤其是如何表达模态算子和解决相应的逻辑悖论。

Method: 利用答案集编程中的默认否定和强否定表达模态算子，采用ASP的全局约束来表示义务和禁止。

Result: 该方法能够优雅地表达规范模态逻辑的模态算子，并有效避免了多种规范逻辑悖论。

Conclusion: 基于ASP的表达方式为规范模态逻辑的实现提供了一种简洁且有效的解决方案。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [175] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 本文提出了一种结合多模态模型微调、结构化推理和智能检索增强生成的皮肤病视觉问答方法，在2025 ImageCLEF MEDIQA-MAGIC竞赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 现实远程医疗中诊断需依赖有限输入、高准确性和可解释性，模拟皮肤科医生系统化推理以提高自动诊断支持。

Method: 微调Qwen、Gemma、LLaMA多模态模型，加入结构化推理层整合模型输出，并结合基于美国皮肤病学会数据库的智能检索增强生成技术。

Result: 团队提交的方案获得竞赛第二名且排名第六，表现出高准确性和竞争力。

Conclusion: 该方法为远程医疗中基于图文的自动诊断支持提供了可靠路径，提升了诊断的准确性与可解释性。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [176] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文提出了WikiHowAgent，一个利用大语言模型的多智能体系统，用以模拟师生互动教学对话，支持程序化学习并评估教学质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏可扩展性，难以利用大规模多样化课程内容，且缺少评估教学质量的有效框架。

Method: 设计包含教师智能体、学习者智能体、交互管理器和评估器的多智能体工作流，基于114,296条师生对话数据集进行教学模拟与质量评估，结合计算指标、量表指标及人工判断进行评价。

Result: 该工作流在多种环境下表现出良好的有效性，揭示了大语言模型在跨领域教学任务中的潜力。

Conclusion: WikiHowAgent为基于大语言模型的教育对话系统提供了可扩展且高质量的教学模拟与评估框架，数据集和实现均已开源。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [177] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 本文批判性审视了AI红队实务，指出当前聚焦模型缺陷忽视系统层面和复杂交互的不足，提出了涵盖宏观系统与微观模型的综合红队框架，并强调多功能团队与系统思维的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI红队主要关注模型漏洞，忽视了复杂的社会技术系统及模型、用户、环境间的交互，导致风险评估不全面。

Method: 提出一个涵盖整个AI开发生命周期的宏观系统红队和微观模型红队相结合的综合框架，结合网络安全经验与系统理论，设计多功能团队的建议方案。

Result: 该框架促进从系统层面识别新兴风险和系统性脆弱性，提升AI红队在治理中的有效性。

Conclusion: 有效的AI红队应超越模型层面，关注社会技术系统的复杂互动，采用多学科团队和系统性思维，以更全面地发现和缓解风险。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [178] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 本文利用大型语言模型GPT-4o-mini生成反事实解释，提升了异常预防和模型鲁棒性的能力。


<details>
  <summary>Details</summary>
Motivation: 反事实解释能够以最小变动改变预测结果，为异常预防和增强模型训练提供人性化见解。

Method: 采用零样本和三样本设置下的GPT-4o-mini生成反事实解释，并在压力预测和心脏疾病检测两个数据集上进行评估。

Result: 相比传统方法，LLM生成的反事实解释在合理性、有效性和稀疏性方面表现优异，并且作为数据增强显著提升下游分类器的准确率。

Conclusion: 提示式生成技术在临床和生理预测任务中具有增强解释能力和模型鲁棒性的潜力。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [179] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: 本文提出了SingLoRA，一种基于单低秩矩阵及其转置的参数高效微调方法，解决了传统LoRA中矩阵尺度不匹配导致的不稳定训练问题，显著提升了性能并降低了参数量。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法中两个低秩矩阵存在尺度差异，导致训练过程不稳定，性能受限。

Method: SingLoRA通过以单一低秩矩阵及其转置分解的方式来更新权重，避免了矩阵间的尺度冲突，从而保证训练稳定且参数量减半。

Result: 在多项任务中，SingLoRA表现优异：例如在常识推理任务中，使用SingLoRA微调LLama 7B模型，准确率达91.3%，超过LoRA和LoRA+，且参数仅为其60%；在图像生成任务中，使用SingLoRA微调Stable Diffusion，图像相似度指标显著提升。

Conclusion: SingLoRA通过重新设计低秩适配结构，有效解决了训练不稳定问题，实现更稳定、参数更少且性能更优的微调方案，适用于多种任务。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [180] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了人工智能测量的形式理论，旨在促进对AI系统和评估方法的比较，结合风险分析技术，并强调测量操作对AI能力的影响。


<details>
  <summary>Details</summary>
Motivation: 当前AI测量缺乏统一的理论框架，导致评估方法多样且难以比较，且AI能力定义依赖于测量方式，亟需形式化测量理论。

Method: 作者提出了一个分层的测量结构，区分直接和间接可观测量，构建一个可校准的AI现象分类方法。

Result: 该框架能够使研究者、从业者和监管者更有效地比较AI系统，结合工程和安全科学的风险分析，并明确测量操作对AI能力定义的影响。

Conclusion: 通过形式化的测量理论，可以实现AI系统的统一分类和标准化评估，从而推动AI评估研究和实践的发展。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [181] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的多模态大语言模型MLlm-DR，用于自动抑郁诊断，能够生成抑郁评分及解释，提升了诊断的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动抑郁诊断方法缺乏对评分确定过程的清晰解释，且现有多模态大语言模型缺乏面向访谈数据的训练，导致诊断性能不佳，限制了临床应用。

Method: 设计了一种整合小型大语言模型和轻量查询模块（LQ-former）的多模态大语言模型MLlm-DR。小型LLM负责生成抑郁评分及评估理由，利用构建的坚实训练集对其进行微调以增强逻辑推理能力；LQ-former则从语音和视觉数据中提取抑郁相关特征，实现多模态信息融合。

Result: 在两个基于访谈的基准数据集CMDC和E-DAIC-WOZ上取得了最先进的诊断效果，验证了方法的有效性和优越性。

Conclusion: 所提多模态大语言模型MLlm-DR在自动抑郁诊断中实现了可解释性和高性能，有望推动临床抑郁诊断的应用。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [182] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型(LLMs)在岩土工程领域的适应与应用，包括关键适应方法和实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 尽管通用型大型语言模型功能强大，但在岩土工程领域的有效应用仍需领域特定的调整，以优化工作流程。

Method: 本文总结了岩土领域适应的关键技术，如提示工程、基于检索的生成、领域自适应预训练和微调，并评述了各类实际应用。

Result: 探讨了岩土工程适应型LLMs在地质解释、地下表征、现场规划、设计计算、数值模拟、安全评估和教学等方面的应用效果及其优势与不足。

Conclusion: 该综述为岩土工程师整合LLMs提供指导，也为学术界进一步研究该跨学科领域奠定基础。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [183] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于注意力扩散模型的方法（ADMC），用于多模态情感和意图识别中的缺失模态特征补全，提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感和意图识别面临传感器故障或数据不完整导致的模态缺失问题，传统方法生成的特征往往过度耦合且不准确。

Method: 构建独立训练的特征提取网络以保持各模态特征独立性，利用基于注意力的扩散网络（ADN）生成符合真实分布的缺失模态特征，同时支持跨模态生成以提升全模态识别性能。

Result: 在IEMOCAP和MIntRec两个基准数据集上，方法在缺失模态和完整模态的情感和意图识别任务中均达到了先进水平。

Conclusion: 所提基于注意力扩散的缺失模态特征补全方法有效解决了模态缺失问题，并提升了多模态情感和意图识别的性能。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [184] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 本研究通过实证研究验证了大型语言模型生成的检索练习题能显著提升学生的知识保持率。


<details>
  <summary>Details</summary>
Motivation: 生成高质量的检索练习题费时费力，尤其在技术快速发展的领域，利用大型语言模型实现自动生成具有潜力，但其效果尚未明确。

Method: 在两门大学数据科学课程中，约60名学生参与，比较一周内使用LLM生成的选择题检索练习与无练习的学习结果。

Result: 使用LLM生成的检索练习的学生知识保持率显著提高，平均正确率从73%提升至89%。

Conclusion: LLM生成的检索练习题能有效支持学生学习，具有推广价值，但其质量需教师手动审核以确保准确性。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [185] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的信息传播仿真环境，通过引入情感引导的记忆和社会信息处理链式思考机制（SIP-CoT），提升了模拟中代理人的心理和行为真实感。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和生成式人工智能的快速发展加速了错误信息的传播，传统模型无法充分模拟在线用户的心理与行为动态。当前大语言模型缺乏情感处理，导致模拟行为与真实人类存在较大差距。

Method: 基于社会信息处理理论，设计了基于情绪引导记忆的SIP-CoT机制，使LLM能够更好地理解社会线索、个性化目标设定和反馈评估，从而改善信息传播模拟的心理真实性。

Result: 实验结果表明，采用SIP-CoT机制的LLM代理人在处理社会信息时表现出更贴近真实人类的行为、态度和情感，提升了模拟的准确性和社会智能水平。

Conclusion: 当前基于LLM的信息传播模拟存在显著不足，整合SIP-CoT和情感记忆机制能显著增强模拟的心理真实性和社会智能，推动更有效的信息传播控制研究。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [186] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 本论文提出利用司法数据构建司法绩效指标体系，基于此进行城市级外商直接投资预测。


<details>
  <summary>Details</summary>
Motivation: 传统基于经济数据的外商直接投资预测存在易被操控的问题，需寻求更可靠的数据源。

Method: 构建基于1200万裁判文书的司法绩效指标体系，形成表格数据集，提出融合行列数据和多专家模型的司法数据表格学习方法（TLJD）进行预测。

Result: 在跨城市和跨时间的预测任务中，TLJD方法在多项指标上优于十个先进基线，R2值达到至少0.92。

Conclusion: 利用司法数据进行外商直接投资预测更为可靠且有效，TLJD方法表现优异，具备实际应用价值。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [187] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 本研究比较了皮肤科专家与两种AI模型生成的治疗方案，发现人类评估者偏好专家方案，而高阶AI评估者则更认可AI方案，表现出评估者性质对方案评价的决定性影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI在诊断以外领域的应用扩大，如何评估AI生成的治疗方案成为关键挑战。

Method: 由10位皮肤科医生、一款通用AI（GPT-4o）、及一款推理AI（o3）为五个复杂病例制定方案，后经匿名化评分；专家组与更高级AI (Gemini 2.5 Pro)分别评价这些方案。

Result: 专家组评分中人类方案优于AI方案，且GPT-4o排名优于推理模型o3；AI评分则相反，推理模型最佳，人类方案得分最低。

Conclusion: 方案质量评价强烈依赖评估者性质，经验驱动与数据驱动逻辑存在巨大差异，未来需发展可解释的人机协同系统以弥合此差异，提升临床护理水平。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [188] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: 该论文提出了ModelAuditor，一种能够在临床实践中检测和修复AI模型性能退化的工具。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医疗成像中的表现虽达到专家水平，但在真实世界中受到设备、环境及人群差异影响，性能大幅下降，且现有的可靠性审计过程复杂且费时。

Method: ModelAuditor作为一个自省代理，通过与用户对话选择任务特定指标，并模拟临床相关的分布变化，生成可解释的报告，识别性能下降原因及提出修复建议。

Result: 在三个实际临床案例中，ModelAuditor成功识别了状态模型的失败模式，提出的策略使性能恢复15-25%，优于基线和现有增广方法。

Conclusion: ModelAuditor通过多代理架构，既高效又经济，在临床部署前可有效审计模型性能，提升医疗AI的可靠性和实用性。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [189] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 提出了一种通过充电结束时的放电脉冲分析电池健康状态（SoH）的方法，适合微电网中实时监控电池健康。


<details>
  <summary>Details</summary>
Motivation: 微电网操作限制传统SoH监测方法的使用，需创新实时监测技术。

Method: 利用等效电路模型参数，解析放电脉冲期间端电压变化，训练模型预测SoH。

Result: 在85%容量退化电池参数训练下，预测90% SoH电池，平均绝对误差约1%，解释性评分约0.9。

Conclusion: 该方法若性能稳定，可集成于电池管理系统，实现连续运行下的电池优化管理。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [190] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了GTA1，一个针对跨平台GUI操作的智能代理，通过测试时扩展方法选择最佳动作方案，并结合强化学习提升视觉元素定位精度，显著提高任务完成率，在多个基准测试中表现领先。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在任务规划中的不确定性和在高分辨率复杂界面中精确定位交互目标的两大挑战。

Method: 引入测试时扩展方法，多候选动作采样结合判别模型选择最佳动作，同时利用强化学习优化动作方案与视觉目标的准确匹配。

Result: GTA1在多个基准测试中表现优异，如Screenspot-Pro、Screenspot-V2和OSWorld-G准确率分别达到50.1%、92.4%和67.7%，任务成功率达45.2%。

Conclusion: 测试时扩展和强化学习方法有效提升了GUI代理的决策质量和视觉定位能力，推动了跨平台GUI自动化任务执行的性能边界。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [191] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 文章提出了一个新的中文早产儿视网膜病变(ROP)风险预测数据集CROP，并开发了Affective-ROPTester框架，用以评估大语言模型在ROP风险预测中的表现及情感偏差。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在ROP风险预测方面的应用和表现尚未被充分研究，尤其是情感偏差对预测结果的影响缺乏系统探讨。

Method: 构建包含993条病例的CROP数据集，设计了Instruction、Chain-of-Thought和In-Context Learning三种提示策略，通过集成情感因素的提示方式，系统评估模型的预测能力与情感偏差。

Result: 模型仅凭内在知识预测效果有限，但运用外部结构化医学知识显著提升了准确性。模型存在情感偏差，倾向于高估中高风险病例；正面情感提示有助于减轻这一偏差。

Conclusion: 情感敏感的提示设计对于提升临床诊断的可靠性至关重要，Affective-ROPTester框架有效评估并缓解了临床语言模型中的情感偏差。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [192] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: 本文探讨了人工智能在游戏中的表现与人类直觉决策的差异，提出了基于认知心理学观察的CogniPlay模型。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在多种游戏中表现优异，但缺乏人类模式化、直觉式决策过程。

Method: 综述认知心理学和相关人工智能行为建模的研究，并基于此提出CogniPlay模型。

Result: 提出了一个基于认知心理学的人工智能模型CogniPlay，用以更好模拟人类行为。

Conclusion: CogniPlay模型有望弥合现有AI与人类直觉决策之间的差距，提高通用游戏人工智能的“人性化”水平。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [193] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 本文提出了神经符号转换系统这一计算模型，旨在结合传统符号算法和大型语言模型，提升自动推理工具的性能与保障。


<details>
  <summary>Details</summary>
Motivation: 当前构建神经符号自动推理系统多为临时性的程序设计模型，缺乏传统符号算法的强保障，也未能充分结合神经网络与符号推理，限制了LLM的推理潜力发挥。

Method: 提出神经符号转换系统，在该模型中，符号状态与“直觉”并行处理，状态转换同时操作符号和直觉，实现神经符号推理的紧密结合。

Result: 该模型能扩展逻辑推理能力，同时保留符号算法的强保障，且有潜力以逻辑编程语言实现。

Conclusion: 神经符号转换系统为构建具备强保障和高效推理能力的神经符号自动推理工具提供了坚实的计算模型基础。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [194] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 本文针对时间序列预测中序列表示、信息提取和目标投射三大核心阶段，评估了多种模块架构，在七个基准数据集上实现了最先进的预测精度和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临序列表示有效性、内存构建及精确预测目标的挑战，不同任务对模型设计提出多样需求。

Method: 将时间序列预测流程分解为输入表示、信息提取与内存构建、目标投射三阶段，分别评估卷积层和自注意力机制等多种架构配置。

Result: 所提模型在七个基准数据集上取得了最先进的预测准确度，并显著降低了训练和推理时间及模型参数数量。

Conclusion: 系统地分解并优化时间序列预测的核心阶段能同时提升预测精度与计算效率，验证了模块化设计优势。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [195] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 本文提出了MusiScene模型，用于根据音乐生成与之匹配的视频场景描述，实现音乐场景想象（MSI）任务。


<details>
  <summary>Details</summary>
Motivation: 人类听音乐时会联想到相应的电影场景，现有音乐字幕模型只关注音乐本身，缺少跨模态场景联想能力。

Method: 构建了一个包含3,371个视频-音频对的大规模数据集，基于Music Understanding LLaMA微调训练MusiScene模型，并对模型进行全面评估。

Result: MusiScene在生成的音乐场景描述上表现出比MU-LLaMA更好的上下文相关性。同时利用生成的字幕提升了基于文本的视频背景音乐生成。

Conclusion: 通过跨模态学习，MusiScene有效实现了音乐场景想象，增强了音乐与视频的结合，拓展了音乐语言模型的应用。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [196] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: BlueLM-2.5-3B是一款紧凑且统一的3B参数级多模态大型语言模型，支持思考和非思考模式，适合边缘设备高效部署，具备强大的多模态与文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一款体积小、性能强大且适合边缘设备部署的多模态大型语言模型，解决现有模型对设备资源要求高、对思考过程控制不足的问题。

Method: 通过多元化数据筛选、关键数据重采样、混合异构强化学习以及高性能训练设施等技术手段训练模型，实现思考与非思考模式并支持思考令牌预算控制。

Result: BlueLM-2.5-3B在多模态和纯文本任务中表现优异，思考模式下性能接近更大规模模型，非思考模式超过同规模竞品，且训练数据需求显著减少，表现出优异的数据效率。

Conclusion: BlueLM-2.5-3B成功实现了高性能、低资源消耗的边缘部署多模态语言模型，推动了高效本地多模态智能的发展，具有重要的研究和应用价值。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [197] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 文章提出了一种统一的基础模型，利用因果Transformer和补丁掩码策略，实现多任务无线网络参数预测，支持多样的预测间隔，具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 无线通信网络日益复杂，准确预测关键参数如CSI、用户位置和网络流量对物理层和MAC层任务至关重要，而传统深度学习方法难以跨场景和任务泛化。

Method: 提出基于因果Transformer的统一基础模型，采用单变量分解统一任务，编码时间间隔粒度，并引入补丁掩码策略支持任意输入长度。

Result: 在大规模数据集训练后，模型在未知场景表现优异，实现了新任务的零样本预测，超过传统全样本基线。

Conclusion: 该基础模型有效提升了无线网络多任务预测的泛化能力和准确性，适用于多样预测需求，推动无线网络参数预测技术发展。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [198] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 本文提出了一种基于属性归因的方法，提升基于可解释人工智能模型的乳腺癌淋巴结放疗后手臂淋巴水肿风险评估的模型解释性。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的人工智能技术缺乏透明度，限制了其在医疗决策中的接受和解释。

Method: 通过使用信息检索中的标准指标，对规则基础的预测模型中的属性进行统计分析，计算每个属性对预测的相关性，提供用户可解释的风险因子影响信息。

Result: 用户研究显示，所提出的方法相比原始的可解释AI模型输出，提供了更高的解释性和实用性。

Conclusion: 基于属性归因的分析方法能够提升医疗场景中可解释人工智能模型的清晰度和用户接受度，有助于淋巴水肿风险预测的理解和应用。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [199] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: HopeBot是一款基于大型语言模型的聊天机器人，通过互动和实时澄清进行抑郁症筛查，研究显示其评分与传统PHQ-9高度一致，用户信任度高且使用体验良好。


<details>
  <summary>Details</summary>
Motivation: 传统静态问卷如PHQ-9有效筛查抑郁症但缺乏互动性和适应性，亟需更具交互性的工具。

Method: 开发HopeBot聊天机器人，结合大型语言模型、检索增强生成与实时澄清技术，进行PHQ-9问卷的口语化管理，并在英国和中国132名成人中进行对比研究。

Result: HopeBot评分与传统自填问卷高度一致（ICC=0.91），71%参与者更信任聊天机器人，舒适度、语音清晰度及敏感话题处理均获得较高评分，87.1%表现出愿意重复使用或推荐。

Conclusion: 基于大型语言模型的语音聊天机器人能够作为抑郁症常规筛查的可行、低负担且可扩展的辅助工具。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [200] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的文本到SQL转换框架CogniSQL-R1-Zero，实现了更高的执行准确率，尤其在复杂查询中表现优异，并发布了两个辅助数据集。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL的转换在生成正确且可执行的复杂SQL查询方面仍存在挑战，传统方法依赖中间监督和复杂奖励机制，训练成本高且效果有限。

Method: 提出利用基于执行正确性和格式标签的轻量级奖励信号进行强化学习，避免中间监督和复杂奖励设计，促进学习的稳定性和模型与任务目标的对齐。

Result: 在Text2SQL基准测试BIRD上，CogniSQL-R1-Zero超越了包括SFT CodeS-7B、DeepSeek-Coder 236B和Mistral 123B在内的现有模型，且仅用较小的7B模型骨干和4块NVIDIA A100 GPU训练。

Conclusion: 强化学习方法证明了在文本到SQL领域的效率和扩展性，同时发布的数据集可推动高效且可解释的文本到SQL研究，促进可执行程序生成的稳定与准确。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [201] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: 本文提出FGNS方法，通过结合局部和全局特征重要性选择具有代表性的邻居实例，提升模型解释的可理解度和用户对模型错误的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释人工智能（XAI）方法难以为非领域专家用户生成清晰和易理解的解释，导致用户难以有效识别模型错误。

Method: 提出了一种后置处理方法——特征引导邻居选择（FGNS），该方法结合局部与全局特征重要性指标，选择更具类别代表性的样本作为邻居而非单纯基于特征空间距离。

Result: 用户研究显示，FGNS显著提升了非专家对模型错误的识别能力，且决策速度和准确率优于传统k-NN解释。定量分析表明FGNS所选邻居更一致且紧密聚类于类别原型。

Conclusion: FGNS促进了更加符合人类认知的模型评估方式，但还需进一步工作以缩小解释质量与用户信任感之间的差距。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [202] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 本文在Lockean论点框架下，研究了置信度确定的信念集合，提出了使信念集在经典逻辑下封闭的新方法，并设计了概率更新的最小修正策略。


<details>
  <summary>Details</summary>
Motivation: 传统的Lockean信念集合在某些场合（如信念变化理论）中不满足经典逻辑推理封闭性，限制了其应用。

Method: 本文提供了两种信念集合经典逻辑封闭的刻画方法，并提出了一种基于概率更新的最小修正策略，使信念集在接受新信息时最少变动并保持逻辑封闭。

Result: 成功构建了可实现经典逻辑封闭的信念集合模型，并通过最小修正概率更新方法实现信念变更，保证了有效且合理的信念调整。

Conclusion: 本文实现了在Lockean框架下信念集合的逻辑封闭以及基于概率的最小修正更新，为信念变化理论提供了新的理论工具和方法。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [203] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: FEVO框架通过多阶段训练显著提升大型语言模型在金融领域的表现，实现了多个金融基准的顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 当前在金融领域应用LLM的研究有限，且金融任务需要丰富的领域知识和复杂的推理能力。

Method: FEVO采用持续预训练扩展金融知识，监督微调灌输结构化推理，强化学习融合知识与推理，结合前沿推理模型和规则过滤构建高质量训练数据。

Result: FEVO系列模型在七个基准测试中表现优异，特别是FEVO-R32B在五个金融基准上超过更大型和专业模型。

Conclusion: 通过扩展领域知识和结构化推理训练，FEVO有效提升了LLM在金融任务中的性能，验证了多阶段强化训练策略的有效性。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [204] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 本文提出了一种结合LSTM、遗传算法和SHAP的AI框架，用于提高医疗设施能耗管理的预测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 医疗设施能耗需求波动大，传统管理方法效率低且成本高，急需高效能量管理解决方案。

Method: 采用LSTM进行能耗时间序列预测，利用遗传算法优化模型参数和负载平衡策略，并用SHAP增强模型解释性。

Result: LSTM在能耗预测中的MAE和RMSE显著优于ARIMA和Prophet模型，遗传算法提升了模型适应性，SHAP提高了透明度。

Conclusion: 该LSTM-GA-SHAP综合方法有效提升了医疗能耗预测准确率和管理效率，促进可持续发展，未来可结合实时部署和强化学习进一步优化。

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [205] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: 本文提出了OpenAgentSafety，一个用于评估AI代理安全性的综合框架，支持真实工具和多样任务，揭示了当前主流大语言模型在安全性上的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理安全评估方法多依赖模拟环境和狭窄任务，缺乏对真实工具交互和多样风险的全面检测，急需一个更完善的评估框架。

Method: 提出OpenAgentSafety框架，涵盖八大风险类别，支持真实工具交互及350+多轮多用户任务，并结合规则分析与大语言模型裁判机制检测安全隐患。

Result: 对五种主流大语言模型进行实证分析，发现它们在51.2%至72.7%的安全敏感任务中存在不安全行为，暴露出显著安全漏洞。

Conclusion: OpenAgentSafety框架有效揭示了AI代理的安全风险，强调部署前需加强安全保障措施，提供了一个可扩展的评估平台以促进更安全的AI应用。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [206] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 本文提出了delta learning假设，利用成对的偏好数据对语言模型进行微调，即使单个数据点质量较弱，也能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当强监督数据稀缺时，单纯依赖高质量训练数据限制了模型性能的提升，故探索利用弱监督数据中的相对质量差异进行有效学习。

Method: 通过构造由较小模型生成的成对偏好数据，利用相对质量差异进行偏好微调，验证delta learning假设，并在大规模8B模型上进行训练。

Result: 在11个标准基准测试（如MATH、MMLU）上，基于弱监督偏好数据的训练结果匹配了使用更强监督（如GPT-4o）的先进开放模型Tulu 3的表现。

Conclusion: delta learning表明即使单个数据点较弱，通过成对弱数据的相对质量差异也能有效指导模型学习，实现简单且经济的模型后训练。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [207] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 本文研究在缺乏完整因果图知识时，通过因果抽象集合来识别因果效应，提出了多个可识别性标准并构建了层级结构。


<details>
  <summary>Details</summary>
Motivation: 实际应用中完整的因果图往往未知，尤其是在复杂和高维场景，需寻找保留部分因果信息的简化表示来识别因果效应。

Method: 将因果抽象形式化为因果图集合，提出并形式化多个因果查询的可识别性标准，构建标准之间的层级关系。

Result: 建立了因果识别标准的层级框架，揭示了不同因果知识水平下的可识别性，辅以文献实例说明并提供推理工具。

Conclusion: 该方法为缺乏完整因果知识时的因果效应识别提供了系统化的理论框架和实用工具，促进对因果识别条件的理解。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [208] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 本文提出了一种名为Aligned Scoring Rule (ASR)的新型评分规则，旨在更好地与人类对文本的偏好保持一致，同时保持评分的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的正确评分规则虽然确保了报告真实信念能最大化期望分数，但并不总是与人类对文本的偏好匹配。

Method: 设计了一个优化方案，通过最小化适当评分规则与参考分数（如人类评分）之间的均方误差来构造ASR。

Result: 实验结果显示，ASR在与人类偏好的一致性方面优于先前的方法，并且保持了评分规则的正确性。

Conclusion: ASR成功地解决了评分规则与人类文本偏好不一致的问题，提升了文本预测评分的实用性。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [209] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 本文将图神经网络中过度平滑现象与无序系统中的安德森局域化进行类比，提出参与度指标量化过度平滑，并探讨减少信息传播中的无序性以缓解过度平滑。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络深度增加，节点表示变得同质化，导致特征失去区分性，严重影响模型性能，需要深入理解和解决过度平滑问题。

Method: 通过类比安德森局域化，定义参与度指标以量化过度平滑现象，分析深层传播中节点特征同质化的机理，系统梳理安德森局域化与过度平滑的关系，并提出减少传播无序性以缓解过度平滑的潜在方法。

Result: 理论分析表明，过度平滑可以理解为低频模式扩展和高频模式局域化的现象。参与度指标有效描述了这一过程。减少信息传播的无序性有望缓解过度平滑问题。

Conclusion: 本文提供了过度平滑的物理类比视角及理论框架，开辟了基于信息传播无序控制的新思路，为解决深层图神经网络过度平滑提供了理论支持和潜在方向。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [210] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种基于全局统计的外生输入预处理方法，通过去冗余和增强全局上下文感知能力，提升了时间序列预测的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于transformer的时间序列预测方法通过结合外生输入提升性能，但存在冗余信息和固定回溯窗口限制捕捉长期依赖的问题。

Method: 本文通过对外生输入进行白化处理，降低冗余并增强其对长期趋势的感知能力，且不增加回溯窗口长度，将改进的外生输入与内生输入结合用于预测。

Result: 在四个基准数据集上，所提方法在性能上显著超越11个基线模型，实现了最先进水平。

Conclusion: 所提方法有效缓解了外生输入的冗余问题，增强了长期依赖捕捉能力，成为时间序列预测中结合外生输入的强有力方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [211] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 本文提出了一种利用可解释人工智能（XAI）技术进行深度神经网络（DNN）压缩的新方法，通过计算参数重要性进行剪枝和混合精度量化，实现了显著的模型压缩和准确率提升。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在性能卓越的同时，存在计算和内存资源消耗大的问题，迫切需要高效的压缩技术。XAI技术可以解释DNN内部机制，有助于更精细地进行模型压缩。

Method: 利用基于梯度的XAI方法Layer-wise Relevance Propagation (LRP)计算DNN中权重的重要性分数，剪除重要性为负或零的权重，然后根据权重重要性分数采用混合精度量化；重要权重用更多位数，低重要权重用更少位数量化。

Result: 该方法使DNN模型大小减少64%，且准确率相比现有基于XAI的压缩方法提高42%。

Conclusion: 结合XAI计算权重重要性进行有针对性的剪枝和混合精度量化，是一种有效的DNN模型压缩方法，可在大幅减小模型体积的同时提升性能。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [212] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图神经网络的物理驱动机器学习框架P-DivGNN，用于微观尺度局部应力场的重构。


<details>
  <summary>Details</summary>
Motivation: 微观局部应力场的准确预测对于断裂分析和局部疲劳准则的设定至关重要，传统有限元方法计算复杂且耗时，需寻找有效替代方法。

Method: 通过将周期性微观结构表征为图结构，结合消息传递图神经网络，并在训练中引入物理约束及周期性边界条件，实现局部应力场的预测。

Result: 在考虑线性和非线性超弹性材料响应及不同几何形状时，模型能准确预测局部应力场，且在非线性情况下相比有限元模拟具有显著的计算加速效果。

Conclusion: P-DivGNN有效结合物理信息和图神经网络技术，提升了局部微观应力场模拟的效率和精度，适合大规模多尺度应用。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [213] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经元传递函数变化速率（神经速度）的动态训练方法NeVe，用于调整学习率和定义停止准则，以优化神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 现有超参数调优方法多依赖监控验证损失，需持出数据，且效率有限。

Method: 引入神经速度概念，通过测量神经元传递函数变化速率动态调整学习率和停止准则，且可通过网络噪声采样神经速度，无需用验证集。

Result: 实验表明神经速度可作为有效指标促进训练收敛，提升训练优化效率。

Conclusion: 神经速度为神经网络训练的关键指标，能高效指导超参数调节与训练停止判断，减少对验证集的依赖。

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [214] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件图神经网络(cGNN)的数据驱动模型，用于模拟软组织在虚拟环境中的变形和受力，解决了高变形性软组织模拟的复杂难题。


<details>
  <summary>Details</summary>
Motivation: 软组织具有高度可变形性，传统方法依赖分割、网格划分及刚度参数估计，难以精准模拟软组织变形及力反馈以提升医疗虚拟环境中的沉浸感。

Method: 设计了条件图神经网络(cGNN)模型，输入为表面点及施力位置，预测点的变形和受力。先利用质量-弹簧仿真进行预训练，再用实验收集的软组织表面跟踪数据进行微调，提高模型泛化能力。

Result: 模型对30 mm变形预测的距离误差为0.35±0.03 mm，对7.5 N力的绝对误差为0.37±0.05 N，展示了准确的预测能力。

Conclusion: 此数据驱动方法有效应对软组织模拟难题，不仅适用于医疗模拟，也有望在其他需真实软组织模拟的领域应用。

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [215] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: 该论文首次将无数据神经网络方法应用于资源约束项目调度问题（RCPSP），提出将调度约束转化为可微目标函数的数学框架，实现基于梯度优化和GPU加速。


<details>
  <summary>Details</summary>
Motivation: 尽管无数据神经网络已应用于最大独立集问题，但尚未扩展到RCPSP领域，存在应用空白。

Method: 通过平滑松弛和自动微分技术，将RCPSP的离散调度约束转为可微目标，利用密集时间网格表示实现内存高效，并支持GPU并行计算。

Result: 当前正在PSPLIB基准（J30、J60、J120）上进行实现和实验，实验结果将在后续版本报告。

Conclusion: 该工作填补了无数据神经网络在项目调度领域的空白，展示了基于神经网络的新型调度算法框架的潜力。

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [216] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架HEPO，它通过最大化策略改进而非策略不变，解决了启发式奖励导致的奖励黑客问题，提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，利用启发式奖励编码人类先验知识可以帮助任务优化，但不合理的启发式奖励经常导致性能下降和资源浪费，现有方法依赖策略不变性理论效果有限且表现欠佳。

Method: 提出Heuristic Enhanced Policy Optimization（HEPO）框架，基于最大化策略改进的目标设计，有效利用启发式奖励并避免奖励黑客问题，提升策略性能。

Result: HEPO在标准基准测试中表现优异，即使是不擅长设计奖励的非专家也能通过HEPO获得良好策略表现，显著减少了人工设计奖励的工作量。

Conclusion: HEPO作为一种即插即用的优化方法，有效利用启发式奖励提升了强化学习策略的表现，降低了对人类先验奖励设计的依赖，展示了较强的实用价值。

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [217] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: 该论文提出了一种基于因果推理的基础模型，通过双编码器和结构化对比学习将物理信号与仪器效应分离，显著提升了结构化时间序列数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在结构化时间序列数据中，由于观测数据同时包含真实物理现象和测量仪器引入的系统性扭曲，导致模型难以泛化，尤其是在多仪器环境。

Method: 设计了一个因果驱动的双编码器架构，利用自然存在的观测三元组进行结构化对比学习，分别学习物理信号和仪器效应的隐含表示。

Result: 在模拟天文时间序列数据（如NASA的TESS任务数据）上，该方法在低数据量条件下下游预测任务表现显著优于传统单隐空间基础模型。

Conclusion: 该模型支持基础模型的关键能力，如少样本泛化和高效适应，强调将因果结构编码到表征学习中对于结构化数据的重要性。

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [218] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 本论文比较了监督微调（SFT）和强化微调（RFT）两种持续后训练范式对知识保留的影响，发现在持续学习多模态任务时，RFT能有效避免灾难性遗忘并提升模型的泛化能力，而SFT则表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前持续后训练主要关注数据重放、模型扩展或参数正则化，忽视了学习范式的根本作用，因此研究SFT和RFT对知识保留的不同影响。

Method: 基于Qwen2.5-VL-7B-Instruct模型，在七个多模态任务集上比较SFT与RFT的表现，分析其对知识遗忘和模型能力变化的影响，同时提出基于rollout的实例过滤算法提升RFT性能。

Result: SFT导致灾难性遗忘和模型能力下降，RFT不仅保留了先前知识，还提升了模型在标准基准上的表现。隐式正则化是RFT的主要优势，显式机制作用不大。

Conclusion: 强化微调（RFT）因其隐式正则化效果显著，是持续后训练中更优越且稳定的学习范式，适合多模态大模型的持续适应。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [219] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 本文提出了一种名为PT-LiRPA的新框架，通过结合LiRPA方法的过度逼近与采样方法，紧致神经网络输出的线性界限，提高形式化验证的效率和置信度。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiRPA的形式化验证方法存在计算开销大且边界松散的问题，迫切需要一种既能保证验证严谨性又能减少计算成本的方法。

Method: PT-LiRPA结合过度逼近技术和采样估计中间可达集，通过概率化方法显著收紧神经网络输出的上下线性界限，提升验证工具的效率和精准性，同时保证验证结果的概率性正确性。

Result: 在国际神经网络验证竞赛等标准测试集中，PT-LiRPA的验证器使得鲁棒性证书提升了3.31倍和2.26倍，且在传统方法失败的挑战性实例中也能以不低于99%的置信度给出答案。

Conclusion: PT-LiRPA是一种有效且实用的神经网络形式化验证框架，大幅提升了验证性能和结果的置信度，特别适用于处理复杂和高难度的验证任务。

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [220] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: AXLearn是一个高性能、可扩展的大型深度学习训练系统，强调模块化设计和异构硬件支持。


<details>
  <summary>Details</summary>
Motivation: 为了实现深度学习模型训练的高效扩展和简化异构硬件上的开发工作，设计一种具备高模块化和低复杂度的系统。

Method: AXLearn采用严格封装的软件组件接口，支持模块组合和快速实验，并提出通过代码行数复杂度来量化模块化，展示系统复杂度随组件规模保持不变。

Result: AXLearn在不同组件扩展时保持代码复杂度恒定，实现了如RoPE功能的高效集成，同时性能与现有顶尖训练系统相当。

Conclusion: AXLearn凭借其模块化设计和对异构设备的支持，实现了高效、灵活的大规模深度学习训练，显著降低了系统复杂度，提高了开发效率。

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [221] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 本论文针对因果相关潜变量的鲁棒判别表示学习问题，利用干预数据强化模型以应对干预分布变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了干预所导致的因果独立关系，导致在干预数据上预测性能差异大，尤其当干预样本有限时。

Method: 识别性能差异与干预因果模型所诱导的独立性条件的强相关性；为线性模型推导干预数据比例的充分条件；提出RepLIn算法，显式约束干预下的统计独立性。

Result: 在合成数据和图像、文本真实数据集（面部属性分类、毒性检测）上验证了RepLIn算法，展示其对连续和离散潜变量干预分布偏移的鲁棒性提升效果，且具备良好扩展性。

Conclusion: 通过显式利用因果干预信息强制潜变量表示的独立性，RepLIn有效缓解干预样本稀缺问题，提高模型在干预分布上的预测性能和鲁棒性。

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [222] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: 本文提出两种基于深度学习的新型模型（EmissionNet和EmissionNet-Transformer）用于预测农业源的N2O排放，改善传统物理模型在捕捉复杂污染物交互上的不足。


<details>
  <summary>Details</summary>
Motivation: 农业排放导致的空气污染对环境和公共健康有重要影响，但传统基于物理的空气质量预测模型难以有效捕捉复杂的非线性污染物相互作用。

Method: 设计并评估两种新颖的深度学习模型EmissionNet和EmissionNet-Transformer，利用卷积和基于Transformer的架构提取高分辨率排放数据中的时空依赖关系。

Result: 通过评估，所提出的模型在捕捉农业N2O排放的时空特征方面表现优异，优于传统物理模型。

Conclusion: 新型深度学习架构有效提升了农业排放N2O的预测性能，为空气质量预测提供了更准确的工具。

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [223] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: 该论文提出了一种针对公司财务报告欺诈的多目标攻击方法MVMO，能够显著提高欺诈成功率，使公司在约50%的情况下能将收益虚增100-200%，同时降低欺诈得分15%。


<details>
  <summary>Details</summary>
Motivation: 坏行为者尤其是陷入困境的公司有动力操纵财务报告以隐藏困境并获取私利，现有攻击方法难以满足多重反向目标的需求。

Method: 提出最大违反多目标(MVMO)攻击，调整攻击者搜索方向，提高攻击成功的满足率达20倍。

Result: 实验证明MVMO在约50%情况下能同时大幅虚增收益与降低欺诈评分。

Conclusion: 通过与律师和专业会计师合作，模型建立在现实可行的欺诈情景基础上，MVMO有效提升财务欺诈攻击的成功率和隐蔽性。

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [224] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: 本文提出了一种统一的分布式多步强化学习框架Horizon-DQN，专注于解决延迟且稀疏奖励问题，在2048游戏中显著提升了得分表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在延迟和稀疏奖励环境中难以有效赋予动作价值，如2048游戏中的局部最优困境。

Method: 提出了Horizon-DQN，结合分布式学习、决斗网络、噪声网络、优先经验重放等技术，并与DQN、PPO、QR-DQN进行了对比。

Result: Horizon-DQN在2048游戏中取得了最高分18.21K，扩展后更是达到41.828K，显著优于其他算法。

Conclusion: 分布式多步目标极大提升了稀疏奖励场景下的强化学习性能，未来可结合模型规划和课程学习进一步提升。

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [225] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: 本文提出了一种基于前向-后向表示（FB）的零样本强化学习探索策略，通过最小化FB表示的后验方差，提升探索效率和样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本强化学习算法在采集数据时依赖其他探索方法，未充分利用FB表示进行高效探索。

Method: 设计了一种从FB表示自然产生的探索策略，目标是最小化FB表示的后验方差以降低不确定性。

Result: 实验证明，该探索策略显著提升了FB算法的样本效率，优于其他探索方法。

Conclusion: 利用FB表示本身进行探索能更有效地学习最优策略，提高零样本强化学习算法的性能。

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [226] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: 本文研究线上凸优化中的动态遗憾问题，将动态遗憾最小化转化为函数空间的静态遗憾问题，利用再生核希尔伯特空间（RKHS）构造，实现了最优的动态遗憾界，并扩展到非线性损失情况。


<details>
  <summary>Details</summary>
Motivation: 现有动态遗憾最小化方法多仅适用于线性损失，且动态与静态遗憾的转换有限，亟需一种通用方法能处理任意损失函数并提供优良的动态遗憾界。

Method: 通过将比较任意比较器序列的问题视为在函数空间中与固定比较器函数竞争的问题，构建适当的RKHS函数空间，完成从动态遗憾到静态遗憾的归约，从而获得更广泛和精细的遗憾界。

Result: 在此框架下，重现了线性损失下最优动态遗憾界$\mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$，并获得了新的无尺度和方向自适应遗憾界。同时，适用于任意损失序列，在指数凹和不适当线性回归中得到复合遗憾界。算法实际可计算。

Conclusion: 本文提出的基于RKHS的动态到静态遗憾归约方法不仅恢复和提升了已有在线优化动态遗憾的界限，还实现了对非线性损失的适用性，具有重要理论推动及实际意义。

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [227] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 提出了无训练的扩散引导框架SDG，利用代理随机最优控制目标和Stein变分推断来校正近似后验，提高扩散模型采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有随机最优控制虽有效但计算昂贵，训练无关方法准确性不足，需结合两者优势提升扩散模型引导效果。

Method: 构建代理随机最优控制目标，推导值函数理论界限，利用Stein变分推断最小化KL散度对近似后验进行校正，设计新的运行代价函数实现低密度区域有效引导。

Result: 在分子生成等困难任务上，SDG显著优于传统无训练引导方法，表现出更高的采样质量和引导可靠性。

Conclusion: 该方法为无训练扩散模型引导提供了理论基础及有效机制，兼具准确性与效率，具备广泛应用潜力。

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [228] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: 该文提出了一种名为Ex-HiDeNN的可解释层次深度学习神经网络方法，结合符号回归，从有限数据中高效发现准确且可解释的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 当前利用数据驱动方法构建复杂函数关系虽有进展，但如何高效发现准确且解释性强的闭式表达式仍具挑战。

Method: 提出了两步算法Ex-HiDeNN，结合可分性检查器，采用节约资源、分离性强且可扩展的神经网络架构与符号回归进行表达式发现。

Result: Ex-HiDeNN在多个基准测试及三个工程实例中表现优异，误差显著低于传统符号回归及参考方法。

Conclusion: Ex-HiDeNN在表达式发现方面具备卓越性能，且可解释性强，未来有望扩展应用领域并克服当前限制。

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [229] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络的校园建筑占用率和建筑间动态流动预测模型，结合Wi-Fi日志和使用时间表，保护隐私且效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 通过利用Wi-Fi数据结合建筑使用时间表，准确预测校园内人员流动，提升场景建模效率且保证隐私安全。

Method: 构建以建筑为节点的图结构，采用图卷积与LSTM结合的GCLSTM模型学习复杂流动模式，实现基于数据驱动的流量估计。

Result: 模型在多伦多都会大学的实际Wi-Fi数据上测试，性能显著优于多层感知器和线性回归等传统预测方法。

Conclusion: 提出的集成GCLSTM架构有效捕捉建筑间人流动态，具备较高预测精度并保障用户隐私，适合实际校园智能管理应用。

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [230] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多层蒙特卡洛（MLMC）压缩方案，利用有偏压缩器构建统计无偏估计，兼顾有偏和无偏方法的优势，提升分布式学习中的通信效率。


<details>
  <summary>Details</summary>
Motivation: 分布式学习中通信开销成为瓶颈，传统梯度压缩方法在有偏和无偏压缩器之间权衡效率和理论保证。

Method: 引入多层蒙特卡洛压缩方案，结合常用的Top-k和逐位压缩器，提出增强版本并设计自适应变体。

Result: 在分布式深度学习任务中验证了该方法的有效性。

Conclusion: 该方法有效弥合了有偏和无偏压缩技术的差距，提升了分布式学习的通信效率和性能。

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [231] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的用户增长营销优化方法，通过直接建模关键业务指标的提升效果，提升活动效果并降低成本。


<details>
  <summary>Details</summary>
Motivation: 用户增长是消费互联网公司的核心战略，传统方法难以高效优化营销投入与用户参与，需新方法提升营销的ROI。

Method: 利用深度学习从历史实验学习，通过软最大门控联合优化聚合损失函数，直接优化关键业务指标的提升效果，并处理复杂业务约束。

Result: 该方法在与R-learner和Causal Forest等先进技术比较中，性能提升超过20%，在成本效益和实际应用中表现优异。

Conclusion: 提出的深度学习优化算法在多种产品及情境中表现出高度灵活性和优越性能，已成功实现全球部署，具备广泛应用价值。

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [232] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的新方法，通过构建神经增强朴素贝叶斯层，实现对多因素异质性治疗效应的建模和排序。


<details>
  <summary>Details</summary>
Motivation: 随着异质性治疗效应评估应用的普及，治疗操作空间从二元变量扩展到包含连续治疗强度和离散治疗分配的复杂策略，亟需有效的建模方法。

Method: 作者从基本原理出发，推导了多政策变量纳入个体及平均治疗效应函数的公式，设计了一个神经增强朴素贝叶斯层来满足贝叶斯假设，并将其集成于深度学习框架中处理连续和离散治疗变量，直接对聚合治疗效应函数进行排序。

Result: 在公开数据集上，所提方法表现出显著的性能提升，验证了其在多因素异质性治疗政策学习中的有效性。

Conclusion: 本文构建了一个通用的深度学习异质治疗政策框架，能够灵活处理复杂治疗因素，为相关领域的HTE估计提供了强有力工具。

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [233] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: 本文提出了一种基于元学习的因果推断模型MACE-TNP，用于在结构不确定性较大的情况下进行贝叶斯模型平均，多于传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 在缺乏因果结构先验知识时，单一结构推断容易过于自信，需要一种能够处理结构不确定性的贝叶斯方法，但传统计算代价高。

Method: 提出了一种端到端的元学习模型——MACE-TNP，训练其预测贝叶斯模型平均的干预后验分布，避免昂贵计算。

Result: 实验证明MACE-TNP优于强贝叶斯基线方法。

Conclusion: 元学习为近似复杂贝叶斯因果推断提供了灵活且可扩展的范式，适用于未来更复杂场景。

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [234] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: 该论文提出了一种名为InterpoLL的方法，通过插值大多数类样本的表示，引入少数类样本的特征，减弱模型对捷径的依赖，提高了模型在少数类样本上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的经验风险最小化（ERM）方法使模型过度依赖于输入与标签之间的捷径相关性，导致模型在少数类样本上泛化能力不足。现有方法存在针对性强、调参困难、计算复杂且无法改善学习表示的问题。

Method: 提出InterpoLL方法，通过在表示空间中插值大多数类样本与少数类样本的特征，减弱捷径特征的影响，使模型能够学习对少数类和多数类均有效的特征。

Result: 在多项自然语言理解任务上，InterpoLL在不损失多数类样本准确率的前提下，显著提升了少数类样本的泛化性能，效果优于ERM和最新捷径缓解方法。该方法适用于多种模型架构，包括编码器、编码器-解码器和仅解码器架构。

Conclusion: InterpoLL是一种通用且高效的捷径缓解策略，通过特征插值得到更鲁棒的表示，提升了模型对少数类样本的适应能力，具有广泛的应用前景。

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [235] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 本文提出了一种针对图神经网络（GNN）的硬件故障攻击方法——渐进式翻转位故障攻击（GBFA），通过逐层选择易受攻击的权重位进行bit翻转，显著降低模型预测准确率。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器在提升GNN性能中被广泛采用，但对硬件故障攻击的安全问题缺乏关注，攻击者可能通过注入故障修改权重参数，导致GNN输出误分类。

Method: 提出GBFA方法，首先通过Markov模型预测各层执行顺序，实现对特定层的攻击；然后通过梯度排序搜索该层内易被攻击的权重位，逐渐翻转这些位以最小修改量破坏模型性能。

Result: 在Cora和PubMed数据集上的多种GNN模型（如GraphSAGE）验证显示，GBFA显著降低预测准确率，如在Cora数据集最后一层仅翻转单个位即可导致准确率下降17%。

Conclusion: GBFA的层感知攻击策略有效揭示了GNN模型对硬件故障攻击的易感性，强调在安全防护设计中需关注不同网络层的脆弱性。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [236] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: 本文首创性地理论分析了结合跳跃连接和图稀疏化的图卷积网络（GCNs）的学习动力学和泛化性能，并验证稀疏有效邻接矩阵对泛化精度的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究在理论上缺乏对跳跃连接和图稀疏化联合使用时GCNs泛化性能的分析。

Method: 提出理论框架分析使用跳跃连接和图稀疏化的GCNs的学习动态和泛化，定义稀疏有效邻接矩阵$A^*$，探讨不同层的稀疏需求。

Result: 发现泛化性能依赖于$A^*$是否保留关键边，跳跃连接导致不同层对稀疏化的敏感度不同，一层的偏差比第二层影响更大。

Conclusion: 跳跃连接影响GCNs的稀疏化需求，其理论分析首次揭示了跳跃连接在保持泛化性能中的作用，实验验证了理论有效性。

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [237] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: 该论文提出了一种名为LSC-GNN的新型图神经网络，通过引入外部“干净”边约束潜在空间，改善了噪声边对图神经网络的影响，在多种基准数据集和异构图中表现优异。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在存在噪声边时性能下降，如何有效利用外部“干净”边来提升模型鲁棒性是该工作的动机。

Method: 设计了两个编码器，一者基于完整图（包含目标图和外部边），另一者基于不含目标图潜在噪声边的正则化图，通过惩罚两个编码器潜在表示的差异来避免过拟合噪声边。

Result: 在基准数据集和蛋白质-代谢物异构图上，LSC-GNN优于传统及抗噪声图神经网络，显著提升了预测性能和模型解释性。

Conclusion: LSC-GNN通过潜在空间约束，有效抵抗了图中的噪声边干扰，提升了图神经网络在噪声关系结构下的表现和可解释性。

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [238] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: 该论文提出了一种结合辅助信息的负载估计框架，用于通过可穿戴传感器收集的步态数据评估搬运负载，提高了泛化能力和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法单纯依赖负载步态与手部负载的直接映射，导致泛化能力和预测准确性不足，且携带方式信息难以获取限制了模型性能。

Method: 引入辅助信息（无负载基线步态和携带方式），结合深度潜变量模型、时间卷积网络和双向交叉注意力机制，融合负载与无负载步态数据，设计无须推断时携带方式标签的条件负载估计模型。

Result: 实验基于实测惯性测量单元数据，显示引入辅助信息显著提升了负载估计准确性，并强调了显式融合机制优于简单特征拼接的重要性。

Conclusion: 该框架有效整合辅助信息提升了负载估计性能，解决了携带方式信息获取难题，具备较强的实用价值和推广潜力。

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [239] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 本文提出多任务预演算法，通过在一个任务上的经验预先学习未执行但可进入的其他任务，支持快速适应和泛化。


<details>
  <summary>Details</summary>
Motivation: 人类虽能执行无限多任务，但同时只能关注少数任务，假设人类利用一个任务的经验预学习其他未执行任务的解决方案。

Method: 提出多任务预演算法，通过在一个任务的经验进行反事实模拟（预演）学习预测性表征，支持后续任务的快速适应。

Result: 多任务预演在小型网格世界和部分可观测的2D Minecraft环境中，比传统方法更好地预测了人类对未执行任务的泛化能力，并促进人工智能代理在新环境中转移学习。

Conclusion: 多任务预演为人类如何跨多个任务进行反事实学习和泛化提供了可扩展理论，赋予人工代理此能力可显著提升其多任务环境中的表现。

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [240] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文综述了大规模语言模型（LLMs）中训练数据记忆现象的现状，探讨了其驱动因素、检测方法、影响及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 深入理解LLMs的记忆机制，揭示其行为、隐私风险以及学习与记忆的界限。

Method: 综合了近期相关研究，分析了数据重复、训练动态、微调等影响因素，评估了基于前缀提取、成员推断和对抗提示等检测技术，并讨论了法律伦理问题和缓解措施。

Result: 系统整理了影响记忆现象的关键因素和检测方法，总结了隐私风险及法律伦理影响，提出了数据清洗、差分隐私和去学习等缓解策略。

Conclusion: 该综述为理解LLMs记忆现象提供了全景视角，强调在降低有害记忆与保持模型性能之间的权衡，并指出未来研究方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [241] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: 本文提出了一种基于近端策略优化（PPO）的无模型强化学习方法，用于原位训练衍射光学处理器，提高了训练速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统光学计算中的衍射层优化受到硬件缺陷、噪声和误差的影响，且现有方法收敛慢、性能不稳定，难以有效利用有限测量数据。

Method: 采用基于PPO的无模型强化学习方法，直接在物理系统上训练，通过高效复用测量数据和限制策略更新，实现更快、更稳定的收敛。

Result: 该方法在多种任务中（如能量聚焦、全息图生成、像差校正和光学图像分类）均表现出更好收敛性和性能，并消除了对系统建模的需求。

Conclusion: 该原位强化学习策略能处理复杂反馈动力学下的物理系统和光学系统，实现快速准确的训练，具备良好可扩展性。

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [242] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: 本文提出了一种统一的傅里叶谱变换器网络，结合谱方法和注意力神经网络，解决偏微分方程长期预测问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法和机器学习方法对复杂动力系统的长期预测效果有限，亟需高精度且高效的预测模型。

Method: 将偏微分方程转化为谱空间的常微分方程，利用高精度数值解算器生成训练数据，并用Transformer网络预测谱系数的演化。

Result: 在二维不可压Navier-Stokes方程和一维Burgers方程实验中，方法在有限训练数据下实现了高精度长期预测，优于传统数值和机器学习方法。

Conclusion: 该谱傅里叶变换器框架对未知数据具备良好泛化能力，为复杂动力系统的实时预测与控制提供了有前景的方法。

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [243] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 本文比较了基于特征和基于GAN的示范学习方法，分析了奖励函数结构及对策略学习的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨不同示范学习方法中奖励结构对策略学习效果的影响及其适用场景。

Method: 分析特征方法提供密集且解释性强的奖励，适合高保真运动模仿；GAN方法通过隐式分布监督实现可扩展性和灵活适应，但训练不稳且奖励粗糙。

Result: 两种方法各有优势与局限，最新进展强调结构化运动表示，改善切换平滑性和任务集成。

Conclusion: 方法选择应基于具体任务需求，如保真度、多样性、解释性和适应性，兼顾算法权衡与设计考量。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [244] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文针对强化学习中的奖励欺骗问题进行了大规模实证研究，提出了自动检测方法并在多环境多算法中验证。


<details>
  <summary>Details</summary>
Motivation: 奖励欺骗严重威胁自主智能体的实际部署，现有检测和缓解方法有限，需要系统的研究和有效工具。

Method: 分析了15个强化学习环境和5种算法的15247个训练回合，设计了覆盖六类奖励欺骗的自动检测算法，并在三个应用场景中进行了验证。

Result: 检测框架在多环境中达到78.4%的准确率和81.7%的召回率，计算开销小于5%。奖励密度和目标一致性显著影响欺骗频率。缓解方法在控制场景中将欺骗率降低了54.6%。

Conclusion: 本文提供了系统的检测与缓解方法，数据和工具公开，促进强化学习安全研究，但实际应用仍面临概念漂移和对抗性适应等挑战。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [245] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 本文提出了一种名为Uncertainty-Aware RL (UARL)的新型强化学习框架，通过不直接与目标环境交互来提高策略的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署强化学习策略面临分布偏移、安全性和政策优化直接交互不可行等挑战，现有方法通过直接与目标域交互提高鲁棒性存在安全隐患。

Method: UARL利用多个评论家评价策略不确定性，结合逐步环境随机化技术，在模拟环境中针对高不确定性区域进行迭代优化，实现无须直接交互的策略适应和OOD检测。

Result: UARL在MuJoCo基准测试和四足机器人上表现出可靠的OOD检测能力、提升的性能和更高的样本效率，优于基线方法。

Conclusion: UARL框架有效提升了强化学习策略在真实世界环境中的安全性和泛化能力，避免了直接目标域交互带来的风险。

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [246] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: 本文综述了图学习的发展及其在机器学习中的重要性，涵盖了其在可扩展性、时间动态、多模态、生成模型、可解释性及责任AI等方面的最新进展。


<details>
  <summary>Details</summary>
Motivation: 图学习能够有效建模复杂的非欧几里得关系，克服传统机器学习的局限，支持药物发现、欺诈检测等实际应用，但面临可扩展性、泛化能力等挑战。

Method: 系统回顾了图学习的关键技术，包括大规模图处理、动态时间依赖、多模态数据融合、生成模型、可解释方法及伦理问题，如隐私和公平性。

Result: 总结了图学习领域的最新技术进展和实际应用，指出了跨AI范式整合的新兴主题，展望了未来研究方向。

Conclusion: 通过全面综述，推动研究者理解和应用图学习技术，促进该领域的可持续和负责任发展。

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [247] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: 本文提出了收敛特征定理(FACT)，揭示了在带非零权重衰减下神经网络权重收敛时的自洽关系，并基于该定理设计了FACT-RFM算法，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习理论中的核心挑战是理解神经网络如何学习和表示特征。

Method: 证明了FACT，建立权重矩阵与输入特征及梯度之间的关系，并基于此修改递归特征机(RFM)构建FACT-RFM算法。

Result: 通过实验证明神经网络特征满足FACT，还展示了FACT-RFM在表格数据上表现优良，并能模拟真实训练中的多种特征学习现象。

Conclusion: FACT揭示了神经网络特征学习的内在规律，FACT-RFM提供了一种有效且解释性强的学习算法，促进了特征学习理论与实际应用的结合。

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [248] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: 该论文利用可穿戴惯性传感器和深度学习模型，实现了犬类步态的神经性和骨科性异常的高精度分类。


<details>
  <summary>Details</summary>
Motivation: 临床上神经性和骨科性步态异常难以区分，现有方法不足，需借助惯性传感器和深度学习提升诊断准确性。

Method: 使用惯性传感器数据，优化传感器配置和深度学习模型结构，完成多类别（健康/骨科/神经）及二类别（健康/非健康）分类任务。

Result: 在含29只犬的数据集上，方法在多类别分类中达96%准确率，在二类别分类中达82%准确率，体现了良好的泛化能力。

Conclusion: 基于惯性传感器的深度学习模型能为犬类步态的神经性及骨科性异常诊断提供有效且客观的辅助，具有临床应用潜力。

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [249] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种智能客户端与专家模型动态匹配的系统设计，旨在提升联邦学习中混合专家架构大规模模型的训练效率与通信效率。


<details>
  <summary>Details</summary>
Motivation: 目前联邦学习与混合专家架构结合存在系统层面挑战，尤其是如何在异构客户端资源与复杂专家协调间实现高效分配尚无量化方案。

Method: 提出包含动态适应评分、全局专家负载监控及客户端容量描述的智能客户端-专家匹配系统设计。

Result: 该设计有助于实现更大规模、更高效更鲁棒的训练机制，减少收敛所需通信轮数。

Conclusion: 通过解决系统性匹配和负载均衡问题，促进大规模联邦MoE模型在边缘计算中的高效部署与通信节省。

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [250] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 本文提出了AutoTriton，一种基于强化学习的Triton编程模型，自动优化高性能深度学习内核。


<details>
  <summary>Details</summary>
Motivation: 深度学习内核开发需在硬件上权衡内存、并行性及硬件优化，传统方法需手动调参，门槛高，效率低。

Method: AutoTriton通过监督微调获得Triton编程基础知识，随后采用组合规则和执行奖励的Group Relative Policy Optimization强化学习算法进一步提升性能。

Result: 在TritonBench和KernelBench的五个评测通道上，AutoTriton实现了与主流大型模型（如Claude-4-Sonnet、DeepSeek-R1-0528）相当的性能。

Conclusion: 强化学习可有效自动生成高性能内核，推动了深度学习系统核心组件自动化优化，为构建更高效的AI系统奠定重要基础。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [251] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文提出了MobileGUI-RL框架，通过在线环境训练视觉GUI代理，提升了任务执行的可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉GUI代理大多在离线环境中训练，依赖预先收集的数据，导致模型对特定UI模板过拟合，缺乏对未知环境的适应能力。

Method: 提出MobileGUI-RL框架，包含自我探索生成的任务课程和改进的GRPO算法，引入轨迹感知优势和复合奖励，平衡任务成功率与执行效率。

Result: 在三个在线移动代理基准测试中，MobileGUI-RL实现了性能的一致提升，验证了其有效性。

Conclusion: 通过在线训练和任务课程设计，MobileGUI-RL显著提升了视觉GUI代理的泛化能力和操作效率，具有较强的应用前景。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [252] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: 本论文提出了一种基于部分卸载的双层无人机辅助边缘计算架构，协调不同高度无人机的计算与中继能力，优化系统延迟和能耗，确保任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有无人机辅助卸载策略难以有效协调异构计算资源及适应动态网络环境，限制了车辆网络中计算密集型和延迟敏感应用的性能。

Method: 设计双层架构，结合高空无人机的中继能力和低空无人机的计算支持，建立联合延迟与能耗优化模型，将问题转化为马尔可夫决策过程，并基于软演员-评论家算法提出分层卸载方案，实现全局连续动作和局部优先调度的解耦。

Result: 仿真结果显示该方案在任务完成率、系统效率和收敛速度方面优于多种基线方法，具备良好的鲁棒性和动态环境适应性。

Conclusion: 所提双层无人机辅助架构有效协调异构资源，实现了动态车辆环境下计算卸载的性能提升，具备较强的实用潜力。

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [253] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 本文提出了一种基于多层感知机的天气预测模型WeatherMixer和一种新型模型并行方案Jigsaw，实现了高效、大规模的气象预测训练。


<details>
  <summary>Details</summary>
Motivation: 传统气象预测模型受限于加速器内存和I/O带宽，难以处理高空间分辨率和长预报时间的海量数据。

Method: 设计了线性复杂度的WeatherMixer模型和结合域并行与张量并行的Jigsaw并行方案，减少了内存冗余，提高了计算通信效率。

Result: 在256 GPU上训练获得9和11 PFLOPs的峰值性能，分别达到理论峰值的23%和28%，并实现了68%和72%的扩展效率，显著优于无模型并行的51%。

Conclusion: WeatherMixer结合Jigsaw模型并行技术有效突破了内存和带宽瓶颈，实现了高精度和大规模气象预测训练，提升了模型性能和扩展性。

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [254] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: 该研究结合深度学习图像配准与物理信息正则化，预测心脏组织的生物力学属性并进行疾病分类，显示出高准确率。


<details>
  <summary>Details</summary>
Motivation: 心脏病具有高发病率和致死率，亟需准确及时的诊断方法。

Method: 利用Neo-Hookean材料的能量应变模型进行心脏组织形变建模，通过深度学习优化变形场，保证物理和生物力学一致性，结合多种分类算法进行疾病分类。

Result: 在ACDC数据集上，图像分割Dice系数分别达0.945、0.908和0.905，分类准确率训练集98%、测试集100%。

Conclusion: 该方法通过可解释人工智能提高诊断准确性和透明度，有助于个性化心脏病患者的有效治疗。

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [255] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: 本文提出了一种结合时间序列预测和基于线性规划的通量平衡分析(FBA)方法来预测动态网络中图结构的变化，克服了现有方法对顶点不变的假设。


<details>
  <summary>Details</summary>
Motivation: 现有图预测方法假设连续图之间顶点不变化，这在实际动态网络中不成立，限制了预测能力。

Method: 结合时间序列预测技术和适应性的通量平衡分析(FBA)，利用FBA处理增长图的各种约束，实现动态图结构的有效预测。

Result: 在合成数据集（基于偏好连接模型）和真实数据集（UCI Message、HePH、Facebook、Bitcoin）上的实证评估，验证了方法的有效性。

Conclusion: 提出的方法有效突破了顶点不变限制，提升了动态图预测的精度和适用范围，对于异常检测等应用具有重要意义。

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [256] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出了一个新的物理推理基准ABench-Physics，用以评估大型语言模型（LLMs）在物理领域的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在物理领域的能力尚未充分探索，现有基准测试难度有限且评价方式不全面，无法充分反映模型的物理建模能力。

Method: 设计了包含静态和动态两部分的问题集，分别为400个高难度静态题目（Phy_A）和100个带自动变体生成的动态题目（Phy_B），要求模型给出精确的数值答案，严格限定格式和误差范围。

Result: 对多种先进的大型语言模型进行测试，发现其在物理推理尤其是对动态变体的泛化能力上存在显著差距和不足。

Conclusion: ABench-Physics作为一个具有挑战性和诊断性的评测框架，有助于推动大型语言模型在科学推理领域的发展。

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [257] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: 本文提出了Soup-Adapter，通过训练多个具有不同超参数的独立适配器并对其输出进行平均，提升了小样本领域适应中基础模型的性能和鲁棒性，同时解决了超参数调优困难和分布变化适应问题。


<details>
  <summary>Details</summary>
Motivation: 解决小样本领域适应中缺乏大规模验证集导致超参数调优困难，以及模型对测试时分布偏移的鲁棒性不足。

Method: 训练多个独立适配器，使用多样化超参数，平均其输出形成集成模型。该集成模型可通过参数连接重新表示为单个适配器，称为Soup-Adapter。同时首次将CLIP适配器技术用于DINOv2并进行直接比较。

Result: 集成模型在性能和对分布变化的鲁棒性上均优于任一单独适配器，且对关键超参数残差比敏感度明显降低。

Conclusion: Soup-Adapter方法有效解决了小样本领域适应中的超参数调优和分布变化鲁棒性问题，同时简化了模型复杂度，提升了CLIP和DINOv2适配器的实用性。

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [258] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: 本文提出了一种基于概念的机械可解释性分析框架及交互式工具BAGEL，通过构建知识图谱全局分析神经网络内部语义概念的传播和交互，揭示模型决策机制与数据偏差影响。


<details>
  <summary>Details</summary>
Motivation: 传统的概念可解释方法多集中于局部预测解释，缺乏对模型全局行为和机制的理解，需要一种能够系统分析模型内部语义表示及其流动的全局框架。

Method: 构建一个模型无关的框架，定量分析语义概念在不同层的表达情况，揭示潜在的神经回路和信息流；开发可视化平台BAGEL，以结构化知识图展示概念与类别关系，支持用户互动探索。

Result: 成功实现了对模型内部语义概念的全局解剖，能够识别伪相关关系并帮助理解模型泛化能力及其对数据偏差的敏感性。

Conclusion: 该框架扩展了概念解释方法到机械可解释领域，提升了对深度学习模型决策机制的理解和信任度，对模型调试和改进具有重要作用。

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [259] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: 本文研究了公平域泛化问题，旨在同时最小化目标域的期望风险和公平性违规，提出了基于互信息的理论界限及Pareto优化框架PAFDG，实现优越的效用-公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 当前域泛化方法忽略公平性，而公平性方法忽略域变迁，导致公平性在未知域中难以保持。作者希望同时解决域泛化和算法公平性的问题。

Method: 提出基于互信息的期望风险和公平违规上界，为方法设计提供理论支持；设计了PAFDG框架，通过Pareto优化实现公平与效用的权衡。

Result: 在真实的视觉和语言数据集上，PAFDG在效用-公平性权衡上优于现有方法。

Conclusion: 通过理论分析与实践验证，PAFDG有效桥接了域泛化与算法公平性，实现了在未知域中公平且性能优越的模型。

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [260] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: 本论文提出一种联邦学习框架，利用原型和轻量级适配器模块解决统计异质性和通信开销问题，同时实现模型内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据协同训练中常遇通信负担大及因客户端数据分布不同带来的统计异质性问题。

Method: 通过原型嵌入和轻量级适配器模块替代完整模型权重传输，客户端局部调整模型以对齐类别原型，同时减小通信量并保证模型全局一致性和解释性。

Result: 在真实视网膜眼底图像数据集上进行分类实验，结果显示该方法在提供可解释能力的同时，分类准确率优于基线算法。

Conclusion: 该框架有效缓解联邦学习中的统计异质性和通信负担，提升了模型性能和可解释性，适合实际临床场景应用。

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [261] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络(PINNs)的混合方法，用以提升电力系统状态估计的准确性和鲁棒性，特别是在故障和网络攻击条件下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统在状态估计和实时监控中面临响应速度和准确性的挑战，尤其是在故障状态或网络攻击时。

Method: 通过将物理定律嵌入神经网络结构，采用PINNs方法对电力传输网的状态进行估计。

Result: 实验结果显示，该方法在未见过的数据子集上准确率提高了83%，在全新数据集上性能提升65%，并在数据篡改攻击下比同等神经网络准确率高出93%。

Conclusion: 基于物理信息的神经网络方法显著增强了电力系统状态估计的精度和安全性，具备优于传统机器学习模型的能力。

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [262] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: 提出了一种将表格数据转化为图结构并使用图自编码器生成通用嵌入的方法，用于无预定义任务的下游分析，实验表明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 工业数据库中表格数据应用任务多样且常未预先定义，需一个通用的表格数据嵌入方法以支持多种下游任务。

Method: 将表格数据转化为图结构，利用图自编码器生成实体嵌入，再聚合为每行数据的嵌入；该方法支持相似实体的新样本无额外训练即可嵌入。

Result: 在真实数据集上，该方法较现有通用表格数据嵌入技术表现更优。

Conclusion: 本文提出的图结构和图自编码器结合的两步生成嵌入策略，实现了任务无关的表格数据嵌入，提升了下游任务的效果。

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [263] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为D2C的扩散数据集凝缩框架，通过选择和增强方法，大幅减少训练数据量，实现更快更高质量的扩散模型训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练资源消耗极大，需大量数据和时间，亟需从数据角度减少训练负担。

Method: 提出D2C框架，先通过扩散难度评分和间隔采样选择紧凑多样子集，再附加丰富语义和视觉信息增强子集条件信号。

Result: D2C在多种数据集和模型上均显著加速训练并降低数据需求，使用仅0.8%数据在40k步内达到FID 4.3。

Conclusion: D2C有效压缩扩散模型训练所需数据，极大提升训练效率且保持生成质量，开拓了扩散模型数据凝缩的新方向。

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [264] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: 本文研究了犬类心脏病听诊数据中标签噪声问题，提出通过多专家意见减少标签噪声，并采用XGBoost等算法提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 标签噪声严重影响兽医领域AI模型训练的分类性能，需探索减少标签噪声的方法以提升诊断准确率。

Method: 利用140条犬类心音记录数据，结合多专家评价筛选出70条高质量数据，通过标签噪声减少扩展训练样本，并训练AdaBoost、XGBoost和随机森林三种分类器。

Result: 应用标签噪声减少后，所有分类算法表现显著提升，XGBoost在心脏杂音轻度、中度及剧烈分类中的敏感性和特异性均大幅提高。

Conclusion: 减少标签噪声对于提升犬类心脏杂音检测的机器学习分类准确率至关重要，多专家评估结合先进算法是有效途径。

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [265] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出将Adam优化器视为类似符号的优化器，简化了其收敛性分析，并首次在较弱假设下证明了其收敛速度为O(1/T^{1/4})，优于之前的理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有Adam的理论分析复杂且依赖强假设，收敛性证明难以验证和推广，亟需一种更简洁且有效的分析方法。

Method: 将Adam重构为符号型优化器形式，通过该新视角简化收敛证明，基于一般$p$-仿射方差和$(L_0,L_1,q)$-平滑性假设，推导其收敛率。

Result: 证明Adam在弱假设条件下达到最优收敛率$O(1/T^{1/4})$，且与模型维度和稳定参数$B5$无关，同时揭示了动量在收敛中的重要作用。

Conclusion: 新理论视角不仅简化了收敛性分析，还为动量重要性提供理论支持，且给予实际学习率调整的指导，有助于理论与实践的结合。

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [266] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: KnowIt是一个用于深度时间序列模型构建与解释的灵活Python工具包。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析工具限制较多，难以适应多样化任务，且解释性不足。

Method: KnowIt通过定义统一接口，解耦数据集、网络架构和解释技术，实现灵活导入数据与自定义模型。

Result: 该框架支持多样时间序列数据建模与多角度解释，易于扩展和定制。

Conclusion: KnowIt致力于成为深度时间序列建模与知识发现领域的可信平台，推动该领域发展。

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [267] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: 本文介绍了Kamae，一个开源Python库，用于在生产推荐系统中实现训练与推理环境中一致的特征预处理，通过将PySpark预处理流程转换为等效的Keras模型，避免了重复逻辑和数据偏移风险。


<details>
  <summary>Details</summary>
Motivation: 生产推荐系统中，特征预处理必须在训练和推理环境中保持一致，但通常需要在线下和线上环境中重复实现相同逻辑，这增加了工程工作量并引入了数据偏移的风险。

Method: 提出了Kamae库，通过将PySpark的预处理管道翻译成Keras模型，提供一套可配置的Spark转换器和估计器，这些组件对应于Keras层，实现了机器学习生命周期中一致的端到端预处理。

Result: Kamae框架在实际案例中得到了验证，包括MovieLens数据集和Expedia的排序学习管道，证明了其实用性。

Conclusion: Kamae有效桥接了线下和线上预处理逻辑差异，简化了工程实现并降低了数据偏移风险，提升了推荐系统的生产效率和可靠性。

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [268] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: 本文提出了一种在高维低样本量环境下使用多视图中间融合技术的通用学习方法，通过构造多视图并验证其有效性，解决了维度远大于样本量的问题。


<details>
  <summary>Details</summary>
Motivation: 高维低样本量（HDLSS）环境中，特征维度远远超过样本数量，现有方法面临挑战，迫切需要新的有效学习方法。

Method: 作者提出三种视图构造方法，将高维特征向量拆分成更小的子集，每个子集作为不同的视图，利用多视图中间融合技术进行学习，无需预设固有视图。

Result: 实验在不同模型和任务中广泛验证了所提方法的有效性和泛化能力，显示出良好的表现。

Conclusion: 该方法为多视图中间融合学习在HDLSS环境下的普适优势奠定了基础，促进未来相关研究。

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [269] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: EdgeCodec是一种为风力涡轮叶片气压数据设计的端到端神经网络压缩器，实现高压缩率和低重构误差，支持实时运行和动态码率调整。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮叶片上的传感器产生大量气压数据，需要高效压缩以减少无线传输能耗，延长传感器运行时间。

Method: 设计一个非对称自编码器体系结构，结合鉴别器和残差向量量化器进行训练，以最大化压缩效率，并能动态调整码率。

Result: 实现2560:1到10240:1的压缩率，重构误差低于3%，在GAP9微控制器上实时运行，码率11.25至45比特/秒，动态调整。

Conclusion: EdgeCodec在最高压缩率下，可将无线数据传输能耗降低2.9倍，显著延长传感器单元的使用寿命。

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [270] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 该论文提出了一种结合轻量级数值代理和卷积神经网络的局部-全局卷积神经网络（LGCNN）方法，用于高效模拟具有异质性流动条件的地下水热输运过程。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在科学与工程领域受限于有限或低质量训练数据，难以准确预测复杂的地下水流和热传输过程。传统数值模拟计算成本高，纯数据驱动模型对对流过程的预测鲁棒性差。

Method: 提出LGCNN模型，将全局轻量数值代理与局部卷积神经网络结合，分别模拟运输过程和水流速度及热扩散过程。通过在随机输入场景下分析，并在德国慕尼黑地区实际地下水数据切片中训练，实现对更大区域预测的无须再训练能力。

Result: LGCNN成功模拟了城市规模的复杂地下水温度场，包括多点注入形成的热羽流，展示了对异质地下水流场和热扩散过程的准确建模和良好的泛化能力。

Conclusion: 该方法有效提高了地下水热输运模拟的计算效率和预测精度，解决了传统数值模拟和纯数据驱动模型的缺陷，具有广泛应用潜力。所有数据和代码已开源保证可复现性。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [271] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: 本文研究了结构化状态空间模型（SSM）在资源受限设备上的量化感知训练（QAT）方法及其对模拟内存计算硬件的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对SSM的量化感知训练虽有所探讨，但缺乏针对模拟内存计算芯片等边缘硬件实施细节的分析。

Method: 通过量化感知训练，分析模型规模与数值精度的关系，提升对模拟噪声的鲁棒性，并结合结构剪枝技术，将SSM部署至忆阻式模拟内存计算平台。

Result: QAT可使SSM复杂度显著降低最多两个数量级，同时增强模型对模拟噪声的抵抗力和支持结构剪枝，提升计算效率。

Conclusion: 量化感知训练结合结构剪枝有效促进了SSM在模拟内存计算边缘设备的高效部署，显著优化了计算资源利用率。

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [272] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 本文提出了一种利用潜在空间中推理链条嵌入（CoRE）进行无标签自我评估的方法，从而提升大规模推理模型的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型在推理过程中存在过度思考的问题，导致推理步骤冗余且效率低。如何让模型自主评估自身推理过程的正确性成为关键挑战。

Method: 提出Chain-of-Reasoning Embedding（CoRE）来表示推理过程的隐藏状态，通过分析CoRE的几何特性发现冗余推理表现为循环波动。基于此，设计无训练、无标签的自我评估框架CoRE-Eval，动态判断推理是否可提前结束。

Result: 在数学推理基准（GSM8K、MATH-500、AIME）和不同规模模型（7B至32B）上测试，CoRE-Eval将推理链长度缩短13.7%至33.2%，同时提升结果准确率约10%，32B模型在AIME上达到70.0%的准确率。

Conclusion: 利用潜在空间的推理链嵌入实现无标签自我评估，有效减少冗余推理步骤，提升模型推理效率和准确性，为大规模推理模型的自我监控提供有效途径。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [273] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: 提出了一种基于子空间近似Hessian的零阶优化方法ZO-SAH，通过选择二维子空间拟合二次多项式估计Hessian，显著加速了无梯度信息情况下的优化收敛。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法主要依赖一阶近似，而加入二阶曲率信息虽有助于加速收敛，但高昂的函数评估成本限制了其实际应用。

Method: ZO-SAH方法在随机选择的二维子空间中拟合二次多项式估计Hessian矩阵的二阶系数，并通过周期性切换子空间策略复用函数评估，降低查询成本。

Result: 在八个基准数据集（包括逻辑回归和深度神经网络训练任务）上的实验表明，ZO-SAH相比现有零阶方法实现了显著更快的收敛速度。

Conclusion: ZO-SAH通过在低维子空间中有效估计二阶信息，显著提高了零阶优化的效率，为无梯度优化提供了实用且高效的解决方案。

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [274] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: 本文提出了一种结合多种矩阵分解技术的层次链接预测框架，用于从材料科学文献中推断隐含关联，促进跨学科探索。


<details>
  <summary>Details</summary>
Motivation: 科学文献网络和知识图谱通常大规模、稀疏且噪声多，存在缺失的节点链接，亟需有效方法揭示潜在关联，推动科研发现。

Method: 融合分层非负矩阵分解（HNMFk）、布尔矩阵分解（BNMFk）和逻辑矩阵分解（LMF），构建三层主题树，利用BNMFk与LMF的集成方法结合离散可解释性和概率评分。

Result: 构建了基于73种过渡金属二硫族化合物的文献主题树，模型成功预测并验证了材料与超导主题间的隐含关联，指示了新的跨学科假设。

Conclusion: 所提框架能有效发掘科学文献中隐性连接，尤其适合多元文献资源的交叉分析，并通过交互式平台促进人机协同科学发现。

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [275] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: 本文探讨卷积层中步幅引起的混叠现象及其对数值稳定性和统计泛化的影响，提出了基于框架理论的方法来分析和抑制混叠，确保Parseval稳定性。


<details>
  <summary>Details</summary>
Motivation: 卷积层中使用步幅会引入混叠，影响数值稳定性和模型的泛化能力，目前缺乏针对混叠及其对稳定性的系统分析。

Method: 采用框架理论对1D卷积核中的混叠进行建模，导出稳定性界估计和Parseval稳定性的特征，设计两个高效优化目标抑制混叠，推导随机卷积核下混叠效果的期望和方差表达式。

Result: 提出的方法能有效促进Parseval稳定性，通过系统地抑制混叠提高数值稳定性，解析了初始化时混叠的基本行为。

Conclusion: 基于框架理论的混叠分析方法为设计稳定且具有良好泛化能力的卷积神经网络提供了理论支持和实用优化目标。

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [276] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: 提出了一种部分优化深度可微逻辑门网络连接的新方法，通过在门输入连接子集上使用概率分布选择连接，显著减少逻辑门数量且提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少深度可微逻辑门网络中连接数量，提高模型效率和性能。

Method: 利用概率分布部分优化每个门输入的连接，选择最优连接后确定门类型。

Result: 在Yin-Yang、MNIST和Fashion-MNIST数据集上，优化连接的LGN性能优于固定连接LGN，且逻辑门使用量减少到1/24。

Conclusion: 实现了高效且性能优越的可部分训练布尔逻辑网络，推动深度可微布尔逻辑的发展。

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [277] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 该论文研究了差分设计技术在选择性状态空间架构Mamba中的应用，提出了一种新颖的差分机制，显著提升了模型的检索能力和性能，缓解了注意力过度分配问题。


<details>
  <summary>Details</summary>
Motivation: 序列模型如Transformer和RNN常常将过多注意力分配给无关上下文，导致中间表示噪声增多、模型能力下降，包括幻觉生成、长期依赖和检索能力减弱以及鲁棒性降低。尽管差分设计已在Transformer中得到验证并改善了其表现，尚不清楚此技术能否迁移到效率更高的Mamba架构。

Method: 对差分设计方法进行适配与修改，设计出适合Mamba的新型差分机制，并在语言建模基准上进行实证验证。

Result: 新提出的差分机制相比于原始Mamba模型提升了检索能力和整体性能。

Conclusion: 精心设计的差分机制有效缓解了Mamba模型中过度注意力分配的问题，提升了模型性能和能力，验证了差分设计方法在状态空间模型上的可行性和效果。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [278] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: 本文介绍了联想记忆（如Hopfield网络）作为完全递归神经网络的优雅模型，重点介绍其信息存储能力及其与先进AI架构（如Transformer和扩散模型）的联系。


<details>
  <summary>Details</summary>
Motivation: 近年来联想记忆因其新颖的理论结果和与先进AI架构的关联而受到关注，这为理解传统AI网络计算提供了新的视角。

Method: 本文采用现代语言和方法，结合拉格朗日方法对联想记忆网络进行数学推导，并提供编程实践代码。

Result: 通过拉格朗日新形式设计了强大分布式模型，提升了表示学习能力，推动新架构设计。

Conclusion: 本文以教程形式系统介绍了联想记忆，强调理论与实践结合，为理解和设计现代神经网络提供了理论基础和应用指导。

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [279] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: 该论文提出了一种利用神经网络优化有固定位置的挤压天线（PAs）激活方案，以最大化无线通信速率的方法。


<details>
  <summary>Details</summary>
Motivation: 现代无线通信系统需要灵活、高效且低成本的天线技术，挤压天线由于其动态控制电磁波的特性成为研究热点。

Method: 将天线激活问题建模为组合分数0-1二次规划问题，利用神经网络结合空间特征和信号结构，直接从数据中学习激活策略，并考虑用户位置不确定性进行训练和评估。

Result: 仿真结果表明所提模型在最大化通信速率方面表现有效且具有鲁棒性。

Conclusion: 该研究证明了基于神经网络的数据驱动方法在复杂的天线激活优化问题中具有较好的性能和实用潜力。

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>
