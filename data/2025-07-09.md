<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 162]
- [cs.CL](#cs.CL) [Total: 114]
- [cs.LG](#cs.LG) [Total: 142]
- [cs.AI](#cs.AI) [Total: 82]
- [cs.RO](#cs.RO) [Total: 58]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 本文提出通过在训练过程中强制一致的字幕结构来提高生成文本到图像模型的提示依赖性问题，构建了一个19百万高质量带结构化字幕的图像数据集Re-LAION-Caption 19M，显著提升了模型的文本-图像对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有大型数据集如LAION-5B数据噪声大且结构不统一，导致生成模型难以准确遵守提示词，需大量提示工程。

Method: 构建了包含19百万高分辨率图像及按照主题、场景、美学、摄影细节四部分结构化字幕的Re-LAION-Caption 19M数据集，利用该数据集对PixArt-Σ和Stable Diffusion 2进行微调，并对比使用结构化和随机字幕的效果。

Result: 采用结构化字幕训练的模型在视觉问答（VQA）任务中表现出更高的文本与图像对齐评分，证明了结构化字幕对模型控制性和对齐性的正面影响。

Conclusion: 强制一致的字幕结构训练能有效提高文本到图像生成模型的可控性和提示词对齐能力，相关数据集已公开，具备较大应用价值。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [2] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 提出了一种名为CorrDetail的可解释性面部伪造检测框架，结合视觉细节增强和自我纠正机制，提升伪造细节识别能力和检测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前面部伪造检测技术存在缺乏可解释性或易产生错误信息的问题，亟需一种能够准确揭示伪造细节且鲁棒性强的方法。

Method: 设计了视觉细节增强模块和基于错误引导提问的自我纠正机制，结合视觉信息补偿和模型偏差减少的融合决策策略，提高伪造细节识别和决策能力。

Result: 实验表明，CorrDetail不仅在性能上超越最新方法，还能精准识别伪造细节，具有良好泛化性。

Conclusion: CorrDetail有效提升了面部伪造检测的准确性和可解释性，具备较强的应用潜力。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [3] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: 本文提出了YOLO-APD，一种改进YOLOv8的行人检测算法，针对复杂几何路面提升检测精度和效率，在定制数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶视觉感知系统在复杂几何路面（如Type-S曲面）上存在行人检测困难，传统基于RGB摄像头的方法效果有限。

Method: YOLO-APD引入了参数免费的SimAM注意力机制、高效的C3Ghost模块、新的SimSPPF多尺度特征池化模块、Mish激活函数和智能聚集分发（IGD）模块，并结合车辆转向动态实现自适应兴趣区域处理。

Result: 在CARLA定制数据集上，YOLO-APD达到了77.7%的mAP@0.5:0.95和超过96%的行人召回率，显著优于YOLOv8基线，同时保持100FPS的实时处理速度。在KITTI数据集验证中表现良好，但需进行领域适应。

Conclusion: YOLO-APD在复杂非结构化驾驶环境中实现了高精度、高效和适应性强的行人检测，推动了基于低成本传感器的自动驾驶感知系统的发展，提高了安全性与可靠性。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [4] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: 本文提出了一种名为Spotlight的虚拟染色方法，利用机器学习从无标记的显微镜图像预测荧光图像，重点关注细胞结构，提升了形态学表示与像素精度。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色方法在训练时对所有像素一视同仁，导致背景噪音和伪影被重现，无法专注于有生物学意义的信号。

Method: Spotlight采用基于直方图的前景估计来掩盖像素级损失，并通过软阈值预测计算Dice损失，实现形状感知学习。

Result: 在三维基准数据集上，Spotlight提升了形态学表示能力，同时保持了像素级精度，生成的虚拟染色更适用于分割和细胞轮廓分析等下游任务。

Conclusion: Spotlight方法有效引导模型关注相关细胞结构，改善虚拟染色的质量和实用性。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [5] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: 本文评估了基础模型在农业监测中的应用效果，并提出了理想农业基础模型的需求框架，强调了开发专门农业基础模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着人口增长和气候变化加剧，全球粮食安全面临挑战，需求创新的农业生产力解决方案。基础模型在遥感和气候科学中表现优异，具备在农业监测中应用的潜力，但相关应用尚未充分探索。

Method: 从农业领域需求出发，构建理想农业基础模型（CropFM）框架，调研比较现有通用基础模型，并在三类农业任务（作物类型映射、作物物候估计、作物产量估计）中对两种模型进行定量评估。

Result: 评估显示现有通用基础模型在农业特定任务上的表现各异，存在不足，支持开发专门的农业基础模型以更好满足农业监测需求。

Conclusion: 现有通用基础模型不足以全面满足农业监测需求，亟需设计针对农业的专属基础模型以提升农业任务的效果和效率。

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [6] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的方法，结合人工主观评估，通过训练分类器和生成对抗网络提升水下图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下图像通常质量较差，需要自动增强方法以提升图像清晰度和视觉效果。

Method: 使用带有专家标注的公开水下图像数据集训练分类器判别图像质量，随后基于多种增强标准训练生成对抗网络优化低质量图像。

Result: 生成对抗网络模型在PSNR、SSIM和UIQM等指标上表现优异，尤其在颜色保真度和图像锐度方面提升显著。

Conclusion: 结合主观评估和多重增强指标的深度学习模型有效提升了水下图像的视觉和测量质量。

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [7] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 提出了pFedMMA，一种结合多模态适配器的个性化联邦学习框架，有效提升视觉-语言模型在去中心化异构数据上的适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在去中心化且异质数据环境中高效适配仍有挑战，且现有基于提示调优的方法常在个性化与泛化之间难以平衡。

Method: 设计了包含模态特定上/下投影层及全局共享投影的多模态适配器，并通过非对称优化策略实现本地个性化和全局泛化的协同训练，且仅共享全局部分以节省通信成本。

Result: 在包含域和标签漂移的11个数据集上，pFedMMA表现出个性化和泛化的最佳折中，优于最新的联邦提示调优方法。

Conclusion: pFedMMA有效解决了视觉-语言模型在个性化联邦学习中的泛化与个性化权衡问题，具备通信效率高的优势。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [8] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: 本文提出了LoongX，一个基于多模态神经生理信号的无手图像编辑方法，结合脑电图、功能近红外光谱等信号，利用扩散模型实现直观高效的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑依赖手动输入，操作繁琐且对行动或语言能力受限的人群不友好，因此提出利用脑机接口信号实现无手图像编辑。

Method: LoongX通过跨尺度状态空间模块提取多模态信号特征，动态门控融合模块整合特征，并通过对扩散变换器微调实现编辑语义对齐，同时利用对比学习预训练编码器以对应认知状态与语义意图。

Result: 实验显示LoongX在多种评测指标上达到或超过传统文本驱动方法，且结合神经信号与语音时表现更优，验证了该方法的有效性。

Conclusion: 神经驱动生成模型在实现无手、直观的图像编辑方面展现出巨大潜力，为认知驱动的创意技术开辟了新方向。

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [9] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 本文对2023年以来动作生成的最新研究进行了系统性综述，重点根据生成策略对方法进行分类，分析架构、条件机制和生成设置，并总结评价指标和数据集，旨在帮助研究者更好理解和比较现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着动作生成领域迅速发展，出现了多种生成范式和方法，研究者需要一个全面系统的综述，以便更清晰地比较不同方法及识别开放问题。

Method: 基于生成策略对多种动作生成方法进行深入分类，分析其架构原则、条件输入、生成设置，并汇总评价指标和数据集。综述重点覆盖2023年之后的顶会论文，反映最新进展。

Result: 本文归纳了动作生成领域多样的生成模型及其优缺点，系统整理了评价标准和数据集，为该领域研究提供了全面参考和比较基础。

Conclusion: 通过详尽的分类和分析，本文为动作生成研究提供了一个及时且系统的基础参考，帮助推动领域未来的发展和挑战解决。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [10] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D扩散编辑和逆向渲染的3D高斯点阵局部编辑方法，实现了视角一致的高效精细修改。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义解析性能不足，难以在3D场景中精确定位和编辑局部区域，限制了编辑的质量和控制能力。

Method: 利用2D扩散编辑准确识别修改区域，通过逆向渲染实现3D定位，结合2D基础模型预测深度图初始化粗略的3D高斯点阵，支持迭代视角一致的细节和纹理增强。

Result: 实验表明该方法达到了最先进的性能，编辑速度提高了4倍，提升了效率和效果。

Conclusion: 该方法为3D场景的局部编辑提供了更精准、高效和视角一致的解决方案，显著改进了编辑质量和操作便捷性。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [11] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: OpenWorldSAM是一种基于Segment Anything Model v2扩展的开域分割框架，能处理未见类别，实现灵活高效的开域语义和实例分割。


<details>
  <summary>Details</summary>
Motivation: 当前模型难以基于开放语言提示进行准确的空间对象分割，尤其面对多样化和未见类别时表现不足。

Method: OpenWorldSAM结合轻量级视觉语言模型的多模态嵌入，采用统一提示、多样化语言描述、冻结预训练模块仅训练4.5M参数，并通过定位破局嵌入与交叉注意力提升实例感知能力。

Result: 在多个数据集（如ADE20k、PASCAL等）上，OpenWorldSAM实现了开词汇语义、实例和全景分割的最新性能，展现强大的零样本泛化能力。

Conclusion: OpenWorldSAM有效结合多模态信息和高效训练策略，成功实现开域分割任务中的灵活性、资源效率和泛化能力。

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [12] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 本文开发了一种基于视觉引导和AI驱动的变速喷雾系统，能够实时检测杂草、估计冠层大小，并动态调整喷嘴控制，实现精准施药。


<details>
  <summary>Details</summary>
Motivation: 传统农业中均匀且过量的除草剂喷洒带来高成本、环境污染及抗药性杂草问题，亟需精准施药技术减少资源浪费和环境负担。

Method: 建设了集成轻量级YOLO11n和YOLO11n-seg深度学习模型的系统，部署在NVIDIA Jetson Orin Nano平台，通过Arduino Uno控制电磁阀喷嘴，实现基于冠层分割结果的动态喷雾。

Result: YOLO11n模型在室内试验中表现优异，mAP@50达0.98，喷雾覆盖率与冠层大小正相关，最大达24.22%，验证了系统能实时依据冠层调整喷雾强度。

Conclusion: 结合实时深度学习与低成本嵌入式硬件的定点除草剂喷洒系统展示出良好的应用潜力，未来将扩展检测多种杂草并进行室内及田间验证。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [13] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 该论文通过分析自然驾驶视频和大型视觉模型，旨在早期识别老年驾驶员的认知状态，特别是轻度认知障碍和阿尔茨海默病。


<details>
  <summary>Details</summary>
Motivation: 认知衰退（包括阿尔茨海默病和轻度认知障碍）诊断昂贵且耗时，常被漏诊。利用驾驶行为作为认知状态的数字指纹有助于早期检测。

Method: 提出一种结合大型视觉模型和自然驾驶视频的框架，分析驾驶行为以分类认知状态并预测疾病进展，将车辆作为诊断工具。

Result: 该方法能识别功能损害的早期预警信号，实现对认知衰退的早期检测。

Conclusion: 研究提升了早期检测能力，支持非侵入式、可扩展的监测系统开发，有助于减轻老年认知衰退带来的社会与经济负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [14] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 本文提出了基于尺度不变噪声的云扩散模型，替代传统白噪声扩散模型，以改善图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用的白噪声与自然图像的尺度不变统计特性不符，限制了模型性能。

Method: 将具有幂律尺度不变特性的噪声引入扩散模型，形成云扩散模型。

Result: 云扩散模型预计可实现更快推断、更丰富高频细节和更强控制能力。

Conclusion: 利用自然图像的尺度统计特性设计扩散模型有望提升图像生成效果，后续工作将进行实证比较。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [15] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet提出了一种新颖的多视角扩散架构，通过共享潜编码空间，实现了从单张图像生成多视角一致性图像，显著提升了图像质量和三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在从单张图像生成多视角图像时难以保证视角之间的空间一致性，导致三维网格重建质量下降。

Method: LoomNet采用相同的扩散模型多次并行处理，结合三正交平面上的视角特定编码，通过编码融合和信息传播，构建统一且一致的潜编码空间，最终渲染多视角图像。

Result: LoomNet在15秒内生成16个高质量且一致的视角图像，实验中在图像质量和重建指标上均优于现有最先进方法，并能创造性地生成多样且合理的新视角。

Conclusion: LoomNet有效解决了单图生成多视角图像的一致性问题，提升了图像质量和三维重建表现，展现了较强的创造力和实用价值。

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [16] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 该论文提出了统一的文本-图像检索模型llama-nemoretriever-colembed，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了满足跨模态检索系统日益增长的需求，提出一个统一模型提升检索性能。

Method: 基于NVIDIA Eagle2视觉语言模型，采用双向注意力替代因果注意力，结合ColBERT风格的晚期交互机制，并通过两阶段训练策略提升检索能力。

Result: 3B模型在ViDoRe V1和V2数据集上分别取得91.0和63.5的NDCG@5分数，均名列榜首。

Conclusion: 该模型在文本-图像跨模态检索领域实现了领先性能，同时对存储和效率的权衡进行了详尽分析。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [17] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种程序化增强管道，通过模拟光学折射和天气引起的失真，增强非洲驾驶场景的低成本单目行车记录仪视频，提升自动驾驶感知能力。


<details>
  <summary>Details</summary>
Motivation: 非洲等发展中地区自动驾驶数据集匮乏，尤其是在不同类型道路上的数据缺失，限制了自动驾驶感知技术的有效应用。

Method: 设计了包括光学折射模块和天气模块的增强管道，模拟镜头失真、空气湍流、雾霾和镜头眩光等影响，并对生成的数据进行基线性能测试。

Result: 构建了带有真实折射和天气失真效果的数据集，并提供了三个图像恢复模型的基线测试结果。

Conclusion: 该工具包和数据集将推动低资源环境下自动驾驶感知的研究，尤其促进非洲驾驶场景的相关研究，无需高成本数据采集和标注。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [18] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReLayout的新方法，利用relation-CoT和显式关系定义，改进大语言模型在内容感知布局生成中的表现，生成结构更合理、美观多样的设计布局。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的布局生成方法无法充分理解视觉主题和设计元素之间的空间关系，导致生成布局存在结构和多样性问题。

Method: 引入显式关系定义（如区域、显著性、边距）细化布局注释，并将布局分解为更小、结构化和递归的子布局；基于这些关系设计了布局原型重平衡采样器，解决样本偏差带来的生成均匀性问题。

Result: 实验结果表明，ReLayout在生成结构合理、风格多样且符合人类审美的布局方面优于基线方法，且生成结果更具解释性。

Conclusion: 通过引入关系推理和采样器，ReLayout有效提升了内容感知布局生成的质量，实现了更结构化、多样且美观的设计布局。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [19] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的跨模态转换引导网络（CTNet）用于多模态人脸反欺骗，旨在提升多模态识别在跨域和缺失模态情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态人脸反欺骗因不同模态间数据分布差异大及部分模态缺失，导致识别难度加大。活体人脸在单模态内差异小，跨模态特征转换更一致，为解决该问题提供思路。

Method: 设计跨模态转换引导网络，学习活体样本一致的跨模态特征转换构建泛化特征空间，同时学习活体与欺骗样本不一致的转换检测OOD攻击，并通过从RGB模态中辅助生成红外和深度特征以应对模态缺失。

Result: 实验结果表明，CTNet在多种协议下均优于现有两类多模态人脸反欺骗方法，展示出更好的泛化能力和鲁棒性。

Conclusion: CTNet有效解决了多模态人脸反欺骗中跨域分布不一致和模态缺失的问题，提升了系统的安全性和实用性。

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [20] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 该文提出了一种基于条件扩散模型的半监督缺陷检测框架DSYM，通过两阶段协同训练和分阶段联合优化，有效利用有标签和无标签数据提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业质检中的缺陷检测依赖人工或早期图像处理方法，存在效率低、成本高及鲁棒性差的问题，需要利用半监督学习提升检测效率和数据利用率。

Method: 本文设计了一个基于条件扩散模型的半监督缺陷检测框架，采用两阶段协同训练机制和分阶段联合优化策略，结合CLIP跨模态特征进行噪声过滤，生成多尺度伪缺陷样本，提升伪标签质量。

Result: 在NEU-DET数据集上，提出方法以相同有标签数据实现了78.4%的mAP@0.5指标；仅用40%有标签数据时依然取得75.1%的mAP@0.5，显示出优越的数据效率和检测性能。

Conclusion: 该框架为工业质检缺陷检测提供了一种高精度、低标注依赖的解决方案，有效提升了半监督学习在工业缺陷检测中的应用价值。

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [21] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于二维高斯的快速视频隐式表示方法GSVR，实现了高帧率解码和快速训练，显著提升了视频隐式表示的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积网络的视频隐式表示方法训练时间长且解码速度慢，限制了其应用。

Method: 设计混合形变场结合三平面运动和多项式运动，提出基于动态的时间切分策略，以及量化感知微调和高斯压缩方法以提升效率和表现。

Result: 在Bunny和UVG数据集上达到35+ PSNR，训练时间降至2秒/帧，解码速度超800 FPS，解码速度比现有方法提升10倍，视频插值任务性能与SOTA相当，视频压缩优于NeRV。

Conclusion: GSVR有效解决了视频隐式表示中的训练速度和解码效率问题，具有较高的实用价值和推广潜力。

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [22] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0是一个开源OCR和文档解析工具包，具备多语种文本识别、层级文档解析和关键信息提取功能。


<details>
  <summary>Details</summary>
Motivation: 为满足大语言模型时代对文档理解日益增长的需求，提升OCR及文档解析效率和准确性。

Method: 提出PP-OCRv5、多语种文本识别，PP-StructureV3层级文档解析，PP-ChatOCRv4关键信息提取三大方案，模型参数少于1亿但准确率高。

Result: 模型在保持较小参数量的同时，准确率和效率与大规模视觉语言模型相当，且支持异构硬件加速。

Conclusion: PaddleOCR 3.0提供高质量模型库和高效训练推理部署工具，方便开发者构建智能文档应用。

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [23] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了Accordion框架，将AI生成的像素图设计转换为可编辑的分层设计，并通过用户提示优化文本内容。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI能提供高质量的非分层设计，但这类设计缺少编辑性，而人类设计师仍受其启发，因此希望实现从像素图到分层设计的转换。

Method: 利用视觉语言模型在三个阶段执行不同任务，采用自上而下的方法基于参考图像全局指导分层设计，结合多种视觉专家模型辅助生成分层元素。

Result: 在自建数据集Design39K上训练并测试，实验证明Accordion在DesignIntention基准任务中表现优异，包括文字到模板、添加文字到背景和文字反解码等任务，并在设计变体生成方面效果出色。

Conclusion: Accordion有效实现了AI像素图设计向可编辑分层设计的转变，提升了AI设计作品的实用性和灵活性，具有较高的应用价值。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [24] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Kernel Density Steering (KDS)的新推断框架，通过集体局部模式搜索提升扩散模型的图像修复质量，显著减少伪影，增强保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像修复时存在保真度不稳定和伪影问题，需一种方法提升生成图像的质量和稳定性。

Method: KDS利用N个扩散样本形成的粒子集，通过计算它们在图像块级别的核密度估计梯度，引导每个粒子的图像块向集体中高密度区域聚集，实现“集体智慧”的局部模式搜索。该方法作为插件框架，无需重训练即可应用于不同扩散采样器。

Result: 大量数值验证显示，KDS在真实世界的超分辨率和图像修复任务中，显著提升了生成图像的定量指标和视觉质量。

Conclusion: KDS通过利用多样本集体智慧，有效避免了独立采样或模型缺陷导致的伪影问题，显著改善扩散模型的图像修复性能，且易于集成应用。

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [25] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的生成方法GenHMC，通过利用大量无配对的头戴摄像(HMC)捕获数据，生成高质量的合成HMC图像，实现了虚拟现实和增强现实中头像动画的更精确表达和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以获得同步的真实脸部状态图像，且依赖大量的配对数据采集，收集成本高且难以复用，不利于虚拟和增强现实中实现真实感的头像动画。

Method: 提出GenHMC，通过生成模型直接基于大量无配对的HMC数据，结合外部球面摄像头捕获的头像状态，生成高质量的合成HMC图像，实现表情、视角和面部外观的解耦和准确建模。

Result: 方法有效解耦了面部表情和视角与面部外观，实现了更准确的地面真实数据生成，且能泛化到未见身份，训练的面部编码器表现出更高的数据效率和先进的准确率。

Conclusion: GenHMC突破了传统依赖配对数据的限制，提升了头像动画的真实性和泛化能力，为VR/AR头像合成提供了有效的新思路。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [26] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: 本文提出AdaptaGen，一种层次化语义优化框架，结合矩阵式提示优化与多角度语义理解，有效提升领域特定图像生成的质量和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定图像生成方法存在语义理解与视觉表现分离、缺乏领域语义约束导致幻觉和语义偏差问题。

Method: 提出AdaptaGen框架，通过跨模态适配机制和智能内容合成保持核心主题，同时设计两阶段标题语义转换以保证语义连贯和视觉多样性。

Result: 在40个类别的多样本数据集上，仅用每类16张图像，显著提升图像质量、多样性和语义一致性。

Conclusion: AdaptaGen有效整合语义和视觉信息，解决领域特定图像生成的关键问题，实现高质量和语义准确的图像生成。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [27] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一个用于复合图像检索的新方法，通过分割和双重聚焦映射减少噪声影响，利用文本引导对焦点进行自适应调整，提高检索效果。


<details>
  <summary>Details</summary>
Motivation: 当前复合图像检索方法忽视了视觉数据中主要与噪声部分的不同以及文本在图像修改中的优先作用，导致查询特征退化和视觉聚焦偏差。

Method: 提出了基于聚焦映射的特征提取器，包括主要部分分割和双重聚焦映射模块，识别图像中的主要部分，减少噪声影响；并设计文本引导的聚焦修正模块，根据文本修改需求自适应调整参考图像上的聚焦。整体构建了基于分割的聚焦偏移修正网络（OFFSET）。

Result: 在四个基准数据集上的全面实验显示，所提方法在复合图像检索任务中表现优越。

Conclusion: 通过聚焦映射和文本引导的聚焦修正，有效提升了复合图像检索的性能，验证了方法的有效性和优越性。

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [28] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种基于复数轮廓波变换的结构知识引导复数扩散模型，用于极化合成孔径雷达图像分类，提升了边缘细节保护和区域一致性。


<details>
  <summary>Details</summary>
Motivation: 传统实值扩散模型难以捕捉极化SAR数据的复数相位信息，且难以保持细结构细节。

Method: 利用复数轮廓波变换将数据分解为低高频分量，设计知识引导的复数扩散网络建模低频统计特征，同时用高频结构特征引导扩散过程，多尺度多方向学习高频特征提升分类精度。

Result: 在三个真实极化SAR数据集上，所提方法显著优于现有技术，尤其在边缘细节保护和复杂地形区域的同质性维护方面表现突出。

Conclusion: 结合复数轮廓波变换与结构知识引导的复数扩散模型有效提升了极化SAR图像分类的性能，特别是细节保持和结构信息利用方面。

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [29] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种动态秩自适应（DRA）方法，通过根据特征重要性动态分配适应秩，提升视觉-语言模型在新类上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对大规模视觉-语言模型的微调方法难以保持对新类别的强泛化能力，原因在于对所有编码器token一视同仁，导致过拟合不重要的特征。

Method: DRA利用序列注意力进行token重要性分组，根据重要性动态分配高低秩适应，并设计通道响应机制优先保留和适应最信息量大的特征通道，同时引入L1正则化稳定训练。

Result: 大量实验表明，DRA在多个基准测试，包括基础-新类分类、跨数据集评估和领域泛化等任务上均优于现有方法，尤其提升了新类性能。

Conclusion: DRA通过动态调整适应秩和通道响应机制，有效防止过拟合，提高了视觉-语言模型对新类别的泛化能力，具有广泛应用前景。

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [30] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种结合RGB摄像头与事件传感器的飞行器姿态估计方法，通过传感器融合克服单一传感器的不足，在复杂光照条件下实现更准确的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB摄像头的姿态估计在恶劣光照条件下面临成像伪影问题，而事件传感器虽在动态范围上表现更好，但空间分辨率低且在低运动时信噪比下降。为了克服各自的局限，提出传感器融合方法。

Method: 采用光学分束棱镜实现RGB和事件传感器的光学与时间对齐，利用基于RANSAC的算法融合两种传感器数据实现姿态估计，同时引入dropout不确定性估计检测极端影响条件。

Result: 在实验室条件下采集了多种复杂光照下的卫星姿态数据，实验证明了事件-RGB融合方法的有效性，提升了姿态估计的鲁棒性与准确度。

Conclusion: 事件传感器与RGB传感器的融合显著提升了航天器姿态估计的性能，支持事件传感器在该领域的应用，并公开了数据集以促进相关研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [31] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 提出基于扩散模型的脑损伤分析与逆转框架，实现核心病灶分离及损伤前脑组织重建，提升病灶分割与表征准确性。


<details>
  <summary>Details</summary>
Motivation: 现有脑损伤分割方法未区分不可逆损伤组织和因病灶生长导致变形的组织，缺乏有效的病灶过程模型及逆向分析方法。

Method: 采用扩散模型框架，首先分割异常区域，接着估计并逆转组织变形，将移位组织恢复至原位，分离出初始损伤核心区域，最后对核心病灶部位进行修复以重建损伤前的健康脑组织。

Result: 该方法在病灶分割、特征表征及脑区标注方面较传统方法表现出更高准确性，通过合成多张模拟病灶脑图像验证逆向模拟效果。

Conclusion: 该框架有效解决了脑损伤不可逆损伤与组织变形的区分问题，且实现了对病灶进展过程的逆转，具备重要的临床及科研价值。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [32] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: 本文提出了R-VLM，一种利用放大区域提议和IoU感知目标函数的GUI元素精确定位方法，显著提升了不同GUI平台上的元素定位和导航任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的GUI代理方法需要处理大量无关信息，且采用的交叉熵损失函数无法有效反映元素定位质量，导致定位精度不足。

Method: 提出R-VLM方法，通过放大关注区域进行更精确的元素定位，同时设计了基于IoU的目标函数，使模型能够更好地学习高重合度的定位结果。

Result: 在ScreenSpot和AgentStudio基准测试中，R-VLM使元素定位准确率提高了13%；在AITW和Mind2Web导航任务中，准确率提升3.2%至9.7%。

Conclusion: R-VLM成功结合了视觉语言模型与目标检测技术，显著提升了GUI元素定位的准确性和自动化导航任务的性能。

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [33] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 本文提出了首个大规模、多样且带有丰富字幕的医疗视频生成数据集MedVideoCap-55K，及基于该数据集开发的医疗视频生成模型MedGen，实现了视觉质量和医疗准确性的领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗视频生成研究缺乏大规模高质量数据，导致生成结果不真实或错误，影响临床训练和教育等应用。

Method: 构建包含55,000多个医疗场景视频剪辑的MedVideoCap-55K数据集，训练和开发MedGen模型，提升医疗视频的生成质量与准确性。

Result: MedGen在多个基准测试中在视觉质量和医疗准确性上表现优异，达到开源模型领先水平，媲美商业系统。

Conclusion: 该数据集与模型为医疗视频生成领域提供了重要资源，推动了相关研究发展。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [34] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了集成结构提示（ISP）方法，通过建模多模态间提示与令牌的结构关系，提升预训练视觉语言模型在迁移学习中的表现，同时动态调整样本损失系数以增强新类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法未考虑提示和令牌之间的结构关系，且难以平衡基础类别与新类别的性能表现。

Method: 提出ISP，包含自我结构提示模块和跨结构提示模块，分别建模同模态与跨模态的结构关系；设计样本探测模块根据样本难度动态调整损失系数，防止模型对简单样本过拟合。

Result: 在基础到新类别泛化、跨数据集评估和领域泛化三大设置中，ISP展现出与最先进方法相媲美的竞争性性能。

Conclusion: ISP有效增强了视觉语言模型中多模态信息的交互和传递，改善了模型对新类别的泛化能力，具有显著的实用价值。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [35] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为LiON-LoRA的新框架，用于改进视频扩散模型中LoRA的融合，实现对摄像机轨迹和物体运动的精准控制。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法难以同时精准控制视频中的摄像机轨迹和物体运动，原因在于融合不稳定和非线性扩展问题。

Method: LiON-LoRA通过线性扩展性、正交性和范数一致性三大原则重构LoRA融合，利用浅层VDM特征正交性实现解耦控制，并在扩散变换器中引入可控token和自注意力机制以线性调节运动幅度。同时扩展到时序生成，实现空间和时间上的可控性统一。

Result: LiON-LoRA在轨迹控制准确性和运动强度调整方面优于现有方法，且在极少训练数据下表现出更好的泛化能力。

Conclusion: LiON-LoRA有效解决了视频扩散模型中LoRA融合的不稳定和非线性扩展问题，实现了对摄像机与物体运动的精确解耦控制，推动了视频生成中的空间与时序可控性发展。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [36] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 该论文综述了高光谱异常检测的多种方法，并对17个数据集上的性能进行了比较，发现深度学习模型在检测准确率方面表现最佳，而统计模型运算速度最快。


<details>
  <summary>Details</summary>
Motivation: 高光谱异常检测在多个领域有重要应用，但现有方法仍面临计算复杂度高、对噪声敏感及泛化能力不足等挑战。

Method: 对统计模型、基于表示的方法、传统机器学习和深度学习四类高光谱异常检测技术进行归类和综合比较，使用17个基准数据集及多种性能指标评估其效果。

Result: 深度学习模型在检测准确率方面表现出色，统计模型在计算速度上具有优势，不同方法各有优缺点。

Conclusion: 本文为高光谱异常检测领域的研究人员提供了全面的性能评估和未来研究方向的见解，促进技术进步和实际应用。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [37] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 本文提出了SenseShift6D，一个涵盖多种传感器曝光、增益、深度模式和光照条件的RGB-D数据集，用于评估6D物体位姿估计在真实环境变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有6D物体位姿估计数据集缺乏对真实环境中传感器设置和光照变化影响的考察，限制了模型在实际应用中的适应能力。

Method: 收集了13种RGB曝光、9种RGB增益、自动曝光、4种深度捕获模式及5种光照强度下的三种物体共101.9k RGB和10k深度图像。采用测试时传感器控制方法，动态调整传感器参数以提升位姿估计性能。

Result: 在SenseShift6D数据集上，测试时传感器控制相较于数字数据增强带来更大性能提升，且单独适应RGB或深度传感器均有效，联合适应效果最佳。性能提升可媲美或超过通过增加真实训练数据量和多样性获得的效果。

Conclusion: SenseShift6D推动6D位姿估计评测从数据中心向传感器感知鲁棒性转变，为自适应、自调节感知系统在复杂真实环境中实现鲁棒操作奠定基础。

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [38] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: 提出了一种用于显微镜图像的全自动白平衡算法，实验验证其优于传统白平衡方法。


<details>
  <summary>Details</summary>
Motivation: 显微镜中获取颜色准确、平衡的图像具有挑战性，传统白平衡算法不足以满足显微图像的需求。

Method: 设计了一种自动白平衡机制，专门用于显微镜图像的颜色校正，并在200幅显微图像上进行了实验验证。

Result: 算法在病理形态学常用显微样本以及免疫组化染色图像上表现优于传统数码摄影白平衡算法。

Conclusion: 该自动白平衡算法能够更有效地纠正显微镜图像颜色，提升图像质量，适合用于显微图像的颜色校正。

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [39] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt框架从单视角图像生成高保真的可交互关节物体，实现了部分分解和关节运动建模，提升了生成质量和应用范围。


<details>
  <summary>Details</summary>
Motivation: 当前图像到3D的方法忽略了部件分解与关节建模，神经重建方法依赖密集多视角数据，限制了扩展性，因此需要一种基于单视角图像的高品质可关节物体生成方法。

Method: DreamArt采用三阶段流程：1）结合图像到3D生成、带掩模提示的3D分割和部分非完整补全，实现带部件分割和完整的3D网格重建；2）微调视频扩散模型，利用移动部件掩模和非完整图像捕获部件级关节先验；3）优化双四元数表示的关节运动，并进行全局纹理细化和重绘保证纹理一致性。

Result: 实验表明DreamArt生成的关节物体具备准确部件形状、高保真外观和合理关节运动，效果优异且具备良好可扩展性。

Conclusion: DreamArt提供了一种有效的单视角图像驱动高质量、可交互关节物体生成解决方案，推动了关节资产生成技术的发展。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [40] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本指令的多功能虚拟试衣助手TalkFashion，实现全身服装更换和局部编辑，提升了灵活性和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法多为端到端单任务网络，缺乏多功能性和灵活性，用户体验受限。

Method: 利用大型语言模型理解用户文本指令，自动选择执行任务的处理流程；引入基于指令的局部重新绘制模型，结合多模态模型实现无人工遮罩的局部编辑。

Result: 实验表明TalkFashion在语义一致性和视觉质量上优于现有方法。

Conclusion: 通过结合大型语言模型和多模态模型，TalkFashion实现了灵活且自动化的多功能虚拟试衣，显著提升用户体验。

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [41] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 提出了SPADE框架，通过结合反演引导的微调和空间感知关系图变换器，显著提升开集场景图生成中的空间关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉语言模型的方法在空间关系推理方面存在局限，难以准确区分物体的相对位置，导致关系预测效果不佳。

Method: 采用扩散模型的反演过程保持空间结构，设计两步法：1) 反演引导的轻量微调，将通用扩散模型校准为适用于PSG的去噪网络；2) 空间感知关系图变换器，捕获局部及长距离上下文信息，提升关系查询质量。

Result: 在PSG和Visual Genome数据集上，SPADE在封闭集和开放集任务中均优于现有最先进方法，尤其在空间关系预测上表现突出。

Conclusion: SPADE有效结合扩散模型的空间结构保留能力和空间感知推理机制，显著改善了开集场景图生成中的空间关系理解与预测。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [42] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为DREAM的端到端自回归模型，用于文档重建，解决了现有方法中的误差传播和布局信息缺失问题，并引入了新的评估指标和数据集，实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档重建方法多为多阶段，存在误差传播导致性能下降的问题，且生成式模型虽端到端但忽视了元素布局信息，影响重建效果。

Method: 提出DREAM模型，将文本图像转化为包含丰富元素信息的文档重建序列，采用端到端自回归方式；并定义标准化重建任务，引入DSM指标和DocRec1K数据集进行评测。

Result: 实验证明DREAM在文档重建任务中表现优异，在布局分析、文本识别、表格结构识别、公式识别及阅读顺序检测等子任务中均显示出竞争力和兼容性。

Conclusion: DREAM模型有效克服了传统方法的不足，实现全面高效的文档重建，具备良好的多任务适应能力，为文档分析领域提供了新的技术手段。

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [43] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 本论文通过引入太阳高度角作为全局条件变量，实现了白天光照变化的合成摄像头传感器数据，解决了人工标注困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中高质量自主驾驶传感器数据获取困难，特别是白天光照变化的合成数据缺乏研究，原因是标注数据稀缺。

Method: 将太阳高度角作为全局条件变量计算，无需人工标注，同时设计了针对光照敏感性的归一化方法，在扩散模型中精准捕捉光照特性和光照相关噪声。

Result: 该方法成功模拟了光照特性及基于光照的图像噪声，提升了合成数据的真实性和多样性。

Conclusion: 引入太阳高度角条件变量和归一化策略有效解决了白天光照变化模拟的难题，为合成自主驾驶传感器数据提供了新的解决方案。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [44] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 提出了一种生成3D桥梁数据的系统框架，解决了真实数据不完整性问题，提高点云分割和补全性能。


<details>
  <summary>Details</summary>
Motivation: 传统桥梁检测人工效率低，3D点云数据因缺失标注和扫描遮挡导致应用受限。

Method: 自动生成带组件级实例标注、高保真颜色和法向量的完整点云，并模拟不完整点云以训练分割和补全网络。

Result: PointNet++在实际桥梁语义分割中取得84.2% mIoU，微调KT-Net在组件补全任务表现优越。

Conclusion: 提出的框架和数据集为桥梁3D视觉分析提供创新方法，有助于基础设施自动化管理与维护。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [45] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出了一种创新的“2D-3D-2D”框架，通过将二维图像中的对象提升为三维表示，在三维空间中进行编辑后再投影回二维图像，实现了高一致性和身份保持的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽在二维图像编辑中表现出色，但由于其基于像素操作的特性，难以保持编辑的一致性和对象身份完整性。

Method: 提出将二维对象提升至受刚性约束的三维环境中进行编辑，再将编辑后的三维对象重新投影并无缝修复回二维图像，从而实现更精准的图像编辑。

Result: 实验表明，该框架在整体性能上优于现有的二维编辑方法，如 DragGAN 和 DragDiffusion，能够实现高度一致且稳定的对象身份保持。

Conclusion: 通过引入三维建模和编辑过程，该方法有效克服了二维像素编辑的局限，大幅提升了图像编辑的一致性和真实性。

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [46] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出一种结合视觉基础模型与大型语言模型的框架，实现视频的高级因果推理和未来预测，表现优越且具备零-shot泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在高阶认知任务如因果推理和未来预测方面表现不足，缺乏常识性世界知识。

Method: 设计一种基于Q-Former架构的融合模块，将复杂的视觉特征转化为语言对齐表示，并采用两阶段训练策略（大规模对齐预训练及针对推理任务的微调）。

Result: 模型在多个复杂基准测试上达到最先进性能，具备显著的零-shot泛化能力，并且消融实验证明各模块的重要性。

Conclusion: 研究推动了机器感知从简单识别向真正认知理解的发展，为更智能的AI系统奠定基础。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [47] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: 本文针对少样本语义分割中的两个关键难点，提出了I$^2$R方法，通过高阶语义特征和定向掩码策略提升分割准确率。


<details>
  <summary>Details</summary>
Motivation: 少样本语义分割受支持图像与查询图像的语义差异以及视觉上相似但语义不同区域的影响，导致分割效果不佳。

Method: 提出I$^2$R方法：1）利用类别特定的高级语义表示聚合支持与查询图像的全局语义线索，实现更准确的区域定位；2）采用定向掩码策略抑制支持与查询像素对中的不一致特征，减少误判。

Result: 在PASCAL-5$^i$和COCO-20$^i$数据集的1-shot设置中，I$^2$R分别提升了1.9%和2.1%的mIoU，优于现有最先进方法。

Conclusion: I$^2$R有效解决了少样本语义分割中图像间语义差异及图像内视觉误差问题，显著提升了分割性能。

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [48] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为USIGAN的免疫组化虚拟染色方法，通过去除弱配对条件下的空间异质影响，提高了生成结果的语义一致性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 传统免疫组化虚拟染色在弱配对条件下，邻近切片的空间异质性导致生成结果在形态结构与染色模式之间的一对多映射不准确，影响病理语义一致性。

Method: 提出了无平衡自信息特征传输(USIGAN)，去除弱配对条件的联合边际分布中的弱配对项，设计了无平衡最优传输一致性(UOT-CTM)机制和病理自我对应(PC-SCM)机制，构建形态和染色图像的相关矩阵。

Result: 在两个公开数据集上的实验表明，该方法在多项临床重要指标（如IoD和Pearson-R相关系数）上表现优越，临床相关性更强。

Conclusion: USIGAN有效解决了弱配对条件下空间异质性问题，提升了虚拟免疫组化染色图像的内容和病理语义一致性，为病理分析提供了一种高效且经济的解决方案。

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [49] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为DFYP的动态融合框架，用于基于遥感数据的作物产量预测，通过增强光谱表示和边缘敏感的空间建模，提高了跨作物类型和年份的预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于遥感的作物产量预测方法面临空间模式复杂、光谱特征异质和农业条件动态变化，导致空间建模能力有限且跨作物和年份泛化能力弱。

Method: DFYP包括三个关键模块：（1）分辨率感知通道注意力（RCA）模块，适应性重加权输入光谱通道；（2）自适应算子学习网络（AOL-Net），动态选择卷积核算子以增强边缘敏感的空间特征提取；（3）双分支架构和可学习融合机制，联合建模局部空间细节和全局上下文信息，实现跨分辨率和跨作物泛化。

Result: 在多年的MODIS数据集和多作物Sentinel-2数据集上的大量实验表明，DFYP在RMSE、MAE和R2指标上均优于当前最先进方法，适用于不同空间分辨率、作物类型和时间段。

Conclusion: DFYP通过光谱通道关注和动态卷积算子选择，有效提升了遥感作物产量预测的准确性和泛化能力，增强了模型在多样化农业场景中的实用性和鲁棒性。

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [50] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为D-FCGS的前馈压缩框架，用于动态高斯点云序列的高效编码，实现了快速压缩和高保真度重建。


<details>
  <summary>Details</summary>
Motivation: 现有的动态3D高斯点云压缩方法通常依赖于场景重建和优化编码，限制了泛化能力和实时性。

Method: 设计了基于I-P帧组结构和稀疏控制点的运动提取方法，结合双重先验的熵模型进行前馈式压缩，并采用控制点引导的运动补偿及细化网络提升视角一致性重建效果。

Result: 该方法无需针对单个场景优化，能在两秒内实现超过40倍的压缩比，且在率失真性能上与基于优化的方法相当，保持了跨视角的视觉质量。

Conclusion: D-FCGS有效提升了动态3D高斯点云的压缩效率和适用性，为沉浸式自由视点视频的传输和存储提供了可扩展的解决方案。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [51] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: 提出了GeoMag框架，提升遥感图像多粒度解析能力，支持像素级任务并减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言模型（RS-VLMs）多局限于图像和区域级任务，像素级任务能力不足，且处理高分辨率图像时计算资源消耗大，限制实际应用。

Method: 提出GeoMag框架，采用任务驱动多粒度分辨率调整（TMRA）和提示引导语义感知裁剪（PSC），动态聚焦任务相关区域，减少任务无关区域分辨率，提高关键区域表征，降低计算成本。

Result: 在10个基准测试中，GeoMag在像素级任务表现突出，同时在其它粒度任务上也表现优异，优于现有RS-VLMs。

Conclusion: GeoMag有效解决了RS-VLMs在像素级任务和计算资源方面的不足，具备广泛的多粒度遥感图像理解能力。

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [52] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 本文首次系统研究了时序缺失多模态数据对视觉追踪性能的影响，提出了一种灵活的异构专家混合融合框架和视频级掩码策略，实现了对不同时序缺失率和场景复杂度的适应。


<details>
  <summary>Details</summary>
Motivation: 现有多模态追踪器因架构刚性，难以应对视频中传感器同步问题导致的模态缺失，性能显著下降，该问题尚未被充分研究。

Method: 提出一种动态激活计算单元的异构专家混合融合机制，结合视频级掩码策略，实现对缺失数据率和场景复杂度的自适应，保证时序一致性和空间完整性。

Result: 该方法在9个基准测试中均达到或超过了现有最先进水平，在完整模态和缺失模态场景下表现优越。

Conclusion: 本文提出的灵活融合框架有效提升了时序不完整多模态视觉追踪的鲁棒性，且适应不同缺失率和场景复杂度，具有广泛应用潜力。

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [53] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本文针对遥感图像场景分类中可解释人工智能方法及其评估指标进行了系统分析，揭示了现有方法和指标的局限性，并提出了选择指导。


<details>
  <summary>Details</summary>
Motivation: 现有xAI方法和评估指标多由计算机视觉领域自然图像发展而来，直接应用于遥感领域存在适用性问题。

Method: 系统分析了五类十种评估指标，结合五种特征归因方法，在三个遥感数据集上进行了详尽的实验验证。

Result: 发现扰动方法对基线和空间特征敏感，梯度方法多标签时表现差，相关传播偏差大；评估指标中忠实度指标与扰动方法问题类似，定位和复杂度指标对大空间类不可靠，鲁棒性和随机化指标表现稳定。

Conclusion: 基于发现的问题，提出了遥感图像场景分类中解释方法、评估指标和超参数选择的指导方案。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [54] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多轮对话的多模态大模型视觉定位强化学习框架MGPO，通过自动裁剪关键视觉区域，提升模型对高分辨率图像的处理能力，无需额外标注即获得强大定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型处理高分辨率图像时生成大量无关视觉token，导致效率低且定位能力不足。

Method: 提出MGPO框架，基于多轮对话自动裁剪图像，通过强化学习训练模型利用二元奖励实现视觉定位能力，并采用多轮对话模版解决冷启动问题。

Result: MGPO在不使用定位标注的视觉问答数据上训练后，较GRPO提升5.4%（MME-Realworld）和5.2%（OOD V* Bench）性能，且在Qwen2.5-VL-7B上超越OpenAI的o1和GPT-4o模型。

Conclusion: MGPO无需额外定位标注即可有效增强大模型视觉定位能力，对处理高分辨率多模态输入有显著提升，具有广泛应用前景。

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [55] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: 本文通过引入几何信息（单目深度估计）提升视频实例分割（VIS）的鲁棒性，提出了三种集成范式，实验证明扩展深度通道（EDC）和共享ViT（SV）方法显著提升了性能，在OVIS基准上取得了56.2 AP的新纪录。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割在处理对象遮挡、运动模糊和外观变化时存在挑战，现有方法难以实现稳定的时序关联。引入几何信息有望解决这些问题。

Method: 提出三种结合单目深度估计与视频实例分割的方法：扩展深度通道（EDC）将深度图作为输入通道；共享ViT骨干网络（SV）实现深度估计与分割任务共享同一网络；深度监督（DS）作为辅助训练引导。

Result: 实验证明ECD和SV显著提升了VIS的鲁棒性，其中EDC方法采用Swin-L骨干网络在OVIS数据集上达到56.2的AP分数，刷新了最先进成绩。DS方法效果有限。

Conclusion: 深度信息是提升视频实例分割鲁棒性的关键因素，合理融合深度估计能显著增强视频理解能力。

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [56] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 本文提出了一种稀疏表示方法，显著提升神经表面重建的分辨率和内存效率，实现了比传统密集体素表示更高精度的3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统的密集3D特征体表示虽然有效，但在提高体素分辨率时内存消耗剧增，限制了重建质量。

Method: 采用两阶段方法：先训练网络预测体素占据率，再仅在占据率高的体素中计算特征和体积渲染，并设计了高效采样和查询的自定义算法以支持稀疏体积。

Result: 在公共数据集上实验显示，该方法在不降低性能的情况下，存储需求降低50倍以上，支持512^3分辨率重建，优于传统128^3分辨率和现有方法。

Conclusion: 提出的基于稀疏表示的神经表面重建方法显著提升了重建分辨率和效率，突破了内存瓶颈，实现了更精确的重建效果。

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [57] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 本文提出了Tora2，一种改进的扩散变换器模型，实现了多实体外观和运动的同时定制。


<details>
  <summary>Details</summary>
Motivation: 现有的运动引导视频生成方法在外观和运动的多样化定制能力上存在限制，难以精细保留视觉细节和处理多模态信息融合。

Method: 引入解耦个性化提取器获取多实体的个性化嵌入，设计门控自注意力机制融合轨迹、文本和视觉信息，采用对比损失优化轨迹动态和实体一致性。

Result: Tora2在多实体外观和运动定制能力上表现突出，相较现有方法具有更好的视觉细节保持和运动控制效果。

Conclusion: Tora2实现了视频生成领域多实体外观与运动的同时定制，推动了多条件视频生成技术的发展。

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [58] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: 本文提出了一种名为T-LoRA的扩散模型微调框架，针对单张图像的模型个性化，解决小样本条件下过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型微调在样本有限时易过拟合，影响泛化和多样性，尤其是单图像自定义需求强烈。

Method: 引入时间步骤依赖的低秩适应方法，采用动态微调策略和正交初始化保证参数组件独立。

Result: 实验显示T-LoRA优于标准LoRA及其他个性化方法，兼顾了概念还原度和文本对齐度。

Conclusion: T-LoRA在数据和资源受限条件下拥有较大应用潜力，提升了扩散模型的个性化效果。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [59] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一个自动生成合成三元组数据集CIRHS和一个新的图文检索框架CoAlign，实现了基于合成数据集的零样本和监督训练下的高效图像检索。


<details>
  <summary>Details</summary>
Motivation: 现有的图文组合检索方法依赖昂贵的手工标注三元组，限制了其扩展性和零样本能力。

Method: 利用大语言模型生成多样提示，控制文本到图像生成模型产出包含相同元素的图像对，从而自动生成合成三元组，构建CIRHS数据集。提出混合上下文对齐(CoAlign)框架实现全局对齐和局部推理。

Result: 基于合成数据集训练的CoAlign在三大基准零样本任务上表现优异，验证了合成训练的可行性。监督训练下优于所有现有的监督方法。

Conclusion: 自动合成数据和新的检索框架为图文组合图像检索提供了高效可扩展的解决方案，提升了零样本和有监督性能。

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [60] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SCINet的部分多标签学习框架，旨在通过捕捉标签与实例之间的共现模式，提升部分标签数据的学习效果。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习面临的核心挑战是准确识别标签与实例之间的模糊关系，现有方法未充分利用标签与实例的共现关系。

Method: 提出了SCINet模型，包括双主导提示模块（使用多模态模型捕获文本-图像相关性）、跨模态融合模块（联合建模标签间、实例间及实例-标签共现关系）、以及内在语义增强策略（通过多样图像变换强化语义理解）。

Result: 在四个广泛使用的基准数据集上的大量实验结果表明，SCINet优于当前最先进的部分多标签学习方法。

Conclusion: SCINet通过有效利用标签与实例的语义共现模式，实现了部分多标签学习的性能提升，验证了其模型设计和增强策略的有效性。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [61] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: 本文提出了一种基于集成的方法来提升深度伪造检测模型在不同数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测模型虽在基准数据集上表现优异，但在分布外数据上表现显著下降，亟需提升泛化能力。

Method: 基于近期开源基准，结合多个顶尖异构模型的预测概率进行集成，实现跨数据集的稳健性能。

Result: 在两个不同的分布外数据集上实验表明，单一模型表现不稳定，集成方法则提供了更稳定可靠的性能。

Conclusion: 异构模型集成为现实应用中缺乏伪造类型及质量先验知识的深度伪造检测提供了一种稳健且易扩展的解决方案。

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [62] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种在无GNSS信号的城市环境中，通过与卫星图像对齐来实现激光雷达点云精准地理配准和空间校正的方法。


<details>
  <summary>Details</summary>
Motivation: 城市高楼和桥梁遮挡导致GNSS信号不可用，传统依赖GNSS和IMU数据的方法定位不准确。

Method: 利用预训练的Point Transformer模型分割道路点，提取路网骨架和交叉点，与目标地图对齐；先全局刚性对齐，再局部RBF插值校正；基于SRTM数据进行高度修正。

Result: 在KITTI和珀斯数据集上，定位精度分别提升55.3%和77.4%，高度相关性分别提升30.5%和50.4%。

Conclusion: 该方法有效提升了无GNSS环境下激光雷达点云的空间定位和高度精度，支持城市大尺度3D地图重建。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [63] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 本论文提出了一种名为GCDA的新型扩散模型框架，用以提升文本到图像生成中生成文字的可读性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽然在生成逼真多样图像方面表现出色，但无法生成可读、准确拼写的文字，限制了实际应用。

Method: GCDA框架包含三部分：双流文本编码器（融合语义和字形信息）、字符感知注意力机制（配合注意力分离损失减少注意力混淆）以及内嵌OCR的微调阶段（通过文本感知损失优化字符准确性）。

Result: GCDA在MARIO-10M和T2I-CompBench等数据集上实现了文本生成相关指标的新高，字符错误率从0.21降至0.08，词错误率从0.25降至0.15，同时保持了图像生成的高质量。

Conclusion: GCDA有效解决了文本到图像生成中字符识别难题，显著提升了文字的可读性和准确性，推动此类模型在广告、教育和设计等实用领域的应用。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [64] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出VisualSpeaker方法，利用光度真实的可微分渲染和视觉语音识别监督，提升3D面部动画的真实感和准确度。


<details>
  <summary>Details</summary>
Motivation: 现有3D面部动画方法依赖网格模型，难以充分利用2D视觉技术的快速发展，限制了动画质量提升。

Method: 采用基于光度真实3D高斯点渲染的可微分渲染技术，结合视觉语音识别模型形成感知式唇读损失函数，监督训练3D面部动画生成。

Result: 在MEAD数据集上，VisualSpeaker在唇部顶点误差指标上提升56.1%，同时生成动画的感知质量显著提高，并保持了网格驱动动画的可控性。

Conclusion: VisualSpeaker在提升高保真3D面部动画质量的同时，为手语动画中相似手势的区分提供了准确的口型信息，展示了良好应用前景。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [65] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出MEDTalk框架，实现基于音频的细粒度动态情感3D面部动画，提升表情自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多使用静态预定义情感标签，导致表情单一且不自然。

Method: 通过跨重构过程分离内容和情感嵌入，实现唇动和表情的独立控制，并融合音频与文本信息动态调整情感强度，多模态输入增强个性化控制。

Result: 生成的3D面部动画表现出生动且同步的唇动和情感变化，可集成于MetaHuman工业生产流程。

Conclusion: MEDTalk有效提升了音频驱动表情动画的自然性和多样性，为动态情感表达提供了灵活控件及多模态交互支持。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [66] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态因果分析模型（MCAM），通过构建视觉和语言模态间的潜在因果结构，实现了驾驶行为识别及因果推理的新突破。


<details>
  <summary>Details</summary>
Motivation: 现有方法挖掘因果关系浅显，无法有效处理模态间的伪相关，也忽视了自车级别的因果建模，限制了自动驾驶视频理解效果。

Method: 设计多层次特征提取器捕捉长距离依赖，构建驾驶状态的有向无环图实现因果分析模块，利用视觉语言变换器对关键视觉特征和对应语言表达进行对齐。

Result: 在BDD-X和CoVLA数据集上，MCAM实现了视觉语言因果关系学习的SOTA性能，并展示出对视频序列中因果特征的优秀捕捉能力。

Conclusion: MCAM有效突破了现有方法局限，提升了自动驾驶视频中的因果关系建模能力，具备良好的实际应用潜力。

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [67] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的法线积分方法，能够显式处理深度不连续性，并适用于通用中心相机模型。


<details>
  <summary>Details</summary>
Motivation: 现有法线积分方法多隐式处理深度不连续性且仅限于正交或理想针孔相机，限制了其应用范围。

Method: 通过局部平面假设，建立表面法线与光线方向之间的约束，显式建模深度不连续性，并支持通用中心相机。

Result: 在标准法线积分基准测试中，本文方法达到最先进的性能，并首次支持通用中心相机模型。

Conclusion: 该方法更精确地近似了深度与表面法线的关系，拓宽了法线积分的应用边界，提升了重建的准确性。

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [68] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新型自然无约束对抗样本生成方法ScoreAdv，能够生成高质量且具有强攻击性的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击方法依赖于ℓ_p范数扰动，与人类感知能力不符，现有GAN方法图像质量差，扩散模型尚未充分利用其去噪能力，急需一种更自然且有效的攻击手段。

Method: 提出了ScoreAdv方法，通过可解释的对抗引导机制逐步调整采样分布，以及利用显著性图将参考图像的视觉信息注入对抗样本生成过程，实现自然无约束对抗样本的高质量生成。

Result: 在ImageNet和CelebA数据集的十个目标模型上，ScoreAdv在黑盒和白盒攻击中均表现出领先的攻击成功率和图像质量，同时在防御措施下保持较强的鲁棒性。

Conclusion: ScoreAdv有效结合扩散模型的去噪能力和对抗引导策略，成功生成自然且强攻击性的对抗样本，可攻击分类和检索模型，推动了自然无约束对抗样本领域的发展。

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [69] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: 提出了一套无接触多模态远程生理情感识别数据库CAST-Phys，包含面部视频与多种生理信号，提升真实场景下的情感识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别依赖接触式设备，可能影响被试情绪真实性，且缺乏高质量多模态数据集，限制了情感识别系统准确性和发展。

Method: 构建了包含光电容积描记(PPG)、皮肤电活动(EDA)、呼吸频率(RR)等多种生理信号及高分辨率无损面部视频的CAST-Phys数据集，用于远程多模态情感识别。

Result: 分析表明生理信号在实际情境下补充面部表情信息关键，融合多模态数据能显著提升远程无接触情感识别性能。

Conclusion: CAST-Phys数据库的建立和多模态远程情感识别方法的实验验证推动了无接触情感识别技术的发展，有利于实现更真实、准确的情感计算应用。

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [70] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer的多物种植物识别方案，在PlantCLEF 2025挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 针对植被象限图像中的多物种植物识别问题，提升识别准确率和模型泛化能力。

Method: 采用微调后的ViT模型进行图像分块推理，结合4x4切分策略匹配感受野，通过PaCMAP和K-Means进行视觉聚类及地理位置过滤，最后用投票和贝叶斯先验加权聚合预测结果。

Result: 该方法在未额外训练的情况下获得私有榜单宏平均F1值0.348，排名第二。

Conclusion: 结合视觉变换器、切分策略及域适应聚类对多物种植物识别效果显著，且方法代码开源保障了复现性。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [71] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 本文提出通过构建合成文化数据集CulTwin并微调CLIP模型，解决视觉语言模型在文化细节识别上的不足，实现了更细粒度的文化差异识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉语言模型难以区分视觉相似但文化背景不同的概念，主要由于缺乏高质量的文化特定数据集、缺少上下文知识融合及缺乏突出微妙差异的负样本。

Method: 设计数据构建流程，利用开源视觉语言模型和文本生成图像扩散模型制作包含概念-标题-图像三元组的合成文化数据集CulTwin；在此基础上微调CLIP，采用定制对比学习方法加强文化概念与上下文增强标题及合成图像的对齐。

Result: CultureCLIP在多个文化相关基准测试中，相较原CLIP模型，在细粒度文化概念识别上最高提升5.49%，且保持了原有的泛化能力。

Conclusion: 通过合成数据集与定制训练策略有效提升了视觉语言模型识别细微文化差异的能力，验证了该方法在文化特征捕捉上的先进性。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [72] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock提出了一种基于3D Gaussian Splatting的几何感知反射建模框架，解决了现有新视角合成方法在反射表面建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和3D Gaussian Splatting在反射场景中通常将反射误判为物理几何，导致重建质量下降，且现有几何约束不完善，难以处理复杂几何反射，产生模糊和表面伪影。

Method: 提出了一个基于3D Gaussian Splatting的Ref-Unlock框架，通过双分支表示和高阶球面谐波捕捉高频反射细节，同时引入反射去除模块和伪深度图，以及几何感知的双边平滑约束，以实现传输和反射成分的显式分离和稳定的几何一致性。

Result: 实验表明，Ref-Unlock在反射建模上显著优于传统GS方法，并在与NeRF模型比较中表现竞争力，同时支持灵活的视觉基础模型进行反射编辑。

Conclusion: Ref-Unlock 提供了一种高效且通用的解决方案，能实现真实感的反射场景渲染，并促进了反射处理和视图合成领域的发展。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [73] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: 该论文提出了Omni-Video，一个统一的视频理解、生成及编辑框架，通过将多模态大语言模型与扩散解码器相结合，实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要专注于图像处理，缺乏统一的视频理解和生成模型，推动视频多任务统一建模需求。

Method: 设计轻量级架构，给多模态大语言模型顶部添加视觉头，扩散解码器输入前加适配器；采用多阶段训练，提升模型连接效率。

Result: 模型在视频生成、编辑及理解任务中表现出良好的泛化能力，能高效生成高质量视频。

Conclusion: 通过结合多模态语言模型和扩散解码器，Omni-Video实现了统一且高效的视频多任务处理，弥补了视频领域基础模型的不足。

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [74] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种无提示条件扩散框架，用于多物体图像增强，结合局部-全局语义融合和计数损失，提高了生成图像的多样性和类别一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多物体图像生成方法依赖文本导致类别偏差，或依赖原图导致多样性不足，限制了下游任务的效果。

Method: 引入局部-全局语义融合替代文本条件，采用LoRA注入知识缓解类别偏差，设计基于计数的奖励模型损失辅助训练以约束类别数量，提升生成图像多样性和数量一致性。

Result: 实验结果表明该方法优于多种先进基线，在下游任务性能和跨域泛化能力上表现突出。

Conclusion: 该框架有效解决了多物体生成中的类别偏差与多样性不足问题，提高了图像增强的实用性和广泛适应性。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [75] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 提出了一种名为SoftReMish的新激活函数，在MNIST数据集上的标准CNN架构中对比了ReLU、Tanh和Mish，结果显示SoftReMish在训练损失和验证准确率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了提升卷积神经网络在图像分类任务中的性能，设计一种新的激活函数。

Method: 在标准CNN架构中将所有可训练层的激活函数替换为SoftReMish，并与ReLU、Tanh、Mish进行对比实验，使用MNIST数据集评价模型表现。

Result: SoftReMish实现了最低训练损失（3.14e-8）和最高验证准确率（99.41%），优于其他激活函数。

Conclusion: SoftReMish在收敛速度和泛化能力方面表现优异，是视觉识别任务中有潜力的新型激活函数。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [76] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: 本文提出了一种基于相似度矩阵归一化的新型光滑算子，用于在无结构域上实现类拉普拉斯热扩散的信号平滑。


<details>
  <summary>Details</summary>
Motivation: 在无明确定义结构的非规则数据域（如点云、稀疏体素网格）上，传统拉普拉斯算子难以构建，现有方法对边界存在偏差。

Method: 利用对称化的Sinkhorn算法对一般相似或邻接矩阵进行归一化，构造出继承拉普拉斯性质的扩散类算子。

Result: 新算子不仅逼近热扩散过程，还保留了拉普拉斯算子的谱特征，在形状分析和匹配等应用中表现良好。

Conclusion: 该方法拓展了热扩散光滑在无结构数据上的应用，有助于提升非欧几里得数据处理的理论保障和效果。

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [77] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: OmniPart是一种新的3D对象生成框架，专注于生成可编辑的部件结构，实现了语义上高度独立且结构上紧密的组件分离。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法通常生成单一整体形状，缺乏清晰且可编辑的部件结构，限制了其在交互应用中的实用性。

Method: OmniPart将任务分为两个阶段：首先，通过自回归结构规划模块生成可控的3D部件边界盒序列，利用灵活的2D部件掩码实现对部件分解的直观控制，无需语义标签；其次，使用基于空间条件的修正流模型，同时合成所有3D部件，确保整体结构协调。

Result: 实验结果表明，OmniPart在生成可解释、可编辑且多样化的3D内容方面达到最先进水平。

Conclusion: 该方法成功实现了用户定义的部件粒度和精准定位，推动了更具解释性和可编辑性的3D资产创建，拓宽了其应用前景。

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [78] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: 该论文针对科学图表的视觉问答任务，提出了结合提示优化、连锁推理和模型集成的方法，实现了较高的性能指标。


<details>
  <summary>Details</summary>
Motivation: 科学技术报告中大量重要信息以半结构化图表形式存在，传统视觉问答方法难以精准理解科学数据中的数值和多步推理需求。

Method: 使用参数规模为5B至8B的视觉语言模型，结合提示优化、连锁推理（chain-of-thought）以及多模型集成策略，提升对科学图表问答的表现。

Result: 单模型InternVL3在SciVQA测试集上达到了ROUGE-1和ROUGE-L F1分别为0.740和0.983，集成模型表现优于大多数单模型，但InternVL3仍是最优单模型。

Conclusion: 通过提示优化、连锁推理和模型集成，可以有效提升视觉问答模型在科学图表问题上的表现，尤其是对数值推理和多步推理的处理能力。

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [79] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督的语义场景完成方法SceneDINO，基于多视图一致性的自监督学习，从单张图像推断3D几何和语义特征。


<details>
  <summary>Details</summary>
Motivation: 现有的语义场景完成方法严重依赖昂贵的带注释数据，急需一种无需语义和几何标注的无监督方法。

Method: 引入SceneDINO，结合自监督表示学习与2D无监督场景理解，利用多视图一致性自监督训练，采用新颖的3D特征蒸馏获得无监督3D语义。

Result: SceneDINO在3D和2D无监督场景理解任务中达到最新的分割精度，线性探测其3D特征的分割准确率与现有监督方法相当。

Conclusion: SceneDINO展示了出色的领域泛化和多视图一致性，为单张图像的3D场景理解奠定了坚实基础。

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [80] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出了RSRefSeg 2，一种基于视觉-语言协同的遥感图像细化分割框架，采用双阶段的粗定位和细分割策略，提升了复杂语义关系的解析能力和分割精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言的遥感图像分割方法因目标定位与边界细化耦合处理，导致语义模糊时误差传播大，泛化能力和解释性不足。

Method: 设计RSRefSeg 2，通过CLIP进行目标粗定位，利用级联二阶提示器分解文本语义，再用SAM生成像素级精细掩膜，实现任务解耦和跨模态协作。

Result: 实验证明RSRefSeg 2在多个数据集上的分割精度提升约3%（gIoU指标），且在复杂语义解析任务中表现优于现有方法。

Conclusion: RSRefSeg 2有效解决了遥感图像语义模糊导致的误差传播问题，增强了模型的泛化性和解释性，为视觉语言遥感图像分割提供了新思路。

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [81] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种利用SMPL模型自动生成伪标签数据的点跟踪训练管道AnthroTAP，实现了高效且准确的人体点跟踪。


<details>
  <summary>Details</summary>
Motivation: 人类运动复杂且具有丰富的监督信息，但人工标注训练数据耗时费力，难以获取大量数据。

Method: 通过SMPL模型拟合视频中的人体，将3D网格顶点投影到2D图像平面生成伪轨迹，采用射线投射处理遮挡，利用光流一致性过滤不可靠轨迹，从而自动生成带标签的训练数据。

Result: 基于AnthroTAP数据集训练的点跟踪模型在TAP-Vid基准测试中达到最先进性能，使用的数据量比真实视频少10000倍，且训练资源远低于最新方法。

Conclusion: AnthroTAP有效减少了训练数据和计算资源需求，显著提升了人体点跟踪的性能和实用性。

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


### [82] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 本文提出通过在训练时强制使用一致的标题结构，提高生成文本到图像模型的提示遵守性和对齐性能。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集如LAION-5B噪声大、结构混乱，导致模型难以严格依照提示生成图像，用户需频繁调试提示。

Method: 构建高质量子集Re-LAION-Caption 19M，使用四部分模板（主体、环境、美学、摄影细节）生成一致结构的标题，微调PixArt-$\Sigma$和Stable Diffusion 2模型，对比结构化与随机打乱标题效果。

Result: 结构化标题明显提升文本与图像的对齐评分，验证了方法有效性。

Conclusion: 在训练中引入一致的标题结构能显著提高生成模型提示遵守性和图像生成质量，数据集公开促进后续研究。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [83] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 本文提出了一个名为CorrDetail的可解释性人脸造假检测框架，通过细节自我校正和视觉细节增强，有效提升了检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸造假检测方法要么缺乏对造假细节的清晰解释，要么容易出现幻觉问题，难以准确揭示造假细节。

Method: 提出了基于错误引导提问的自我校正框架CorrDetail，结合视觉细粒度细节增强模块，并设计融合决策策略以提高模型对极端样本的判别能力。

Result: 实验结果表明，CorrDetail在检测性能上优于最新方法，且能准确识别造假细节，表现出较强的泛化能力。

Conclusion: CorrDetail框架有效解决了现有人脸造假检测方法中细节解释不足和幻觉问题，提升了可解释性和检测准确率，具有良好的应用前景。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [84] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: 本文提出了针对复杂弯曲道路上的行人检测问题，基于YOLOv8架构改进的YOLO-APD模型，显著提升检测准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂几何形状路面上（如Type-S弯曲曲面）需要鲁棒的行人检测，而传统的RGB摄像头方法存在局限。

Method: 引入无参数SimAM注意力机制、高效C3Ghost模块、新型SimSPPF多尺度特征池化、Mish激活函数和智能聚合分发（IGD）模块，结合车辆转向动力学自适应感兴趣区域处理。

Result: 在定制的CARLA数据集上，YOLO-APD达到77.7% mAP@0.5:0.95和超过96%的行人召回率，实时处理速度达100 FPS，显著优于YOLOv8等基线模型。消融实验验证了各组件的贡献，在KITTI数据集上也表现出潜力。

Conclusion: YOLO-APD实现了高准确度、高效率且适应性强的基于经济型传感器的感知系统，有助于提升复杂复杂驾驶环境中的自动驾驶安全性和可靠性。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [85] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Spotlight的虚拟染色方法，通过引导模型关注细胞结构，改进了3D显微镜图像的荧光预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色方法训练时对所有像素一视同仁，容易复制背景噪声和伪影，影响对生物学信号的准确捕捉。

Method: Spotlight采用基于直方图的前景估计来掩盖像素级损失，并使用基于软阈值的Dice损失进行形状感知学习，从而使模型重点关注相关细胞结构。

Result: 在3D基准数据集上，Spotlight提高了细胞形态的表示能力，同时保持了像素级准确性，使得虚拟染色结果更适合下游的分割和特征分析任务。

Conclusion: Spotlight通过聚焦生物学相关结构，有效提升了虚拟染色的质量，推动了显微镜图像在生物医学研究中的应用。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [86] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: 本文评估了现有基础模型在农业任务中的效果，提出了理想农业基础模型的需求框架，比较并实证了两种模型在农业任务中的表现，强调了开发农业专用基础模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 食品安全问题迫使农业监测技术需要创新，而基础模型在遥感和气候科学中表现突出，显示了其在农业任务中的潜力，但农业相关应用尚未充分挖掘。

Method: 提出农业领域的基础模型需求框架，调研和对比现有通用基础模型，并选取其中两种模型在作物类型划分、作物物候估计和产量预测三项农业任务中进行实证评估。

Result: 现有通用基础模型在农业任务中表现不一，但整体能力有限，显示出需要专门针对农业设计的基础模型。

Conclusion: 农业需要专门定制的基础模型以提升作物类型、物候和产量等关键农业任务的监测效果，从而促进精准农业和保障食品安全。

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [87] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的水下图像增强方法，通过整合专家主观评判提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统水下图像质量提升方法效果有限，且缺乏主观感知的指导，因此引入专家标签作为训练依据。

Method: 首先训练分类器区分高质量与低质量图像，然后利用生成对抗网络（GAN）依据多个增强标准对低质量图像进行提升。

Result: 通过PSNR、SSIM、UIQM等指标及主观分析验证，模型在颜色保真度和图像锐度等方面显著提升了水下图像质量。

Conclusion: 结合人类主观评价的深度学习方法有效增强了水下图像质量，提升了图像的感知效果和客观指标。

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [88] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 提出了pFedMMA，一种利用多模态适配器的个性化联邦学习框架，实现视觉-语言模型在分布异构数据上的高效适应。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在视觉-语言模型中虽参数高效，但在实现个性化的同时往往牺牲了模型对未见类别或领域的泛化能力。

Method: pFedMMA设计了包含模态特异的上下投影层与全局共享投影的多模态适配器，通过非对称优化策略实现本地个性化适配及共享投影的协同训练，同时仅交换共享组件以节省通信开销。

Result: 在包括领域和标签变动的十一组数据集上，pFedMMA展示了个性化与泛化能力的最佳平衡，优于近期的联邦提示调优方法。

Conclusion: pFedMMA有效解决了视觉-语言模型在异构分布下的个性化与泛化权衡，实现了通信效率及性能提升，具备广泛应用潜力。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [89] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: 本文提出了LoongX，一种结合脑机接口信号与扩散模型的免手动图像编辑方法，实现了基于神经信号的高效图像编辑。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑依赖手动操作，限制了行动或语言障碍者的使用，亟需无障碍的编辑方式。

Method: LoongX利用电生理信号（EEG、fNIRS、PPG、头部运动）捕捉用户意图，采用跨尺度状态空间编码和动态门控融合模块整合多模态特征，并通过对扩散变换器的微调实现语义对齐，编码器预训练采用对比学习匹配认知状态与语义意图。

Result: 实验显示LoongX在图像编辑性能上可与文本驱动方法相媲美，且结合语音时表现更优。

Conclusion: LoongX展示了神经驱动生成模型在无障碍和直观图像编辑中的潜力，促进了认知驱动创意技术的发展。

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [90] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 本文综述了近年来运动生成领域的方法，特别关注2023年以来不同生成模型策略的发展，涵盖架构、条件机制、生成设置及评估指标和数据集。


<details>
  <summary>Details</summary>
Motivation: 运动生成领域快速发展，出现多样化生成方法，亟需一份系统的综述来整理和比较最新进展。

Method: 基于生成策略对运动生成方法进行深入分类，分析架构原理、条件机制和生成设定，编制评估指标及数据集综述。

Result: 梳理出了不同方法的优势与局限，明确了各类方法和评价标准，识别了该领域的开放挑战。

Conclusion: 该综述为研究人员和应用者提供了一个及时且系统的参考，促进对运动生成方法的理解和应用。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [91] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于2D扩散编辑和3D高斯点雾技术的局部3D场景编辑方法，实现了高效且一致的多视角编辑。


<details>
  <summary>Details</summary>
Motivation: 3D语义解析性能较2D差，导致3D空间中的目标修改难度大，编辑质量受限。

Method: 利用2D扩散编辑定位修改区域，通过逆渲染实现3D定位，结合深度图初始化粗略3D高斯点雾模型，并通过迭代细化保证多视角一致性。

Result: 该方法在局部3D编辑任务中性能达到最先进水平，且编辑速度提升约4倍。

Conclusion: 提出的方法有效提升了3D场景局部编辑的精度和效率，适用于需要高控制度的3D编辑场景。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [92] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: 提出了OpenWorldSAM框架，基于SAM2和轻量级视觉语言模型，实现了开放词汇的图像分割，有效处理多样且未见类别的分割任务。


<details>
  <summary>Details</summary>
Motivation: 解决基于开放语言提示的对象分割挑战，使模型能将文本语义准确映射到空间掩码，且能处理多样和未见类别。

Method: 通过冻结SAM2和VLM预训练组件，只训练450万参数，并引入统一提示、多实例空间理解增强（位置决胜嵌入和交叉注意力层）方法，提高分割效率和多实例分割能力。

Result: OpenWorldSAM在多个基准数据集（ADE20k、PASCAL、ScanNet、SUN-RGBD）上的开放词汇语义分割、实例分割和全景分割任务中均达到了最先进的性能。

Conclusion: OpenWorldSAM有效扩展了SAM2对开放词汇的支持，具有高效训练、强实例感知和优异的零样本泛化能力，在多种分割任务中表现优异。

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [93] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 该论文开发了一种基于视觉引导的AI驱动变速喷雾系统，实现了杂草检测和动态喷洒控制，显著提高了喷药精准度。


<details>
  <summary>Details</summary>
Motivation: 传统农业中均匀且过量的除草剂喷洒导致成本增加、环境污染及抗药性杂草出现，亟需精准喷洒技术解决该问题。

Method: 系统采用轻量级YOLO11n和YOLO11n-seg深度学习模型在NVIDIA Jetson Orin Nano上进行实时推理，通过Arduino Uno控制电磁阀喷嘴，实现基于植被冠层检测的喷药调节。通过室内不同冠层大小的扶桑花模拟杂草场景进行实验验证。

Result: YOLO11n模型的平均精度达到0.98，精确率和召回率接近1；YOLO11n-seg模型性能稍逊。水敏纸测试表明喷药覆盖率随冠层大小增长，系统能基于冠层大小调整喷药量，验证了实时精准喷药的可行性。

Conclusion: 结合实时深度学习与低成本嵌入式硬件的选择性除草技术显示出良好潜力，未来拟扩展识别更多杂草种类并进行室内及田间更广泛测试。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [94] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 本论文提出利用自然驾驶视频和大规模视觉模型，基于驾驶行为识别老年驾驶员的认知状态，实现早期发现认知衰退。


<details>
  <summary>Details</summary>
Motivation: 当前认知衰退如阿尔茨海默氏症和轻度认知障碍的诊断方法耗时且费用高，且常被漏诊，亟需一种便捷非侵入的早期诊断手段。

Method: 结合大型视觉模型和自然驾驶视频分析驾驶行为，提取与认知功能衰退相关的数字指纹，构建基于驾驶行为的认知状态分类及疾病预测框架。

Result: 方法能够通过驾驶行为准确识别认知状态，检测功能障碍的早期警示信号，为干预提供依据。

Conclusion: 该研究推动了基于驾驶行为的无创早期认知衰退检测，有助于开发可扩展的监测系统，缓解老龄化社会认知障碍负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [95] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 本文提出了一种基于尺度不变噪声的扩散模型，称为"云扩散模型"，以替代传统的白噪声扩散模型。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用白噪声，但自然图像在低阶统计特性上表现出尺度不变性，白噪声难以捕捉图像的尺度相关特征。

Method: 提出使用具有幂律尺度特性的噪声分布替代白噪声，构建云扩散模型，从而更好地符合自然图像的统计属性。

Result: 该模型预计能加快推理速度，提高高频细节表现，并增强模型的可控性，（具体效果将在后续论文中验证对比）。

Conclusion: 引入尺度不变噪声的云扩散模型为图像生成带来了潜在改进，将在后续工作中进一步训练并与传统白噪声扩散模型进行比较。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [96] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: 本文提出了LoomNet，一种用于从单张图像生成多视角一致图像的扩散模型架构，能够提升3D表面重建的空间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以保证多视角图像的一致性，导致3D重建质量下降。作者希望设计一个多视角扩散模型，解决空间一致性问题。

Method: LoomNet通过多视角扩散模型并行运行，生成每个视角的编码，将其投影到三个正交平面并融合，再通过信息传播和插值形成统一潜在空间，最终渲染一致的多视角图像。

Result: 生成了16个高质量、多视角一致的图像，且仅需15秒，且在图像质量和重建指标上均优于最新方法。还可生成多样且合理的新视角。

Conclusion: LoomNet有效提升了单图像到多视图图像生成任务的空间一致性和质量，为三维重建和多视角合成提供了创新方案。

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [97] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 本文提出了llama-nemoretriever-colembed，一个统一的文本-图像检索模型，在多个基准测试中达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 随着跨模态检索系统需求的增长，提出一种能有效处理文本与图像的统一检索模型。

Method: 基于NVIDIA Eagle2视觉语言模型，改进为双向注意力机制，并引入ColBERT风格的晚期交互机制，实现细粒度的多模态检索；采用两阶段训练策略提升检索能力。

Result: 3B模型在ViDoRe V1和V2两个排行榜分别实现91.0和63.5的NDCG@5分数，领先所有模型。

Conclusion: 该模型在精度、存储和效率间取得平衡，显著提升跨模态检索性能，推动相关领域发展。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [98] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 本文针对非洲地区缺乏自动驾驶数据集的问题，提出了一种程序化增强流水线，通过模拟光学和天气效应，提升低成本单目行车记录仪视频质量，支持感知研究。


<details>
  <summary>Details</summary>
Motivation: 非洲多样的城市、乡村和非铺装道路缺乏自动驾驶数据，限制了低资源环境下的感知性能。

Method: 设计了折射模块模拟低质量镜头和空气湍流的光学效应，以及天气模块模拟均匀和非均匀雾气及镜头光晕，实现数据增强。

Result: 基于三种图像恢复模型建立了基准性能，验证了增强数据的有效性。

Conclusion: 通过发布失真工具包、增强数据集划分和基准测试，推动非洲自动驾驶感知研究，无需昂贵数据采集和标注。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [99] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: ReLayout通过引入关系链思维和明确的元素间关系定义，优化内容感知布局生成，实现结构化与多样化设计，提升美学一致性和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的布局生成方法未能充分理解视觉元素间的空间关系，导致布局结构和多样性方面存在问题。

Method: 引入relation-CoT增强布局注释，定义显式关系（区域、显著性、边距）分解布局，提出布局原型再平衡采样器解决数据偏差，促进更结构化和风格多样的布局生成。

Result: 实验结果表明，ReLayout在结构性、多样性和美学一致性方面优于基线方法，生成的布局更符合人类审美且具更好的可解释性。

Conclusion: ReLayout通过关系链思维和布局原型重平衡机制，有效提升了内容感知布局的合理性和美观度，为自动布局生成提供了新的思路。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [100] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 提出了一种新颖的跨模态迁移引导网络（CTNet）用于多模态人脸反欺骗，通过学习一致与不一致的跨模态特征转换，提高了活体检测的鲁棒性和对缺失模态的处理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态人脸反欺骗因不同传感器和环境条件导致训练测试分布差异大，且推理阶段常遇到模态缺失问题，需有效方法提升模型的泛化能力和鲁棒性。

Method: 基于活体样本跨模态特征转换一致性构建泛化特征空间，同时利用活体与欺骗样本间特征转换不一致性检测异常攻击，且从RGB模态学习辅助的红外和深度特征以应对模态缺失。

Result: 大量实验表明CTNet在多数协议下优于先前的双分类多模态人脸反欺骗方法，表现出更强的泛化能力和检测效果。

Conclusion: CTNet有效解决了多模态反欺骗中的分布差异和模态缺失问题，提升了系统在复杂场景下的活体检测性能。

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [101] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散模型的半监督缺陷检测框架DSYM，通过两阶段协同训练和分阶段联合优化，实现高效的工业缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统工业缺陷检测方法依赖人工或早期图像处理算法，存在效率低、成本高及鲁棒性差的问题。

Method: DSYM框架利用标注数据进行初始训练，结合生成的伪标签利用未标注数据，采用条件扩散模型合成多尺度伪缺陷样本，通过CLIP跨模态特征过滤噪声。

Result: 在NEU-DET数据集上，DSYM以同等标注数据实现78.4% mAP@0.5，且仅用40%标注数据时仍达75.1% mAP@0.5，显著提升数据利用效率。

Conclusion: 该方法为工业质量检测中缺陷检测提供了一种高精度且依赖标注少的有效解决方案。

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [102] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 本文提出了基于2D高斯的高效视频隐式表示方法GSVR，实现了极快的解码速度和较低的训练时间，同时保持较高的视频重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积网络的视频隐式表示方法在解码速度和训练时间上存在显著瓶颈，限制了其实用性。

Method: 提出了混合变形场结合三平面运动和多项式运动处理相机与物体运动耦合；利用动态感知时间切分策略适应视频动态变化；采用量化感知微调和图像编码实现高效压缩。

Result: 在Bunny和UVG数据集上，实现了800+ FPS解码速度，训练时间降至每帧约2秒，视频重建质量（PSNR）达到35+，解码速度较现有方法快10倍，收敛更快。

Conclusion: GSVR实现了高效且高质量的视频隐式表示，兼具快速解码和训练速度，与现有方法相比具有明显优势，且在视频插值和压缩任务上表现优异。

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [103] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0是一个开源的OCR及文档解析工具包，提供多语言文本识别、层次化文档解析和关键信息提取三大解决方案，模型参数少于1亿，性能媲美大型视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 面对大语言模型时代文档理解需求增长，PaddleOCR 3.0旨在提供高效且准确的OCR及文档解析工具。

Method: 引入PP-OCRv5实现多语言文本识别，PP-StructureV3实现层次化文档解析，PP-ChatOCRv4实现关键数据提取，同时支持高效训练、推理与多硬件加速。

Result: 模型参数少于1亿且准确高效，性能可与数十亿参数规模的视觉语言模型相媲美。

Conclusion: PaddleOCR 3.0是一个高质量、易用且性能优异的OCR及文档处理开源方案，适合构建智能文档应用。

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [104] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了Accordion，一个将AI生成的像素设计转换为可编辑分层设计的框架，同时通过用户提示改进AI文本。


<details>
  <summary>Details</summary>
Motivation: 传统图形设计分层编辑需要专业技能，而AI生成的设计虽丰富但缺乏可编辑性，非分层设计仍能启发设计师。

Method: 利用视觉语言模型(VLM)分三阶段自上而下解析图像分层，结合多视觉专家模型辅助分层，训练时使用自建数据集和定制修复模型。

Result: 在DesignIntention基准测试及用户研究中，Accordion表现优越，支持文本模板生成、文本背景添加及文本反解码等任务，并能生成设计变体。

Conclusion: Accordion成功实现了将AI生成的图形设计转换为可编辑分层设计，降低设计门槛，增强设计灵活性和可控性。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [105] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: 本文提出了一种名为Kernel Density Steering (KDS)的推断时框架，通过局部模式搜索提升扩散模型在图像恢复任务中的保真度和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在图像恢复过程中常常面临保真度不一致和伪影问题。

Method: KDS利用多个扩散样本的集合，通过计算其补丁层面的核密度估计梯度，驱动样本向更高密度的区域移动，实现集体的局部模式搜索，从而避免伪影并提升输出质量。

Result: 实验证明，KDS在超分辨率和图像修复等真实场景中显著提升了图像恢复的定量和定性表现。

Conclusion: KDS作为一种即插即用的框架，无需重新训练或外部验证器，即能有效提高扩散模型图像恢复的质量。

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [106] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的生成方法GenHMC，利用大规模未配对的头戴式摄像头(HMC)捕获数据，生成高质量的合成HMC图像，从而促进VR/AR环境中写实头像动画的发展。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以获得同步的完整面部状态数据，且依赖大量成对采集数据，导致数据收集成本高且难以泛化。

Method: 提出基于生成模型的GenHMC方法，以未配对HMC数据和全视角Dome摄像头数据为基础，准确分离面部表情、视角和面部外观，实现高质量合成图像生成。

Result: 方法能够正确分离条件信号，生成高质量且准确的合成HMC图像，可泛化至未见过的身份，移除对成对数据的依赖。训练的面部编码器在效率和准确性上达到最先进水平。

Conclusion: GenHMC提供了一种高效且精准的头像动画建模途径，有望推动虚拟现实和增强现实中真实感头像的广泛应用。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [107] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: 本文提出了一种针对特定领域图像生成的分层语义优化框架AdaptaGen，结合矩阵型提示优化和多视角语义理解，有效提升图像质量和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将提示工程与模型适应分离，忽视了语义理解与视觉表现的内在联系，且在生成过程中缺乏领域特定的语义约束，导致图像出现幻觉和语义偏差。

Method: 提出AdaptaGen框架，包括矩阵型提示优化、多视角语义理解、跨模态适应机制和两阶段标题语义转换，确保语义一致性并提升视觉多样性。

Result: 在40个类别的多样本数据集上，每类仅用16张图像训练，实验表明该方法在图像质量、多样性和语义一致性方面均显著优于现有方法。

Conclusion: AdaptaGen有效融合语义优化和模型适应，显著改善特定领域图像生成的质量和语义准确性，解决了现有技术中存在的关键问题。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [108] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种结合RGB摄像头和事件传感器的融合方法，以提升航天器姿态估计的鲁棒性，特别是在恶劣光照条件下。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的视觉姿态估计在极端光照条件下表现不佳，而事件传感器虽然光照适应性强，但空间分辨率低且低运动时信噪比差，因此需要融合两种传感器以克服各自不足。

Method: 采用光学分束棱镜实现RGB与事件传感器的光学及时间同步，利用基于RANSAC的技术融合两类传感器信息进行姿态估计，同时引入dropout不确定性估计检测极端条件影响。

Result: 在实验室环境下采集了包含多种复杂光照条件的RGB与事件数据集，验证了该融合方法在姿态估计上的有效性。

Conclusion: 事件与RGB传感器结合的融合方法显著提升了航天器姿态估计的鲁棒性，且该数据集将公开以促进相关研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [109] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: 该论文提出了一种基于焦点映射的特征提取方法（OFFSET）以提升复合图像检索的性能。


<details>
  <summary>Details</summary>
Motivation: 现有复合图像检索方法存在图像中主导与噪声部分同质性忽视及文本引导图像修改时视觉焦点偏差的问题。

Method: 设计了两个模块：主导部分分割和双重焦点映射，用于识别图像重要部分及引导视觉和文本特征提取；提出文本引导的焦点修正模块，实现对参考图像焦点的自适应调整。

Result: 在四个基准数据集上的实验表明，该方法显著优于现有技术。

Conclusion: 通过焦点映射和文本引导的焦点修正，OFFSET有效降低噪声干扰并提升了复合图像检索的效果。

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [110] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于Contourlet变换的结构化知识引导复数扩散模型，用于极化合成孔径雷达（PolSAR）图像分类，显著提升分类精度并更好地保护边缘细节。


<details>
  <summary>Details</summary>
Motivation: 传统的实值扩散模型难以捕捉PolSAR数据中的复数相位信息，且难以保持细节结构，限制了其在极化SAR图像分类中的性能。

Method: 利用复数Contourlet变换分解数据，提取统计特征和边界信息，设计知识引导的复数扩散网络，结合高频结构信息引导扩散过程，并多尺度多方向联合学习高频特征以提升分类准确性。

Result: 在三个真实PolSAR数据集上，所提方法优于现有先进方法，尤其在边缘细节保持和区域均匀性方面表现出色。

Conclusion: 结合复数Contourlet变换和结构知识引导的复数扩散模型有效提升了PolSAR图像分类性能，实现了细节保护与区域一致性的平衡。

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [111] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了动态秩自适应（DRA）方法，通过基于特征重要性动态分配适应秩，提升视觉语言模型在新类别上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有微调视觉语言模型的方法忽略了不同token的重要性，导致模型过拟合无关特征，削弱新类别的识别能力。

Method: DRA通过序列注意力对token进行重要性分组，动态分配不同适应秩，并设计通道响应机制及L1正则化以稳定训练。

Result: 大量实验表明DRA在基类-新类分类、跨数据集评估和域泛化任务中显著优于现有方法。

Conclusion: DRA有效提升了视觉语言模型对新类别的泛化能力，是一种增强模型适应性的有效手段。

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [112] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的脑损伤分析和逆向重建框架，能区分脑组织中的损坏核心区和由于损伤增长引起的组织形变，进而恢复健康脑组织结构。


<details>
  <summary>Details</summary>
Motivation: 现有脑损伤分割方法不能区分不可逆损伤区域和由损伤引起的组织形变，影响准确医疗诊断和研究。

Method: 通过扩散模型，先对异常区域进行分割，再估计并逆转脑组织的形变，隔离损伤核心区，最后对核心区进行修复，恢复损伤前的健康脑结构。

Result: 该方法在脑损伤分割、组织表征及脑区标注方面优于传统方法，提升了分析的准确性和鲁棒性。

Conclusion: 该框架基于生物力学损伤模型，成功实现了脑损伤过程的逆转，为脑损伤的诊断和研究提供了有效工具。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [113] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: 本文提出了一种基于大规模视觉语言模型的新型GUI元素定位方法R-VLM，通过缩放区域提议和IoU感知目标函数显著提升了多平台GUI自动化中元素定位的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的GUI元素定位方法在大而复杂的截图中难以准确定位目标元素，且训练目标函数不足以反映定位质量，限制了准确率提升。

Method: 提出R-VLM方法，利用缩放的区域提议实现精确定位，并设计基于交并比(IoU)的感知目标函数，结合视觉语言模型与检测技术提升定位性能。

Result: R-VLM在ScreenSpot和AgentStudio基准测试中将定位准确率提升了13%，在AITW和Mind2Web的GUI导航任务中提升3.2%至9.7%。

Conclusion: 该方法有效结合了视觉语言模型与目标检测技术，显著提升了GUI元素定位和自动化执行任务的性能，推动了多平台GUI自动化的发展。

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [114] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 本文介绍了首个大型医疗视频生成数据集MedVideoCap-55K及相应生成模型MedGen，显著提升了医疗视频生成的视觉质量和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术在开放领域取得进展，但医疗视频生成因缺乏大规模、高质量的医疗专用数据集，产生的内容常不现实或错误，限制了临床培训和教育等应用的发展。

Method: 构建包含5.5万多个真实医疗场景视频片段的多样化、高质量带字幕数据集MedVideoCap-55K；基于该数据集开发通用医疗视频生成模型MedGen，实现视觉质量和医学准确性的平衡。

Result: MedGen在多个公开基准测试中表现领先，视觉效果和医学准确度均与商业系统相当，显著优于现有开源方案。

Conclusion: MedVideoCap-55K数据集和MedGen模型为医疗视频生成提供了坚实基础，有望推动临床培训、教育等领域的研究和应用发展。代码和数据已公开，共享促进进一步研究。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [115] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一种集成结构提示（ISP）方法，增强视觉语言模型（VLM）中跨模态的结构关系建模和信息交互，提高了模型在新旧类别上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法大多忽视了提示与模态内外令牌之间的结构关系，且难以在基础类别与新类别间实现性能平衡。

Method: 提出ISP，包括自结构和跨结构提示模块，建模跨模态和模态内的结构关系，以及样本探测模块，根据样本难度动态调整损失权重，防止过拟合简单样本。

Result: 在基础到新类别泛化、跨数据集评估和领域泛化三个场景上，ISP表现出与最先进方法相当的竞争力。

Conclusion: ISP有效提升了视觉语言模型的结构信息交互和泛化能力，为提示学习提供了一种新的思路。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [116] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: 提出LiON-LoRA框架，通过线性可扩展性、正交性和范数一致性，提升视频扩散模型对相机轨迹和物体运动的精确控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型难以实现对相机轨迹和物体运动的精准控制，主要由于LoRA融合不稳定及非线性扩展问题。

Method: 提出LiON-LoRA框架，分析浅层VDM中LoRA特征的正交性，实现解耦控制；实施范数一致性以稳定融合；引入可控token，借助修改的自注意力机制线性调整运动幅度，并扩展至时序生成。

Result: LiON-LoRA在轨迹控制准确性和运动强度调整上优于现有方法，且在极少训练数据情况下具备更强的泛化能力。

Conclusion: LiON-LoRA有效解决了视频扩散模型在空间和时间上的控制难题，提供了一种稳定且线性可调的融合策略，为精准视频生成带来新思路。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [117] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 本文综述了高光谱异常检测(HAD)技术，比较了统计模型、基于表示的方法、传统机器学习和深度学习模型在17个数据集上的性能，发现深度学习在准确度上表现最好，统计模型计算速度最快。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像数据维度高，传统异常检测方法在计算复杂性、噪声敏感性及跨数据集泛化方面存在挑战，需系统评估不同方法的性能优势与不足。

Method: 归纳并比较了统计模型、基于表示的方法、经典机器学习和深度学习四类HAD技术，在17个基准数据集上以ROC、AUC和可分离性地图等指标进行性能评估。

Result: 深度学习模型在检测准确度上表现最好，统计模型则在所有数据集上表现出极快的计算速度，各方法具有不同的优缺点。

Conclusion: 本研究为HAD领域的研究者和应用者提供了详尽的性能比较和分析，指明未来研究方向，促进该领域技术发展。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [118] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 介绍了SenseShift6D数据集，研究了测试时传感器控制对6D物体姿态估计性能的提升，超越传统数字增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有6D物体姿态估计数据集中，真实环境下的光照、曝光、增益和深度传感器模式变化影响未被充分研究，亟需一个多样化传感器参数和光照条件的数据集。

Method: 构建SenseShift6D数据集，涵盖13种RGB曝光、9种增益、自动曝光、4种深度模式和5种光照等级，采集超过10万张RGB-D图像，评估先进模型在传感器参数调整下的表现。

Result: 测试时通过传感器控制显著优于数字数据增强，个别调整RGB或深度传感器均有效，联合调整RGB-D传感器参数带来更大提升，性能接近甚至超过通过增加训练数据实现的提升。

Conclusion: SenseShift6D推动6D姿态估计从数据中心评估向传感器感知鲁棒性转变，为自适应、自动调节的感知系统在复杂现实环境中稳定运行奠定基础。

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [119] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: 本文提出了一种全自动白平衡调整机制，能有效纠正显微镜下的彩色图像颜色。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像的准确着色和平衡难度较大，传统白平衡算法效果有限。

Method: 设计并应用了一种自动白平衡算法，并在200张显微镜图像上进行了实验验证，图像包括病理形态学常用的三种标本扫描。

Result: 该算法在血红素-苯莳-番红染色和免疫组化染色显微图像上效果优于传统数字摄影白平衡算法。

Conclusion: 本文提出的自动白平衡方法适用于显微图像，提升了色彩校正的准确性和效果。

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [120] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt框架实现了从单视图图像生成高保真、可交互的关节物体，涵盖部件分割、形状重建和运动优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦表面几何和纹理，忽略部件分解与关节建模，且依赖多视角密集数据，限制了可扩展性。

Method: DreamArt采用三阶段流程：图像到3D的部件分割与完整网格重建；视频扩散模型的细调以捕捉部件级关节先验；基于双四元数进行运动优化及全局纹理细化。

Result: 实验表明，DreamArt能有效生成结构准确、纹理高质量且关节运动合理的关节物体。

Conclusion: DreamArt为关节资产生成提供了一种可扩展且高质量的解决方案，解决了单视图下复杂物体的生成难题。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [121] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了基于文本指令的多功能虚拟试衣解决方案TalkFashion，通过大语言模型理解用户指令，实现全身更换和局部编辑，提升了试衣的灵活性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试衣方法多依赖端到端网络，任务单一且缺乏灵活性，难以支持多任务及基于文本的交互。

Method: 提出TalkFashion系统，利用大语言模型分析用户文本指令，激活不同的处理流程；同时设计基于指令的局部重绘模型，无需手动提供掩码，实现自动局部编辑。

Result: 实验结果显示，TalkFashion在语义一致性和视觉质量上优于现有方法，实现了更加智能和多样的虚拟试衣体验。

Conclusion: 通过结合大语言模型及多模态模型，TalkFashion有效提升了虚拟试衣的多功能性和用户交互的自然度，推动了虚拟试衣技术的发展。

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [122] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型反演的空间感知去噪网络SPADE，用于开放词汇的全景场景图生成，显著提升了空间关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间关系推理中存在局限，难以准确区分对象的相对位置，导致关系预测效果不佳。

Method: 利用扩散模型的反演过程校准预训练扩散模型为场景图生成特定去噪网络，并设计空间感知关系图Transformer捕获局部与远程上下文信息以生成优质关系查询。

Result: 在基准全景场景图和视觉基因组数据集上，SPADE在闭集和开放集场景下均优于现有最先进方法，尤其在空间关系预测方面表现突出。

Conclusion: 通过反演引导的校准与空间感知的上下文推理，SPADE有效提升了开放词汇全景场景图生成的空间关系识别能力，推动了该领域的发展。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [123] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: 本文提出了一种端到端自回归模型DREAM，用于文档重构，解决了多阶段方法中误差传播的问题，并保留了元素布局信息。


<details>
  <summary>Details</summary>
Motivation: 现有多阶段文档分析方法存在误差传播，且生成模型缺乏布局信息，限制了文档重构效果。

Method: 提出DREAM模型，将文本图像转化为文档重构序列，包含更多元素信息，并定义任务标准，引入文档相似性度量(DSM)和DocRec1K数据集。

Result: 实验证明DREAM在文档重构任务中表现优异，并在多个子任务如布局分析、文本识别、表格结构识别等方面表现出竞争力。

Conclusion: DREAM有效提升了文档重构性能，兼容多种文档分析子任务，推动了文档理解领域的发展。

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [124] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 提出了利用太阳高度作为全局条件变量生成合成相机传感器数据的方法，以应对真实数据获取的限制及白天光照变化的挑战。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据采集成本高且受限，尤其是标注成本及白天光照变化缺乏有效建模，影响自动驾驶系统的鲁棒性。

Method: 引入太阳高度作为全局条件变量，从地理坐标和当地时间自动计算，并设计针对该变量敏感度的归一化方法，在扩散模型中捕捉光照特征及光照相关噪声。

Result: 方法能够准确模拟光照特性及光照依赖的图像噪声，弥补了白天光照变化模拟的研究空白。

Conclusion: 利用太阳高度进行条件生成有助于提升合成数据质量，促进安全鲁棒自动驾驶功能的开发，减少对人工标注的依赖。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [125] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种系统化框架，用于生成带有实例标注、颜色和法向量的完整3D桥梁点云数据，并模拟生成不完整的点云以训练深度学习模型，实现了高效准确的桥梁语义分割和构件补全。


<details>
  <summary>Details</summary>
Motivation: 传统桥梁检测方法效率低，3D点云技术受限于数据不完整性，特别是缺少标注和扫描遮挡，限制了其应用潜力。

Method: 提出一个自动生成桥梁完整3D点云的框架，包含组件级实例标注、高保真颜色及精准法向量；并扩展产生多样且物理真实的不完整点云，用于训练分割和补全网络。

Result: 用合成数据训练的PointNet++模型在真实桥梁语义分割任务中达到了84.2%的mIoU，微调后的KT-Net在组件补全任务中表现优异。

Conclusion: 该方法提供了创新的3D桥梁可视化分析手段和基础数据集，对桥梁自动化管理和维护具有重要意义。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [126] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的“2D-3D-2D”图像编辑框架，通过3D表示提升2D物体，实现更真实和一致的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在2D图像编辑中虽然精度高，但因像素操作特性，难以保持编辑一致性和物体身份。

Method: 将2D物体提升至3D表示，在受刚性约束的3D环境中编辑，再重新投影并修补回2D图像，实现更真实的编辑。

Result: 大量实验证明该框架性能优越，编辑效果比现有方法（如DragGAN和DragDiffusion）更一致且稳健保持物体身份。

Conclusion: 通过在3D环境中编辑，本文方法有效解决了现有2D编辑方法的一致性和身份保持问题，提升了编辑质量。

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [127] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出了一种结合视觉基础模型和大型语言模型的新框架，实现视频的高级认知分析如因果推理和未来预测，达到多项基准测试的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解模型虽然能识别事件内容，但缺乏常识性世界知识，导致难以完成因果推理和未来预测等高级认知任务。

Method: 设计了一个融合模块（灵感来源于Q-Former架构），将复杂的时空和对象视觉特征转化为语言对齐的表达，再通过两阶段训练策略（大规模视频文本对齐预训练+定向指令微调），提升模型的推理和预测能力。

Result: 模型在多个挑战性基准上表现出色，具备优秀的零样本泛化能力，消融实验证明各关键组件对性能提升的重要性。

Conclusion: 该研究推动了机器感知从识别向认知理解的转变，有望应用于机器人和人机交互等领域，提升AI系统智能水平。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [128] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: 本文针对语义分割中少样本分割的注释瓶颈问题，提出了一种新方法I²R，通过类别特定的全局语义特征和方向性掩码策略，提高了分割性能，在两个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统少样本分割方法存在支持图像与查询图像间的语义鸿沟导致的特征不匹配，以及图像内部视觉相似但语义不同区域带来的错误预测问题，严重影响分割效果。

Method: 提出I²R方法，包括1）利用类别特定的高层全局语义表示改善不同图像间区域定位，2）方向性掩码策略抑制支持与查询图像中视觉相似但掩码矛盾的像素对，减少误判。

Result: 在PASCAL-5^i和COCO-20^i两个数据集的1-shot设置下，I²R方法分别提升了1.9%和2.1%的mIoU指标，优于现有最先进方法。

Conclusion: I²R有效解决了少样本分割中的语义不匹配和伪阳性问题，显著提升了分割性能，展示了在少样本语义分割领域的重要应用价值。

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [129] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种名为USIGAN的免疫组化虚拟染色方法，通过不依赖空间位置对应的非平衡自信息特征传输，有效解决了弱配对条件下相邻切片间空间异质性带来的挑战，提高了生成图像的内容一致性和病理语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的免疫组化虚拟染色在弱配对条件下，相邻切片之间的空间异质性导致映射不准确，生成的虚拟染色图像缺乏病理语义一致性，影响诊断效果。

Method: 提出USIGAN方法，利用非平衡自信息特征传输提取全局形态语义，移除弱配对项以改善联合边缘分布；设计非平衡最优传输一致性机制（UOT-CTM）和病理自对应机制（PC-SCM）构建不同层级相关矩阵，增强图像间的语义一致性。

Result: 在两个公开数据集上的实验结果显示，USIGAN方法在包括IoD和Pearson-R相关系数等多个临床相关指标上表现优异，明显优于现有方法。

Conclusion: 该方法有效解决了弱配对条件下免疫组化虚拟染色的空间异质性问题，提升了生成图像的病理语义一致性和临床相关性，具有较高的应用价值。

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [130] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: 提出了一种名为DFYP的动态融合框架，用于基于遥感数据的作物产量预测，显著提升了空间建模能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感作物产量预测方法在空间模式建模、跨作物类型和年份的泛化能力方面存在不足，难以应对复杂多变的农业条件。

Method: DFYP框架包括三个主要组件：（1）分辨率感知通道注意力模块（RCA），自适应调整输入通道权重；（2）自适应算子学习网络（AOL-Net），动态选择卷积核以增强边缘敏感的空间特征提取；（3）双分支结构结合可学习的融合机制，同时建模局部空间细节和全局上下文信息，实现跨分辨率和跨作物的泛化。

Result: 在多年份MODIS数据集和多作物Sentinel-2数据集上，DFYP在均方根误差（RMSE）、平均绝对误差（MAE）和决定系数（R2）指标方面均优于现有最先进方法，展现出较强的适应性和鲁棒性。

Conclusion: DFYP框架有效克服了遥感作物产量预测中的空间异质性和动态变化挑战，提升了不同空间分辨率和多种作物类型下的预测准确性，具有广泛的实际农业监测应用潜力。

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [131] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种用于动态高斯点云序列的前馈压缩框架D-FCGS，通过引入帧组结构和稀疏控制点运动提取，实现即时高效的三维场景压缩，并在保持视觉质量的同时大幅提升压缩率。


<details>
  <summary>Details</summary>
Motivation: 动态三维表示的高效压缩仍具挑战，现有基于优化的编码方法通用性差，限制了应用推广。

Method: 提出D-FCGS框架，采用组帧结构与I-P帧编码，利用稀疏控制点提取运动信息，结合双先验熵模型前馈压缩运动张量，结合控制点运动补偿和细化网络保证重建质量。

Result: D-FCGS在不用场景优化的情况下，达到与优化方法相当的率失真性能，实现超40倍压缩且重建时间小于2秒，保持多视角视觉一致性。

Conclusion: 本研究推动了动态3D高斯点云的前馈压缩发展，为沉浸式应用的自由视角视频传输和存储提供了可扩展的解决方案。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [132] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: GeoMag是一个专为遥感图像设计的大规模视觉语言模型框架，能够高效处理多粒度图像解析任务，尤其擅长像素级任务，小目标识别，并有效降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言模型（RS-VLMs）多局限于图像级和区域级任务，难以处理像素级任务且在小目标识别中表现欠佳，同时高分辨率遥感图像处理计算资源消耗大，限制了其实用性。

Method: 提出GeoMag框架，采用基于提示语义动态调整注意力范围，通过任务驱动的多粒度分辨率调整（TMRA）和提示引导的语义感知裁剪（PSC），自适应降低任务无关区域的空间分辨率，增强任务相关区域的视觉表现，从而提升关键目标感知，抑制背景冗余，并降低计算成本。

Result: 在10个基准测试中，GeoMag在像素级任务上表现出色，同时在其他粒度任务上也维持了与现有遥感视觉语言模型的竞争性能。

Conclusion: GeoMag有效解决了遥感图像多粒度、多粒度任务识别和高分辨率处理的难题，实现了性能与计算效率的平衡，具备广泛的实际应用潜力。

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [133] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 本文针对多模态视觉跟踪中传感器同步缺失导致的数据不完整问题，提出了一种灵活的异构专家混合模型，实现动态适应缺失率和场景复杂性的跟踪方法，显著提升缺失模态情况下的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 多模态数据对视觉跟踪提升鲁棒性有益，但传感器数据同步缺失导致临时数据缺失，现有跟踪器在此情况下表现较差，缺乏适应性。

Method: 提出基于异构专家混合机制的灵活融合框架，通过视频级掩码策略保证时空一致性，实现根据缺失率动态激活计算单元，适应缺失模态和场景复杂性变化。

Result: 模型在9个基准测试中实现了最先进性能，且在完整模态和缺失模态设置下均表现优异。

Conclusion: 提出的方法有效解决了多模态跟踪中的数据缺失问题，提升了模型的适应性和鲁棒性，推动了多模态视觉跟踪的发展。

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [134] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本文针对遥感图像场景分类中可解释人工智能方法及其评估指标的适用性进行系统分析，揭示了当前方法和指标的局限性，提出了选择建议。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能方法及相关评估指标大多基于计算机视觉领域的自然图像，直接应用于遥感图像可能不适用，因此需要针对遥感图像场景分类问题重新评估这些方法和指标的有效性。

Method: 系统分析了5种特征归因方法（Occlusion, LIME, GradCAM, LRP, DeepLIFT）在3个遥感数据集上的表现，同时评估了10种解释评估指标，涵盖真实性、鲁棒性、定位、复杂度和随机化5类指标，通过方法论和实验分析发现各自的优缺点。

Result: 扰动基方法表现依赖扰动基线和遥感场景空间特征，GradCAM难以处理多标签图像，LRP等相关传播方法空间分布不均；评估指标中真实性指标存在同样问题，定位和复杂度指标对大空间范围类别不可靠，鲁棒性和随机化指标稳定性较好。

Conclusion: 针对遥感图像场景分类，本文总结了各解释方法和指标的局限和适用场景，提出了合理选择解释方法、评估指标及超参数的指导原则。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [135] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了MGPO，一种基于多轮对话的强化学习框架，自动裁剪图像关键区域，强化大规模多模态模型的视觉定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态模型处理高分辨率图像时生成大量无关视觉tokens，影响下游任务性能，且训练依赖昂贵的视觉定位标注。

Method: MGPO利用多轮对话框架和强化学习，通过模型预测的定位坐标自动裁剪子图像，仅用基于最终答案正确性的二元奖励函数训练；并设计对话模板解决冷启动问题。

Result: MGPO在无定位标注的标准视觉问答数据上训练，在多个测试集上优于GRPO，尤其在OOD V* Bench数据集上提升5.2%，且在Qwen2.5-VL-7B模型上表现优于OpenAI的o1和GPT-4o模型。

Conclusion: MGPO框架有效提升了视觉定位能力，减少对额外标注的依赖，促进大规模多模态模型在高分辨率图像处理中的表现，具备良好的鲁棒性和泛化性。

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [136] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: 本文通过引入单目深度估计提升视频实例分割在遮挡、运动模糊和外观变化中的鲁棒性，提出三种融合策略，其中扩展深度通道（EDC）和共享ViT（SV）显著提升性能，EDC方法在OVIS基准上达到56.2 AP，创下新记录。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割面临物体遮挡、运动模糊及外观变化导致的时序关联挑战，现有方法难以有效应对，故引入几何感知即深度信息以增强鲁棒性。

Method: 系统探索三种深度融合范式：扩展深度通道（将深度图作为输入通道）、共享ViT（深度估计与分割共用ViT骨干网络）、深度监督（以深度预测作为辅助训练信号）；其中EDC与SV方法效果显著。

Result: 基于Swin-L骨干网络，EDC方法在OVIS数据集上取得56.2 AP，刷新状态-of-the-art成绩；DS方法提升有限。

Conclusion: 深度线索作为提升视频理解鲁棒性的关键因素得以确立，验证了几何感知在视频实例分割中的重要作用。

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [137] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种稀疏表示方法以提高神经表面重建的分辨率和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的密集体素表征在提高分辨率时内存占用剧增，限制了重建质量。

Method: 使用两阶段方法，先预测体素占用率，再仅在高占用体素上进行特征计算和体积渲染，开发了高效采样和查询算法。

Result: 存储需求降低50倍以上，支持512^3分辨率重建，重建精度优于现有方法。

Conclusion: 稀疏体素表示极大提升了内存效率和重建质量，是通用神经表面重建的有效方案。

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [138] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 本文提出了Tora的升级版Tora2，改进了运动引导视频生成中的外观和运动定制能力，实现了多实体的同时定制。


<details>
  <summary>Details</summary>
Motivation: 现有的运动引导视频生成方法在多实体的外观和运动定制上存在限制，且多模态条件下的对齐效果不佳。

Method: 引入了去耦合的个性化提取器生成多实体的个性化嵌入，设计了门控自注意力机制融合轨迹、文本及视觉信息，并采用对比损失优化运动与个性化嵌入的一致性。

Result: Tora2在多实体外观和运动定制上效果优异，显著减少多模态条件下的错位，与最先进方法表现竞争力强。

Conclusion: Tora2实现了多实体视频内容的同步定制，推动了多条件视频生成技术的重要进展。

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [139] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: 本文提出了T-LoRA，一种针对扩散模型个性化的低秩适应方法，解决了单图像定制时过拟合的问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在样本有限时微调容易过拟合，降低生成的多样性和泛化能力，尤其是在单图像定制任务中效果最差。

Method: 提出了基于时间步依赖的动态低秩适应策略，针对不同扩散时间步调整微调力度，并通过正交初始化保证适配器组件独立性。

Result: 大量实验表明，T-LoRA及其组件优于标准LoRA和其他个性化方法，在概念忠实度和文本一致性之间取得更好平衡。

Conclusion: T-LoRA有效提升了单样本扩散模型微调的表现，适用于数据和资源受限场景，展现出较大的实际应用潜力。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [140] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一种自动生成具有高质量合成三元组数据集CIRHS的可扩展流水线和创新的混合上下文对齐（CoAlign）模型，用于提升图像检索任务中的多模态查询效果，实现了零样本下的优异性能并超越现有监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统的组合图像检索方法依赖昂贵的人工标注三元组，限制了模型的可扩展性和零样本适应能力。

Method: 设计了利用大型语言模型生成多样化文本提示，并控制文本生成图像模型制作图像对，再经过筛选重组形成合成数据集CIRHS；引入CoAlign框架进行全局和局部信息融合以增强表示学习能力。

Result: 基于合成CIRHS数据集训练的CoAlign在三个公开基准零样本测试中表现出色，且在监督训练下击败所有现有最先进的监督模型。

Conclusion: 首次证明了利用纯合成三元组数据训练组合图像检索模型的可行性；提出的方法提高了模型的泛化能力和检索性能，具有良好应用前景。

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [141] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: 提出了一种名为SCINet的新方法，通过匹配标签和实例之间的共现模式，解决了部分多标签学习中的标签歧义问题。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习面临标签和实例之间关系不明确的挑战，核心在于准确识别标签之间的模糊关系。

Method: 设计了SCINet，包括双主导促发器模块利用多模态模型捕捉文本与图像关联，跨模态融合模块联合建模标签与实例关系，以及内在语义增强策略通过图像变换提升语义理解。

Result: 在四个广泛使用的基准数据集上进行实验，SCINet表现优于现有最先进方法。

Conclusion: 匹配标签和实例的共现模式是解决部分多标签问题的关键，SCINet提供了有效的框架和策略，显著提升了预测性能。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [142] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: 该论文提出了一种基于集成的方法来提升深度伪造检测模型在不同数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测模型在跨数据集测试时性能显著下降，需要提升模型的泛化性能以应对真实应用中的多样化数据。

Method: 基于多个最先进的非对称模型预测概率进行集成，利用最近的开源基准平台作为基础，进行跨域数据集的实验验证。

Result: 在两个不同的域外数据集上实验表明，单一模型表现不稳定，而基于集成的预测具有更稳定和可靠的性能。

Conclusion: 非对称集成方法为实际应用中的深度伪造检测提供了一种稳健且可扩展的解决方案，不依赖于伪造类型或质量的先验知识。

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [143] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种结构化的激光雷达点云地理配准和空间校正方法，结合预训练模型与卫星图像，实现无GNSS信息下的3D地图重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖GNSS和IMU数据，假设定位稳定，但在高楼密集的城市区域容易产生误差，导致激光点云不能准确配准。

Method: 利用预训练的Point Transformer进行道路点分割，提取道路骨架及交叉点与目标地图对齐，先进行全局刚性配准，再用径向基函数插值做局部精调，结合SRTM数据做高程校正。

Result: 在KITTI数据集上平面配准误差标差降低了55.3%，高程相关性提升30.5%；在缺乏GNSS的珀斯数据集上，平面误差降低77.4%，高程相关性提升50.4%。

Conclusion: 该方法有效实现了无GNSS信息情况下的激光雷达点云准确地理配准和高程校正，提升了城市级3D地图重建的精度和可靠性。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [144] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 本文提出了一种名为GCDA的新框架，通过结合字符感知注意力和OCR反馈机制，显著提升了扩散模型生成图像中的文字可读性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的扩散模型虽能生成高质量图像，但生成的文字往往不可读或拼写错误，限制了其在广告、教育和设计等领域的应用。

Method: GCDA框架通过双流文本编码器融合语义和字形信息，引入字符感知注意力机制及新的注意力分离损失，避免字符间的注意力混淆，并采用OCR反馈进行细粒度微调以提升文字准确性。

Result: 在MARIO-10M和T2I-CompBench等多个大规模数据集上，GCDA在字符错误率（0.08 vs 0.21）和单词错误率（0.15 vs 0.25）等文字相关指标上优于先前最佳模型，同时保持高分辨率图像质量（FID 14.3）。

Conclusion: GCDA通过对文本特征和注意力机制的优化显著提升了生成图像中文字的可辨识度和准确率，推动了文本生成图像技术向实用化迈进。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [145] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出VisualSpeaker方法，结合光度真实可微渲染和视觉语音识别，提升3D面部动画的真实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的网格域方法限制了利用2D计算机视觉和图形领域快速发展的视觉创新的能力，需要一种更高保真度的3D面部动画方法。

Method: 通过光度真实的3D高斯点渲染生成头像，并将渲染结果输入预训练的视觉自动语音识别模型，引入感知唇读损失来监督训练。

Result: 在MEAD数据集上，VisualSpeaker在标准的Lip Vertex Error指标上提升了56.1%，同时显著提升了生成动画的感知质量。

Conclusion: VisualSpeaker不仅提升了动画的视觉质量和准确度，还保持了网格驱动动画的可控性，并有效支持手语头像中的准确唇形表达。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [146] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了MEDTalk，一种细粒度动态情感驱动的3D面部动画生成框架，实现了唇动和表情的高质量同步与个性化控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于静态预定义情感标签，导致动画多样性和自然性受限，亟需细粒度、多动态情感表达的解决方案。

Method: 设计交叉重构过程解耦内容和情感嵌入空间，实现唇动和表情的独立控制；结合音频和语音文本，预测帧级情感强度变化，动态调整情感特征；引入多模态输入（文本描述和参考表情图像）以增强控制与个性化；优先支持MetaHuman平台。

Result: 生成的3D面部动画唇动与表情表现自然细腻，能够动态呈现情感强度变化，并支持多模态定制化表达，方便应用于工业生产。

Conclusion: MEDTalk有效提升了3D情感面部动画的表现力和控制精度，促进了更加真实和个性化的音频驱动动画生成。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [147] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: 提出了多模态因果分析模型（MCAM），通过构建视觉和语言模态之间的潜在因果结构，实现对自动驾驶视频中驾驶行为的准确识别和推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在挖掘浅层因果关系、忽视跨模态的伪相关以及未能建模自车级别因果关系的问题。

Method: 设计多层特征提取器捕获长距依赖，利用因果分析模块构建驾驶状态的有向无环图，结合视觉语言变换器对关键视觉特征与语言表达进行对齐。

Result: 在BDD-X和CoVLA数据集上，模型在视觉-语言因果关系学习任务中达到SOTA性能，且表现出优越的视频序列因果特征捕获能力。

Conclusion: MCAM有效提升了自动驾驶视频理解中的因果关系建模能力，证明其在自动驾驶领域的实用性和优越性。

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [148] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的表面法向整合方法，能够显式处理深度不连续性并适用于通用中央相机模型，显著提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有的法向整合方法仅隐式处理深度不连续且局限于正射或理想针孔相机，无法满足更复杂的相机模型和深度变化场景需求。

Method: 基于局部平面假设，建立表面法线与光线方向间的约束，形成一种能显式处理不连续性的法向整合新框架，适用于通用的中央相机模型。

Result: 在标准法向整合基准测试中取得了最先进的结果，准确地模拟了深度与法向的关系，实现了对通用中央相机的直接处理。

Conclusion: 该方法有效突破了传统限制，提升了法向整合的准确性和适用范围，为形状重建技术提供了更强的工具。

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [149] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的自然无限制对抗样本生成方法ScoreAdv，实现高成功率和优质图像。


<details>
  <summary>Details</summary>
Motivation: 传统的$l_p$-范数对抗攻击与人类感知不符，现有基于GAN和扩散模型的方法存在图像质量差和未充分利用扩散模型噪声去除能力的问题。

Method: ScoreAdv利用解释性对抗引导机制，逐步将采样分布向对抗分布转变，并通过显著性图注入参考图像视觉信息，生成无限自然的对抗样本。

Result: 在ImageNet和CelebA数据集上的十个目标模型中，ScoreAdv在黑盒和白盒设置均取得了最先进的攻击成功率和图像质量。

Conclusion: ScoreAdv动态平衡去噪与对抗扰动，不仅有效攻击分类模型，还能攻击检索模型，并对防御措施表现出强鲁棒性。

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [150] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: 本文提出了一个名为CAST-Phys的新型多模态远程生理情感数据库，旨在无接触环境下通过面部和生理信号识别情绪。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别系统受限于缺乏多模态数据集和接触式设备影响情感真实性，需要一种无接触方式提取多模态情感线索。

Method: 构建高质量数据集，包含PPG、EDA、呼吸频率及高分辨率面部视频，分析单一及多模态融合对远程情感识别效果的影响。

Result: 研究显示生理信号在真实场景中补充面部表情的信息不足，融合多模态信号可有效提升无接触情感识别性能。

Conclusion: 提出的CAST-Phys数据库及多模态融合方法促进了无接触远程情感识别技术的发展。

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [151] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: 该论文介绍了DS@GT团队在PlantCLEF 2025多物种植物识别竞赛中的第二名解决方案，采用细调的Vision Transformer和基于领域先验的聚类与地理位置过滤方法，实现了较好的植物识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决多物种植物识别问题，提高植被样方图像中植物的识别准确性。

Method: 采用细调的Vision Transformer ViTD2PC24All进行图像块级推断，使用4x4的切片策略，使得切片大小匹配网络的接受视野；结合PaCMAP和K-Means进行视觉聚类，并结合地理位置信息进行过滤，以实现领域先验的适应。最后通过多数投票和聚类特定的贝叶斯先验权重整合切片预测结果。

Result: 模型在私有排行榜上实现了宏平均F1分数0.348，不依赖额外训练。

Conclusion: 结合细调Vision Transformer与领域先验的可视化聚类与地理位置过滤的策略有效提升了多物种植物识别性能，且方法公开透明，方便复现。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [152] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 该论文针对预训练视觉语言模型在区分视觉相似但文化不同的概念上能力不足的问题，提出了合成文化数据集CulTwin，并基于此数据集微调CLIP模型，提升了细粒度文化辨识能力。


<details>
  <summary>Details</summary>
Motivation: 当前预训练视觉语言模型难以捕捉具有文化差异的细粒度视觉特征，主要由于缺乏高质量文化特定数据集、缺乏上下文知识集成以及缺少强调细微差异的难负样本。

Method: 设计数据构建流程，结合开源视觉语言模型和文本生成图像扩散模型，构建合成文化数据集CulTwin，包含视觉相似但文化背景不同的三元组（概念-标题-图像）；基于CulTwin微调CLIP模型，采用定制对比学习方法对文化概念进行更精细的语境对齐。

Result: CultureCLIP在文化相关基准测试中，细粒度概念识别性能较基础CLIP提升最高5.49%，同时保持了原有的通用化能力。

Conclusion: 通过合成数据集和定制训练策略，本方法有效提升了视觉语言模型对细微文化区别的感知能力，验证了其在文化语境理解任务中的有效性。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [153] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock提出了一种基于3D高斯散点的几何感知反射建模框架，通过显式解耦传输和反射组件，提升复杂反射的建模与几何一致性，实现了真实场景下的高质量反射渲染。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF和3D Gaussian Splatting方法难以准确处理反射面，常误将反射误判为物理几何形状，导致重建质量下降，且现有几何约束不完整，难以适应复杂真实场景。

Method: 提出Ref-Unlock，采用双分支表示结合高阶球谐函数捕捉高频反射细节，并利用伪反射去除模块引导干净分解；引入伪深度图和几何感知的双边平滑约束增强几何一致性。

Result: 实验结果显示Ref-Unlock在反射建模上显著优于传统基于GS的方法，并在与NeRF模型的竞争中表现良好，同时支持灵活的视觉基础模型驱动反射编辑。

Conclusion: Ref-Unlock为复杂反射场景的真实感渲染提供了一种高效且可推广的解决方案，提升了反射处理的准确性和重建质量。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [154] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: 提出了Omni-Video，一个高效统一的视频理解、生成和基于指令的视频编辑框架，通过多模态大语言模型生成的连续视觉线索引导扩散解码器，实现高质量视频的生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型多聚焦于图像处理，缺乏统一的视频理解和生成模型，导致视频多模态任务发展受限。

Method: 设计轻量级架构扩展多模态大语言模型，通过视觉头生成视觉令牌，并用适配器适配扩散解码器的条件空间；采用多阶段训练方案，提升模型连接速度和训练效率。

Result: 模型在视频生成、编辑和理解任务中展现出良好的泛化能力，数据和计算资源要求较低。

Conclusion: Omni-Video有效统一了视频的理解、生成和编辑任务，促进了视频多模态模型的发展，提供了高效实用的多任务视频建模解决方案。

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [155] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种免提示条件扩散框架用于多物体图像增强，通过局部-全局语义融合策略和LoRA注入知识，结合计数损失提升生成图像的多样性和准确性，在下游任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有多物体图像生成方法依赖文本条件易产生类别偏差，或依赖原图导致生成图像多样性不足，限制了数据增强效果。

Method: 引入局部-全局语义融合替代文本条件，利用LoRA注入类别知识，设计基于计数的奖励损失辅助重建损失，约束生成图像中各类别对象数量，提高生成数据的多样性和准确性。

Result: 实验结果显示该方法在多物体图像增强方面优于多种先进方法，提升了下游任务性能和跨域泛化能力。

Conclusion: 该免提示条件扩散框架有效缓解类别偏差和生成多样性不足的问题，为多物体图像数据增强提供了新途径。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [156] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SoftReMish的新激活函数，用于提升CNN在图像分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 提升卷积神经网络在图像分类任务中的训练性能和泛化能力。

Method: 在标准CNN架构中，用SoftReMish替代所有可训练层的激活函数，并使用MNIST数据集进行对比实验。

Result: SoftReMish实现了最低训练损失3.14e-8和最高验证准确率99.41%，表现优于ReLU、Tanh和Mish。

Conclusion: SoftReMish具有更好的收敛性和泛化能力，是视觉识别任务中的有力激活函数选择。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [157] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: 该论文提出了一种基于广义相似矩阵的平滑算子，通过对称Sinkhorn算法将其归一化为类似扩散的算子，实现了在不规则数据上的拉普拉斯式平滑。


<details>
  <summary>Details</summary>
Motivation: 现有的拉普拉斯算子构建需要结构良好的域，但许多实际数据如点云、稀疏体素网格缺乏此类结构，传统方法存在边界偏差问题。

Method: 利用对称Sinkhorn算法对正的平滑算子进行重标定，使其具有扩散过程的结构行为，从而构建拉普拉斯类算子用于不规则数据的平滑与处理。

Result: 构建的算子不仅近似热扩散过程，还保留了拉普拉斯算子的谱特征，能有效应用于形状分析和匹配。

Conclusion: 该方法弥补了传统平滑算子在非结构化数据上的不足，提供了一种理论和应用兼备的拉普拉斯式平滑工具。

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [158] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: 提出OmniPart框架，实现具有可编辑部件结构的3D物体生成，支持语义解耦和结构连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法多为整体形状，缺乏可编辑的部件结构，限制交互应用。

Method: 采用两阶段方法：1）自回归结构规划生成可控的3D部件包围盒序列，利用灵活的2D部件掩码指导分解；2）基于预训练整体3D生成器的空间条件修正流模型同时合成所有部件，保持布局一致。

Result: 支持用户定义的部件粒度和精确定位，实验证明性能领先且适用性广。

Conclusion: OmniPart实现了更具语义可解释性和编辑性的3D内容生成，促进交互式应用发展。

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [159] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: 本文提出了一种结合多模型集成和链式思维推理的科学图表视觉问答方法，在SciVQA 2025任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答方法难以精准理解科学数据中的数值、多步骤推理及视觉与文本一致性问题。

Method: 采用参数规模5B至8B的多模型，重点优化提示设计和链式思维推理，并通过多模型集成提升性能。

Result: 单模型InternVL3在测试集上ROUGE-1和ROUGE-L F1分别达0.740，BERTScore达0.983，集成模型表现优于多数单模型。

Conclusion: 提示优化、链式推理及模型集成能显著提升科学图表视觉问答的效果，InternVL3为最强单模型。

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [160] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了SceneDINO，一种无需监督的语义场景完成方法，通过多视角自监督学习，实现单图推断3D几何和语义，达到甚至超越有监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义场景完成依赖昂贵的标注，缺乏无需监督的方法，因此该研究旨在开发一个无须语义和几何真值的SSC模型。

Method: SceneDINO利用自监督表示学习和2D无监督场景理解技术，通过多视角一致性自监督训练，结合3D特征蒸馏，推断3D几何和语义。

Result: 在3D和2D无监督场景理解任务中，SceneDINO达到了最先进的分割准确率，线性探测的3D特征表现与当前有监督方法相当。

Conclusion: SceneDINO证明了无监督学习在单图3D场景语义完成中的有效性，具备良好的领域泛化和多视图一致性，为单图3D场景理解奠定基础。

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [161] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了RSRefSeg 2，一种将远程感知图像分割中的定位和精细分割解耦的双阶段方法，通过结合CLIP和SAM的优势，提升了语义理解和分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法因将目标定位与边界描绘耦合处理，导致在复杂语义关系和跨模态对齐中效果有限，模型泛化性和可解释性较差。

Method: 提出RSRefSeg 2框架，首先使用CLIP进行粗定位，通过级联二阶提示器分解文本嵌入提升定位精度，随后利用SAM进行精细分割，实现跨模态语义传递和像素级掩码生成。

Result: 在RefSegRS、RRSIS-D和RISBench数据集上，RSRefSeg 2在分割精度（gIoU提升约3%）及复杂语义理解方面优于当前方法。

Conclusion: 通过解耦目标定位与分割及战略性基础模型协作，RSRefSeg 2提升了遥感影像中的语义解析能力和分割性能，具有更好的泛化性和解释性。

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [162] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: AnthroTAP通过自动生成伪标签训练数据，利用SMPL模型极大提升了点跟踪模型的性能，且训练效率远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 人为标注点跟踪训练数据工作量大且困难，尤其是在人类运动复杂且存在遮挡情况下。

Method: 利用SMPL模型拟合视频中检测到的人体，生成3D网格顶点投影到2D图像平面形成伪轨迹，通过射线投射处理遮挡，基于光流一致性过滤不可靠轨迹，从而自动生成伪标签数据进行训练。

Result: 基于AnthroTAP生成的数据训练的点跟踪模型在TAP-Vid基准测试中性能达到最先进水平，训练所需数据量比传统方法少1万倍，训练耗时与计算资源大幅减少。

Conclusion: AnthroTAP提供了一条高效自动化生成点跟踪训练数据的途径，解决了数据标注难题，实现了高效且性能优越的模型训练。

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [163] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 提出了TokenShapley，一种结合Shapley值和KNN检索的新颖token级数据归因方法，有效提升了大语言模型生成内容中特定关键词的归因准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的句子级归因方法无法满足用户对生成结果中特定关键词（如数字、年份、姓名）归因的需求。

Method: 提出结合Shapley值数据归因与KNN辅助检索的TokenShapley方法，通过预计算的数据存储和上下文检索，细粒度评估token的重要性。

Result: 在四个基准测试中，TokenShapley在token级归因准确率上较现有最优方法提升了11%至23%。

Conclusion: TokenShapley有效提升了对大语言模型生成内容中关键token的归因能力，促进生成内容的可验证性。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [164] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文提出用用户行为预测替代知识检索和推理任务来衡量大语言模型的泛化能力，并在推荐系统数据集上验证了该方法，结果显示GPT-4o优于其他模型但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 由于数据污染，传统的知识检索和推理任务不适合评估大语言模型的泛化能力，因此需要一种理论上合理且可扩展的替代方法。

Method: 提出基于用户行为预测的框架，并在电影和音乐推荐数据集上对GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct进行了测试。

Result: 测试结果与框架预测一致，GPT-4o表现优于GPT-4o-mini和Llama，所有模型均有较大提升空间，尤其是Llama。

Conclusion: 用户行为预测是衡量大语言模型泛化能力的有效替代方案，能够更好地反映模型在个性化场景下的性能。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [165] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出了ASCEND方法，通过自适应阈值对比学习显著提升隐性性别歧视检测效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上隐性性别歧视内容难以被传统检测方法识别，需要更有效的检测框架。

Method: 引入基于阈值的对比学习，通过学习的相似度阈值选择正样本对，结合词级注意力和情感、情绪、毒性特征进行联合优化。

Result: 在EXIST2021和MLSC数据集上，ASCEND在多个任务中Macro F1值平均提升9.86%、29.63%和32.51%。

Conclusion: ASCEND方法有效捕捉隐性性别歧视的细微语义差异，显著优于现有检测技术。

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [166] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 本文提出了一种结合情感分析和多模态信息融合的AI框架，有效预测远程学习中的学生辍学风险，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型难以捕捉学生互动中复杂的情感与上下文因素，导致远程学习辍学问题难以有效预测和干预。

Method: 采用基于检索增强生成(RAG)的BERT模型进行领域特定情感分析，通过提示工程识别学业压力指标，利用跨模态注意力融合文本、行为和社会人口统计数据，构建全面的风险画像。

Result: 在包含4423名学生的纵向数据集上，该框架实现了89%的准确率和0.88的F1分数，较传统模型提升了7%，假阴性率降低21%。

Conclusion: 该系统不仅提升了辍学风险预测精度，还能生成针对性干预措施，推动预测分析与教育实践的融合，为全球教育系统降低辍学率提供了可扩展方案。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [167] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: 本文提出了LCDS系统，通过逻辑控制和来源映射解决大语言模型生成出院小结时的幻觉和溯源难题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成出院小结时存在内容不准确和虚构信息的问题，且电子病历数据冗长，难以准确溯源。

Method: 通过计算电子病历与出院小结的文本相似度构建来源映射表，利用逻辑规则约束生成内容范围，支持内容来源归因，并记录专家修正以用于模型增量微调。

Result: LCDS生成了更可靠的出院小结，可针对不同临床领域定制，支持专家高效审核和反馈。

Conclusion: LCDS有效缓解了大语言模型在出院小结生成中的幻觉问题和溯源难题，提升了生成内容的准确性和可追溯性，有助于临床应用。

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [168] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: MindFlow是首个面向电商的多模态大语言模型代理，基于CoALA框架，整合记忆、决策和动作模块，提高了复杂查询处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂多模态电商场景下能力有限，难以满足客户服务需求。

Method: 基于CoALA框架构建，采用模块化“MLLM-as-Tool”策略，结合记忆、决策和动作模块，实现视觉-文本推理。

Result: 通过在线AB测试和仿真消融实验，MindFlow在处理复杂查询、提升用户满意度及降低运营成本方面表现优异，实际部署中相对提升达93.53%。

Conclusion: MindFlow显著增强了电商多模态客户服务的能力，提升了服务效率和用户体验，具备广泛应用前景。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [169] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoRA-Augmented Generation (LAG)的方法，用于高效选择和组合大量任务特定的LoRA适配器，在无需额外训练或数据访问下提升知识密集型任务的性能。


<details>
  <summary>Details</summary>
Motivation: 面对大量针对特定任务和领域微调的语言模型专家，迫切需要高效的选择和组合方法以更好地利用这些模型。

Method: 提出LAG方法，通过在每个token和层级上筛选、检索并应用专家模型，且无须额外训练或访问数据，实现模型的高效组合。

Result: 在多个知识密集型任务上，LAG的性能优于现有无数据方法，并且在有额外数据时能与如检索增强生成（RAG）等方法兼容。

Conclusion: LAG是一种高效、灵活的专家模型组合方法，能提升任务性能且无需额外训练，适用于多种知识密集型应用场景。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [170] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型中推理流程的策略，发现训练中使用较长且结构合理的推理路径比使用最优路径更有助于模型泛化。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型解决复杂问题时的推理能力，探索计算资源分配与推理路径结构对性能的影响。

Method: 构建基于分层图最短路径任务的控制实验，训练解码器型Transformer模型，比较训练于最优动态规划路径与更长且含回溯的有效路径的效果。

Result: 在相同训练资源下，训练于较长、系统性且局部递增的推理路径的模型，在未见图结构上的泛化能力更强。简单增加冗余路径不能提升效果。

Conclusion: 有效的训练推理路径应保证长度、连贯性和递增性，这有助于提升模型的信心和优化效率，从而促进泛化能力。

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [171] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

TL;DR: EduCoder是一款专门用于教育对话逐句标注的工具，支持复杂教学特征的编码及多标注者比较，提升数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有通用文本标注工具难以满足教育对话中多样师生互动及复杂教学特征的标注需求。

Method: EduCoder支持复杂代码本定义、分类与开放式标注、上下文材料支持以及多标注者并列比较，便于研究者和领域专家协作标注。

Result: 实现了一个开源平台，能够提高教育对话标注的准确性和一致性，支持协同标注和数据校准。

Conclusion: EduCoder有效解决了教育对话标注中的复杂挑战，促进了教育领域对话数据的高质量标注和研究。

Abstract: We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [172] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

TL;DR: 本文提出InfoRidge框架，揭示Transformer模型中间层在信息流动和泛化中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在自然语言生成任务中表现优异，但其层间的任务相关信息传播机制尚不清楚。

Method: 通过信息论方法估计隐藏表示与目标输出间的互信息，并引入可训练的残差缩放系数，分析信息在各层的变化及权重。

Result: 发现预测信息在中上层形成峰值，表现非单调趋势，且分布迁移时模型更依赖中间层，显示其泛化能力。

Conclusion: 中间层在Transformer中起关键的泛化作用，揭示其重要性，有助于理解模型内部机制及提升模型鲁棒性。

Abstract: Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [173] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: 本文提出通过隐私配置文件控制大语言模型查询隐私，使用本地模型改写查询以保护敏感信息，在公开模型和用户隐私间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过商业API访问时存在用户数据暴露风险，用户希望对数据隐私有更多控制。

Method: 设计本地模型利用自然语言隐私配置文件改写用户查询，仅隐藏用户定义的敏感信息，然后发送给外部模型；构建多语言PEEP数据集，包含带隐私标注的真实查询和合成隐私配置文件。

Result: 轻量级大语言模型能够部分遵守隐私指令，但仍面临挑战，表明需要更好理解和执行用户隐私偏好的模型。

Conclusion: 采用隐私配置文件进行查询改写是平衡隐私和性能的有效途径，但现有模型能力有限，未来需改进隐私指令的理解与执行能力。

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [174] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: 本文提出GeoFact-X多语言地理事实推理基准和BRIDGE训练方法，提高大语言模型多语言推理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在低资源语言中的推理能力较弱，存在偏向高资源语言的问题，影响准确性和可解释性。

Method: 设计包含五种语言注释推理轨迹的GeoFact-X基准，提出BRIDGE训练方法结合有监督微调和带语言一致性奖励的强化学习，并开发自动评测协议。

Result: BRIDGE显著提升了多语言推理的语言一致性和推理质量，实现更强的跨语言泛化能力。

Conclusion: 基于推理意识的多语言强化学习对增强大语言模型跨语言推理鲁棒性至关重要。

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [175] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出了CoPE框架，系统衡量大型语言模型在多语言环境中对上下文知识和参数知识的利用，发现模型存在对后文信息忽视的偏差。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何优先及整合上下文知识与参数知识，填补该领域研究空白。

Method: 构建MultiWikiAtomic多语言数据集，通过CoPE框架评估模型在开放式问答中的知识整合和信息利用，分析模型的位置信息偏差及推理能力的作用。

Result: 发现模型存在“lost-in-the-later”现象，推理模型及使用链式思考提示的模型反而更少利用上下文，且CoT提示降低信息召回率和回答长度。设计基于提示的方法提升上下文利用效率。

Conclusion: CK知识引导的提示方法可增强模型的事实性基础，减少幻觉现象，表明合理利用上下文知识对提升模型效果有显著帮助。

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [176] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

TL;DR: 美国最高法院在Dobbs案件中的裁决引发了围绕堕胎权利的分歧，研究通过分析约1千万条与堕胎相关的X平台帖子，揭示了性别和地区政治环境如何交互影响公众言论。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨性别和地方社会政治背景如何共同影响公众对堕胎的态度及情绪表达，填补性别和地区互动影响方面的研究空白。

Method: 基于X平台近一千万条与堕胎相关的帖子，结合用户推测的性别、意识形态和地理位置，对其言论态度及情绪表达进行分析。

Result: 发现性别显著调节堕胎态度和情绪表达，尤其在保守地区更为显著，形成了加剧的性别差距；Dobbs草案的泄露加剧了网络上的参与度，特别是激发了保守地区支持堕胎的女性的动员。

Conclusion: 堕胎话语不仅意识形态对立明显，还受到性别和地域因素深刻影响，强调身份认同在制度变革时期政治表达中的核心作用。

Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [177] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: 本文提出了一种名为PhoniTale的跨语言助记词生成系统，通过基于语音相似性的L1关键词检索结合大语言模型生成助记词，有效帮助第二语言学习者词汇习得。


<details>
  <summary>Details</summary>
Motivation: 词汇习得对学习 typologically不同语言的第二语言学习者具有挑战性，尤其是英语和韩语之间存在语音和结构差异，影响词汇学习效果。现有研究多集中在以英语为母语学习其他语言的情形，缺乏反向研究。

Method: 设计了PhoniTale系统，通过检索与目标单词语音相似的母语（L1）关键词序列，并利用大语言模型生成助记词。系统输出通过自动指标和人工评估与真人及现有自动化方法生成的助记词进行比较。

Result: 实验和短期记忆测试显示，PhoniTale生成的助记词在质量上与人类原创助记词相当，证明了其有效性。

Conclusion: PhoniTale在帮助跨语言词汇习得方面表现出色，但仍需在助记词质量和生成方法上进一步改进。研究为跨语言助记策略提供了新思路。

Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [178] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: 本文探讨大语言模型（LLMs）如ChatGPT在词汇和句子层面的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前对于LLMs是否真正理解语言存在争议，本文意在通过细化至语义层面来探究这一问题。

Method: 通过分析LLMs的内部机制及其语言表示，并结合Frege和Russell的经典语义理论，进行深入研究。

Result: 获得了LLMs在语义能力方面更细致的理解，展示了其潜在的语义处理能力。

Conclusion: LLMs在语义理解上具有一定潜力，但仍需更精准的评估和理论支持，才能全面评价其语义能力。

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [179] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: 本论文提出了MODELCITIZENS数据集，包含多元身份群体的毒性语言标注，并引入对话情境以提升毒性检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有毒性语言检测模型忽视了不同社区规范和语境对毒性的多样化认知，且通常将多样注释合并为单一真值，导致重要的情境特异性毒性概念被忽略。

Method: 构建包含6.8K社交媒体帖子及4万条标注的MODELCITIZENS数据集，涵盖多身份群体视角，利用大语言模型生成对话情境以增强数据，基于该数据集微调LLaMA和Gemma模型，提升检测效果。

Result: 现有主流毒性检测工具如OpenAI Moderation API和GPT-4-mini在MODELCITIZENS数据集及其对话情境增强版上表现较差，而微调后的LLAMACITIZEN-8B和GEMMACITIZEN-12B模型在样本内评测中性能提升了5.5%。

Conclusion: 社区视角驱动的注释和建模对于构建包容性的内容审核系统至关重要，能够有效提升毒性语言检测的准确性和适应性。

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [180] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文针对护士口述报告的结构化表格生成和医生-患者会诊中的医疗指令提取这两个临床NLP高难任务，评估了多种大型语言模型的表现，并提出了生成真实非敏感护士口述的自主管道，同时发布了首个公开的数据集以支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 这两个任务由于数据稀缺和敏感性，研究较少，但其解决方案能够显著减轻医疗工作者的文档负担，提高患者护理质量。

Method: 利用私有和开源临床数据集，评估开放权重和闭源大型语言模型在这两任务中的表现，分析优缺点；提出基于智能代理的生成管道，以生成真实且非敏感的护士口述，实现临床观察的结构化提取。

Result: 评测展示了不同LLM在这两任务中的性能与局限性；成功开发了生成管道用于生成非敏感护士口述；同时发布了首个护士观测提取和医疗指令提取的开放数据集SYNUR和SIMORD。

Conclusion: 通过多模型评估和新技术方法，本研究推动了临床NLP中两个未充分探索的重要任务的研究，为实际医疗应用减负提供了重要资源与技术基础。

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [181] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

TL;DR: 本文提出了R2-LLMs，一种无需蒸馏链式思维训练数据的新型分层检索增强推理框架，通过双层检索结合蒙特卡洛树搜索提升大语言模型的推理性能，在多个复杂推理数据集上效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在测试时扩展推理能力依赖复杂的链式思维训练数据，且性能提升受限，急需一种无须蒸馏更高级模型数据且能增强推理能力的新方法。

Method: R2-LLMs采用双层检索策略：粗层级提取抽象模板并检索相似题目与答案促进高级上下文学习，细层级在蒙特卡洛树搜索中检索相似中间求解步骤，辅以过程奖励模型优化候选生成与决策。

Result: 在MATH500、GSM8K及OlympiadBench-TO数据集上，基于LLaMA-3.1-8B模型，R2-LLMs相比基线方法最高提升16%的准确率，证明其在复杂推理任务中的有效性。

Conclusion: R2-LLMs为提升大语言模型测试时推理能力提供了有效且灵活的分层检索增强方案，显著改善了上下文推理及步进决策过程，推动了复杂推理任务的性能提升。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [182] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: 本文提出了Re5，一种自我评估与修订框架，旨在提升大语言模型的指令遵循能力，同时保持生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用强大模型生成高质量数据，但复杂指令下生成效果有限，且多次修订成本高昂；利用开源模型自评能力有限，过度修订又影响质量。

Method: Re5通过提取任务及约束，进行结构化评估防止错误累积，再基于细粒度约束做内容评估和选择性修订，最后用高质量输出进行调优，实现高效的迭代优化。

Result: 实验显示，Re5在少量数据下，指令遵循表现可与基于GPT-4o-mini生成数据训练的模型媲美，同时保持回答质量，修订后回答胜率达64.24%。

Conclusion: Re5是一种资源高效且有效的方案，能在最小外部监督下显著提升大语言模型的指令遵循能力及输出质量。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [183] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一种倒置知识蒸馏范式，即让大型语言模型从小型模型学习，通过LoRA实现编码器-解码器架构，采用边缘感知对比学习提升文本匹配任务的性能，在金融和医疗领域验证有效并已在线部署。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏是由大型模型向小型模型迁移知识，但在文本匹配任务中，经过微调的小型模型能够更好地捕捉领域特定的语义信息。为了结合两者优势，提出让大型模型向小型模型学习的新方法。

Method: 采用LoRA将解码器结构的大型模型转化为编码器-解码器架构，由编码器生成压缩表示，解码器映射输出。训练时通过编码器产生的表示及其相似度，与教师模型的小型模型输出相似度进行对齐，设计了边缘感知对比学习（MCL）方法，准确处理正负样本的相似度信息。

Result: 新范式使大型模型能够利用表现良好的小型模型知识提升性能。通过金融和医疗领域的多个基准测试及实际应用验证了该方法的有效性，模型已成功部署上线。

Conclusion: 倒置知识蒸馏结合了小型模型的领域优势和大型模型的深层语义理解，有效提升文本匹配等任务性能，具有良好的实用价值和推广前景。

Abstract: Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [184] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: SARA框架通过结合自然语言片段和语义压缩向量，在有限上下文预算下提升检索增强生成模型的答案准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成模型面临上下文长度限制和冗余信息问题，纯压缩方法又会丢失重要细节，影响事实准确性。

Method: SARA采用双层表示策略，将细粒度文本与压缩向量结合，通过迭代证据选择模块动态重排序上下文，提升信息利用效率。

Result: 在9个数据集和5个开源大模型上，SARA显著提高答案相关性（+17.71）、答案正确性（+13.72）和语义相似度（+15.53）。

Conclusion: 结合文本细节和压缩表达的融合方法能够实现更稳健、更高效的检索增强生成。

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [185] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文提出了ECom-Bench，一个用于评估具有多模态能力的电商客服大语言模型代理的基准框架。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对电商客服场景中多模态大语言模型代理的评测基准，难以衡量其实际应用效果。

Method: 构建基于真实电商客服交互的用户画像动态模拟和真实电商对话任务数据集，涵盖多种复杂业务场景，设计具有高度挑战性的任务。

Result: 在该基准上，即使是GPT-4o等先进模型的通过率仅为10-20%，说明复杂电商场景的巨大挑战。

Conclusion: ECom-Bench为多模态电商客服大语言模型的评测提供了首个实际且高度挑战性的基准，对推动该领域研究具有重要价值。代码和数据将开源以促进后续研究。

Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [186] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出了Smoothie-Qwen，一种轻量级后处理方法，通过调整token输出概率减少多语言大模型的语言混淆问题，显著降低了不期望语言输出，提高模型语言可控性。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型在生成响应时容易出现语言混淆，即无视提示语言而生成支配语言，影响模型的多语言适用性和准确性。

Method: Smoothie-Qwen方法通过选择性调整token级别的输出概率，抑制不期望的语言生成，无需重新训练模型，简单高效。

Result: 应用于Qwen模型后，不期望的中文输出降低了95%以上，同时多语言基准任务的准确性得以保持。

Conclusion: Smoothie-Qwen为提升多语言大模型的语言可控性提供了实用、高效的解决方案，增强了模型在全球应用中的可靠性。

Abstract: Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [187] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: 本文提出了DualDistill微调框架，通过多教师模型蒸馏互补推理策略，训练出了Agentic-R1模型，能够针对不同查询动态选择推理策略，提高数学和逻辑问题的解题准确率。


<details>
  <summary>Details</summary>
Motivation: 现有长链推理模型虽然在数学推理方面表现优异，但依赖缓慢且易出错的自然语言线索。工具增强型代理虽然能通过代码执行解决算术问题，但在复杂逻辑任务上表现不佳。

Method: 提出DualDistill微调框架，将多位教师模型的互补推理策略蒸馏到一个统一的学生模型中。通过训练Agentic-R1模型，实现根据查询动态选取最优推理策略，算术和算法问题调用工具，抽象问题采用基于文本的推理。

Result: 该方法提升了多个任务的准确率，涵盖计算密集型和标准基准测试，验证了多策略蒸馏在实现稳健高效推理中的有效性。

Conclusion: 多策略蒸馏通过整合不同推理方法，可显著提高模型解决数学与逻辑复杂问题的能力，使推理更高效和准确。

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [188] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文提出了DRAGON，一个针对俄语的动态Retrieval-Augmented Generation（RAG）基准，基于不断更新的新闻语料库，支持全面评估检索和生成模块，且自动生成多类型问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准多为英文，其他语言如俄语的资源稀缺且静态，无法反映真实环境中的动态变化需求。

Method: 构建动态更新的俄语新闻及公开文档语料库，基于知识图谱自动生成与子图模式对应的四类核心问题，提供自动问题生成及评测脚本。

Result: 发布了完整评测框架和数据，支持检索器与生成器的综合评测，适用于多语言设置，并创建了公开排行榜促进社区交流。

Conclusion: DRAGON填补了俄语RAG动态评测资源的空白，有助于推动多语言RAG系统的实用性和准确性提升。

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [189] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种针对检索增强生成模型（RAG）的层次思维指令微调方法（HIRAG），通过多级渐进链式思维提升模型处理复杂知识检索和推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型在面对实时信息和领域问题时，存在文档质量不一和检索系统缺陷，且缺乏对模型具体能力的深入研究及链式思维的充分利用。

Method: 提出RAG模型应具备过滤、组合和特定推理三层次能力，引入多级渐进链式思维的"先思考再回答"策略，设计HIRAG指令微调方法以提升模型能力。

Result: HIRAG策略显著提升了模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的表现。

Conclusion: 层次思维指令微调有效增强了RAG模型的开放书本考试能力，改善了信息选择、语义合成和基于内外知识的推理能力。

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [190] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: 本文提出了Omni-router Transformer，通过在不同MoE层共享路由器，提升了语音识别模型中专家间的协作和专业化，实现了更低的训练损失和更优的识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法中各层路由器独立选择专家，导致不同层间专家选择相关性弱，限制了专家间的协作和专业化。

Method: 设计了一个跨层共享路由器的Omni-router Transformer模型，增强了不同MoE层间专家的合作。

Result: 在大规模伪标签数据集和10个多样化领域外语音识别测试中，Omni-router Transformer较稠密模型和Switch Transformer分别降低了11.2%和8.2%的词错误率，训练损失更低且专家使用更有结构性。

Conclusion: 通过共享路由器机制，Omni-router Transformer实现了专家间更好合作和专业化，提升了自动语音识别的性能和鲁棒性。

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [191] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
*Yujia Hu,Tuan-Phong Nguyen,Shrestha Ghosh,Moritz Müller,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文介绍了GPTKB v1.5，一个由GPT-4.1构建的包含1亿三元组的知识库，实现对大规模语言模型知识的系统化材料化和分析。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的事实知识尚不清楚，且难以进行即席浏览和大规模统计分析，因此需要一个高效的知识库来系统地展现和分析LLM知识。

Method: 利用GPTKB方法和大量递归技术，从GPT-4.1生成1亿条三元组，构建一个高度互联的知识库。

Result: 实现了三大应用场景：基于链接遍历的知识探索、基于SPARQL的结构化查询、以及对LLM知识优劣对比的探索，证明了知识材料化的有效性。

Conclusion: 大规模递归语言模型知识材料化为LLM知识的系统性分析和自动知识库构建提供了突破性机遇，GPTKB工具已上线开放访问。

Abstract: Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

</details>


### [192] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
*Jing Yang Lee,Hamed Bonab,Nasser Zalmout,Ming Zeng,Sanket Lokegaonkar,Colin Lockard,Binxuan Huang,Ritesh Sarkhel,Haodong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种通过将多篇相关文档合成为多轮多主题对话数据的预训练方法，以提升大语言模型在多轮会话任务中的上下文记忆与理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的预训练数据多为连贯文本，缺乏多轮对话结构，导致模型在多轮会话中的表现受限。

Method: 设计一个管道，将多个相关文档聚合转化为信息检索式的多轮多主题对话，基于维基百科文章构建了包含73万轮对话的DocTalk数据集，用于预训练。

Result: 利用DocTalk进行预训练，可在不影响基础性能的前提下，在上下文记忆和理解能力上提升多达40%。

Conclusion: 合成的多轮对话预训练数据有助于增强大语言模型的多轮会话能力，DocTalk数据集公开可用。

Abstract: Large Language Models (LLMs) are increasingly employed in multi-turn
conversational tasks, yet their pre-training data predominantly consists of
continuous prose, creating a potential mismatch between required capabilities
and training paradigms. We introduce a novel approach to address this
discrepancy by synthesizing conversational data from existing text corpora. We
present a pipeline that transforms a cluster of multiple related documents into
an extended multi-turn, multi-topic information-seeking dialogue. Applying our
pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training
dialogue corpus consisting of over 730k long conversations. We hypothesize that
exposure to such synthesized conversational structures during pre-training can
enhance the fundamental multi-turn capabilities of LLMs, such as context memory
and understanding. Empirically, we show that incorporating DocTalk during
pre-training results in up to 40% gain in context memory and understanding,
without compromising base performance. DocTalk is available at
https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [193] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
*Anand A. Rajasekar,Praveen Tangarajan,Anjali Nainani,Amogh Batwal,Vinay Rao Dandin,Anusua Trivedi,Ozan Ersoy*

Main category: cs.CL

TL;DR: Flippi是一个基于大语言模型的端到端电商对话助手，通过自然语言交互提升用户产品发现和购物体验。


<details>
  <summary>Details</summary>
Motivation: 面对庞大复杂的电商产品，用户难以高效找到合适商品，需个性化、智能的购物助手。

Method: 利用查询重构、意图识别、检索增强生成、命名实体识别和上下文缩减等先进NLP技术，实现精准商品信息获取和个性化推荐。

Result: Flippi能识别并展示最优优惠，支持产品特征和价格的对比分析，提高用户决策效率和满意度。

Conclusion: Flippi优化了电商购物体验，提升用户参与度和转化率，具备跨平台集成能力，代表数字市场客户服务新标准。

Abstract: The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

</details>


### [194] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
*Amane Watahiki,Tomoki Doi,Taiga Shinozaki,Satoshi Nishida,Takuya Niikawa,Katsunori Miyahara,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文构建了一个基于基本形式本体的多模态完成基准，评估大型视觉-语言模型（LVLMs）对无形完成文本推理能力，发现不同模型在某些物体类型上的表现差异显著，且在日语提示下部分模型表现异常。


<details>
  <summary>Details</summary>
Motivation: 旨在填补LVLMs在无形完成（amodal completion）文本推理能力方面的研究空白，评估其是否能够理解和推断被遮挡物体的信息。

Method: 构建一个基于基本形式本体的无形完成系统分类基准，使用该基准系统评估多个LVLM模型在处理相关文本和视觉信息的表现。

Result: 发现多数LVLM整体表现可与人类相媲美，但在某些物体类型的无形完成任务中表现不一。尤其是LLaVA-NeXT部分变体和Claude 3.5 Sonnet在日语提示下对原始图像的准确度低于空白无视觉内容的刺激。

Conclusion: LVLM具备一定的无形完成推理能力，但存在语言特异性缺陷，特别是在日语处理能力方面表现不足，提示模型多语言能力亟需加强。

Abstract: One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.

</details>


### [195] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
*Tanvina Patel,Wiebke Hutiri,Aaron Yi Ding,Odette Scharenborg*

Main category: cs.CL

TL;DR: 本文研究了自动语音识别（ASR）系统中存在的针对不同说话人群体（如性别、年龄、口音）的偏见问题，比较了不同性能和偏见测量方法，提出了评估和缓解策略，并给出更全面的性能报告建议。


<details>
  <summary>Details</summary>
Motivation: ASR系统存在针对不同说话人群体的偏见，现有研究主要集中在检测和减缓偏见，但如何有效衡量系统性能和偏见仍是未解问题。

Method: 本文比较了多种现有及新提出的性能和偏见衡量指标，利用多种偏见缓解策略，评估荷兰语端到端ASR系统。

Result: 实验结果表明，传统的平均错误率指标不足以全面反映系统性能，需辅以其他偏见衡量指标。

Conclusion: 建议在报告ASR系统性能和偏见时，应结合多种指标，以更全面体现系统在不同说话人群体上的表现和整体偏见水平。

Abstract: There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

</details>


### [196] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一个利用大型语言模型（LLMs）模拟虚拟受访者，以生成和验证心理测量调查题目有效性的框架。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLMs进行心理测量调查时，确保生成题目真正测量预期特质（构念效度）需大量昂贵的人类数据采集，效率低下。

Method: 通过模拟具有不同中介变量的虚拟受访者，利用LLMs从特质定义生成合理的中介变量，并模拟受访者行为，识别稳健测量预期特质的调查题目。

Result: 在Big5、Schwartz和VIA三种心理特质量表上的实验表明，该框架和中介变量生成方法能有效鉴别高效度题目。LLMs展现了生成合适中介变量及模拟受访者行为能力。

Conclusion: 该方法为成本效益高的测验题目开发和理解LLMs模拟人类行为提供了新方向，并将公开数据集和代码支持后续研究。

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [197] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
*Teodor-George Marchitan,Claudiu Creanga,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 本文介绍了Unibuc - NLP团队在2025年SemEval任务11中使用大型语言模型进行文本情感检测的实验，最终在多标签情感检测中取得不同语言子集的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决文本情感检测中的跨语言难题，提升不同语言子集上的情感检测效果。

Method: 采用大型语言模型（Gemini, Qwen, DeepSeek），结合少样本提示和微调方法进行多标签情感检测。

Result: 在多标签情感检测轨道A中，英文子集F1-macro为0.7546（排名26/96），葡萄牙语子集为0.1727（排名35/36），Emakhuwa语子集为0.325（排名第一）。

Conclusion: 通过大型语言模型及相应训练策略，团队在复杂多语言文本情感检测任务中取得了阶段性成果，特别是在低资源语言Emakhuwa上表现突出。

Abstract: This paper describes the approach of the Unibuc - NLP team in tackling the
SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion
Detection. We mainly focused on experiments using large language models
(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With
our final system, for the multi-label emotion detection track (track A), we got
an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36
teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams)
for the Emakhuwa subset.

</details>


### [198] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
*Sebastian Pohl,Max Ploner,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了知识编辑模型评估方法的鲁棒性和公平性，发现不同评估指标和方法会导致模型排名变化，并指出字符串匹配评估存在误判问题。


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑领域缺乏对评估方法的鲁棒性研究，且现有评估方法可能对部分编辑器存在偏向，同时对编辑对模型整体能力影响了解不足。

Method: 通过比较不同评估指标、方法及编辑批量大小对知识编辑器排名的影响，结合手工评估字符串匹配方法的准确性，分析评估方法的偏差。

Result: 发现不同评估策略会显著影响知识编辑器排名，同时字符串匹配评估方法易产生误判，且编辑对通用语言理解任务的影响也被揭示。

Conclusion: 评价方法选择和参数设定对模型编辑结果排名有显著影响，现有字符串匹配评估存在误判问题，未来需要设计更鲁棒和公平的评估体系。

Abstract: Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

</details>


### [199] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的持续多模态错误信息检测方法DAEDCMD，以应对在线数据流中持续涌现的新事件，解决遗忘旧知识和适应未来环境变化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态错误信息检测方法依赖离线数据训练，无法适应不断出现的新事件，导致模型过时且效果下降。

Method: 通过采用基于狄利克雷过程的专家混合结构隔离事件特定参数，防止旧知识遗忘；并利用连续时间动力学模型预测环境分布变化，提高对未来数据的泛化能力。

Result: 实验表明，DAEDCMD在持续检测多模态错误信息任务中，显著优于包括六个基准MMD方法和三个持续学习方法在内的对比方法。

Conclusion: DAEDCMD有效解决了持续多模态错误信息检测中旧知识遗忘和环境变化适应问题，提升了模型的持续学习能力和泛化性能。

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


### [200] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
*Sandeep Mishra,Anubhab Mandal,Bishal Santra,Tushar Abhishek,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文研究了聊天场景下的文本自动补全（Ghosting）问题，评估了多种方法的性能，并提出了动态早停策略。


<details>
  <summary>Details</summary>
Motivation: 随着聊天系统（如ChatGPT）的普及，预测用户意图输入（Ghosting）变得愈发重要，但该问题尚缺乏统一基准与性能分析。

Method: 利用四个公开对话数据集，比较了trie、n-gram统计模型与包括T5、Phi-2在内的深度学习模型的效果，同时提出基于熵的动态早停策略。

Result: n-gram模型和trie在已见前缀上表现和推理效率优于深度模型，神经网络模型在未见查询上表现更佳，加入对话上下文显著提升性能。

Conclusion: 统计方法与深度学习方法各有优势，结合对话上下文可提升自动补全质量，研究代码和数据公开促进未来工作。

Abstract: Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

</details>


### [201] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文介绍了开源实现OpenFActScore框架，用于评估大型语言模型生成文本的真实性，通过事实原子生成和验证实现，支持任意兼容Hugging Face的模型，性能接近闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有FActScore依赖闭源商业模型，缺乏开放性和可重复性，需构建开源实现以支持广泛模型并提高透明性和成本效益。

Method: 设计并实现OpenFActScore，使之支持任意Hugging Face模型进行事实原子生成（AFG）和验证（AFV），改进设计并在标准基准上进行性能评测。

Result: 多款开源模型在AFG和AFV任务中表现良好，Gemma模型表现最佳，整体结果与原始FActScore实验达0.99的皮尔逊相关系数。

Conclusion: OpenFActScore成功实现了基于开源模型的文本真实性评估框架，具有高度透明性和可重复性，为研究和实际应用提供了成本效益高的评估工具。

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [202] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
*Ehud Reiter*

Main category: cs.CL

TL;DR: ACL领域的论文中极少有关注NLP系统实际影响的评估，主要集中于指标评估，缺乏对真实世界影响的深入理解。


<details>
  <summary>Details</summary>
Motivation: NLP技术的实际应用和推广需要更深入、系统的真实影响评估，但目前ACL社区对此关注甚少。

Method: 通过对ACL论文集的系统性调查，统计分析涉及实际影响评估的论文数量及质量。

Result: 发现约0.1%的论文包含真实影响评估，且大多数仅进行简单的评估，主要关注指标评估。

Conclusion: 建议NLP领域应更多关注技术的真实世界影响评估，以促进技术的实际应用和推广。

Abstract: The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [203] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文介绍了针对新加坡多语言环境的安全评估基准RabakBench，涵盖新加坡土语、中文、马来语和泰米尔语，构建了5000多个多语言安全标注样本。实验结果显示现有安全分类器在该基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和安全分类器在资源匮乏语言上的表现受限，缺少针对新加坡独特语言环境的安全评估数据集。

Method: 构建了一个三阶段可扩展流程：生成（通过大语言模型对新加坡土语网络内容进行对抗样本生成），标注（使用多数投票的半自动多标签安全注释），翻译（保持语言细节与毒性特征的高保真翻译跨多语言）。

Result: 最终数据集包含5000多个包含严重等级的数据显示安全类别样本，涵盖4种语言和6个细化安全类别。对11种热门开源和封闭源分类器的评测显示性能显著下降。

Conclusion: RabakBench为东南亚多语言安全评估提供了实证基础，并提出了可复制的构建本土化安全数据集的框架，数据集及评测工具已公开。

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [204] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
*Minghang Zhu,Shen Gao,Zhengliang Shi,Jiabao Fang,Pengjie Ren,Zhaochun Ren,Zhumin Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 提出一种语言模型自我进化方法，通过多层原则生成和基于原则的实例生成，有效扩展数据集，提升小型模型性能并减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有利用大规模语言模型扩展数据集的方法面临高碳排放和数据泄露风险。

Method: 先用大规模模型总结任务完成原则，再用小型模型基于这些原则生成大量训练数据进行模型训练。

Result: 方法显著提升了小型模型性能，同时大幅降低了训练过程中的碳排放。

Conclusion: 利用原则指导的数据生成策略不仅提升了模型表现，还解决了传统方法在碳排放和数据安全上的问题。

Abstract: A common training approach for language models involves using a large-scale
language model to expand a human-provided dataset, which is subsequently used
for model training.This method significantly reduces training costs by
eliminating the need for extensive human data annotation. However, it still
faces challenges such as high carbon emissions during data augmentation and the
risk of data leakage when we use closed-source LLMs. To address these issues,
we propose a self-evolution method for language models. First, we introduce the
Multi-level Principle Generation, which enables a large-scale model to
summarize task-completion principles based on a small amount of task data.
Then, we propose the Principle-based Instance Generation, in which a
smaller-scale language model uses these task principles to generate a large
amount of data. This data is then used for model training. Experimental results
show that our proposed method significantly improves model performance compared
to directly using a smaller-scale language model to generate data.
Additionally, since we only use the large-scale language model to generate the
task-completion principles, the carbon emissions associated with training the
model are greatly reduced.

</details>


### [205] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
*Nicholas Popovič,Ashish Kangen,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大语言模型的自动合成数据生成和上下文学习管道，用于文档级实体和关系抽取，解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前文档级实体和关系抽取在零样本或少样本设置下缺乏高质量的标注语料。

Method: 提出结合合成数据生成和基于检索的上下文学习，利用推理优化的大语言模型动态检索示例，避免人工标注。

Result: 生成了包含约5千个维基百科摘要、5.9万个实体和3万个关系三元组的合成数据集；在DocIE共享任务中测试了零样本上下文学习性能。

Conclusion: 即使对于最先进的大语言模型，文档级联合实体和关系抽取在零样本设置下依然具有挑战性。

Abstract: Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

</details>


### [206] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
*Youmna Farag,Svetlana Stoyanchev,Mohan Li,Simon Keizer,Rama Doddipatla*

Main category: cs.CL

TL;DR: 本文提出了一种基于零样本链式提示的多阶段条件故障恢复框架，显著提升了实体代理执行复杂任务时的故障恢复能力。


<details>
  <summary>Details</summary>
Motivation: 实体代理在执行复杂任务时易遭遇执行失败，亟需高效的故障恢复机制。

Method: 设计了包含四个错误处理阶段的多阶段恢复框架，利用大型语言模型的推理能力在任务执行和事后反思阶段分析执行环境中的挑战并制定解决策略。

Result: 在TEACH数据集的TfD基准测试中，该方法比无错误恢复基线提高11.5%，超越现有最强模型19%。

Conclusion: 提出的多阶段故障恢复框架有效增强了实体代理的执行鲁棒性，提升了复杂任务的完成率。

Abstract: Embodied agents performing complex tasks are susceptible to execution
failures, motivating the need for effective failure recovery mechanisms. In
this work, we introduce a conditional multistage failure recovery framework
that employs zero-shot chain prompting. The framework is structured into four
error-handling stages, with three operating during task execution and one
functioning as a post-execution reflection phase. Our approach utilises the
reasoning capabilities of LLMs to analyse execution challenges within their
environmental context and devise strategic solutions. We evaluate our method on
the TfD benchmark of the TEACH dataset and achieve state-of-the-art
performance, outperforming a baseline without error recovery by 11.5% and
surpassing the strongest existing model by 19%.

</details>


### [207] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: 本文研究大语言模型中训练数据的记忆难度，提出了熵-记忆法则，发现数据熵与记忆分数线性相关。


<details>
  <summary>Details</summary>
Motivation: 探讨如何刻画大语言模型中训练数据的记忆难度，这是一个基础但尚未充分研究的问题。

Method: 通过对OLMo模型系列的实验，提出并验证了熵-记忆法则；进一步分析了高随机性字符串的经验熵，采用该法则区分训练数据与测试数据，实现数据集推断。

Result: 发现数据熵与记忆分数呈线性关系；高随机性的“无意义字符串”表现出相对较低的经验熵；利用该现象成功区分训练和测试数据，实现数据集推断。

Conclusion: 熵-记忆法则为理解LLM中数据记忆机制提供了新视角，并可应用于训练和测试数据的区分，具备实际应用价值。

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [208] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: 本文综述了提示调优技术，通过添加可训练的连续向量且保持模型不变，实现对语言模型的高效适配。


<details>
  <summary>Details</summary>
Motivation: 为提高语言模型适配的参数效率，减少全部模型微调的成本与复杂度。

Method: 将提示调优方法分为直接提示学习和迁移学习两类，进一步细分多种具体技术，并详细分析设计、创新、优缺点。

Result: 归纳和比较了不同提示调优框架，指出了计算效率和训练稳定性方面的挑战。

Conclusion: 强调需提升训练鲁棒性和扩展提示调优应用范围，指明未来研究方向。

Abstract: This survey reviews prompt tuning, a parameter-efficient approach for
adapting language models by prepending trainable continuous vectors while
keeping the model frozen. We classify existing approaches into two categories:
direct prompt learning and transfer learning. Direct prompt learning methods
include: general optimization approaches, encoder-based methods, decomposition
strategies, and mixture-of-experts frameworks. Transfer learning methods
consist of: general transfer approaches, encoder-based methods, and
decomposition strategies. For each method, we analyze method designs,
innovations, insights, advantages, and disadvantages, with illustrative
visualizations comparing different frameworks. We identify challenges in
computational efficiency and training stability, and discuss future directions
in improving training robustness and broadening application scope.

</details>


### [209] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: NeoBabel是一个支持六种语言的多语言文本生成图像框架，提升了生成质量和效率，并推广了多语言评测标准。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像方法主要面向英文，存在语义漂移、计算负担和文化不匹配问题，限制了非英语用户的使用体验和数字公平。

Method: NeoBabel结合大规模多语言预训练与高分辨率指令微调训练，支持英语、中文、荷兰语、法语、印地语和波斯语六种语言，并扩展相关评测基准为多语言版本。

Result: NeoBabel在多语言基准测试中超过了现有多语言基模型，在保持英文能力的同时，多语言性能分别提升0.11和0.09，模型体积却是英文模型的1/2到1/4。

Conclusion: 多语言能力不仅不是性能折中，而是提升生成模型鲁棒性、效率和文化贴合度的关键，NeoBabel为促进包容性AI研究提供了完整工具和数据集。

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [210] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了Code Triangle框架，从编辑分析、代码实现和测试用例生成三个维度系统评估大规模语言模型（LLMs）的编程能力，并发现模型在多样性和鲁棒性上仍不及人类。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型虽然在代码生成上取得了显著进展，但其真实编程能力和表现尚未被系统全面评估，特别是在多维度能力和与人类专家的差异方面。

Method: 构建Code Triangle框架，从编辑分析、代码实现和测试用例生成三方面评估LLMs，进行大规模竞赛编程基准测试，并分析模型认知与人类技能的分布差异。

Result: 发现LLMs虽然能在三维度间构建自洽系统，但其解法缺乏人类的多样性和鲁棒性，错误多因训练数据偏差和推理迁移能力有限。加入人类编辑、解法和多样化测试用例及模型混合能显著提升性能和鲁棒性。

Conclusion: 研究揭示了LLMs认知的一致性与不一致性，这为模型的自我反思和自我提升提供了潜在方向，有助于开发更强大的编程模型。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [211] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
*Wei Shen,Jiangbo Pei,Yi Peng,Xuchen Song,Yang Liu,Jian Peng,Haofeng Sun,Yunzhuo Hao,Peiyu Wang,Yahui Zhou*

Main category: cs.CL

TL;DR: Skywork-R1V3是一种先进的开源视觉语言模型，通过后训练的强化学习框架，有效提升视觉推理能力，实现了领先的多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 将文本大语言模型的推理能力迁移到视觉任务中，提升视觉语言模型的推理表现。

Method: 采用精心设计的后训练强化学习框架，激活并增强模型推理能力，利用连接模块实现跨模态对齐，并引入关键推理token的熵作为训练过程中模型优选指标。

Result: Skywork-R1V3在MMMU任务上表现达76.0%，显著超过之前的64.3%，达到接近入门级人类水平。大规模模型经强化学习后可媲美顶级闭源视觉语言模型。

Conclusion: 通过强化学习的后训练策略，Skywork-R1V3实现了多模态推理能力的重大突破，为开源视觉语言模型的发展提供了强大动力。

Abstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.

</details>


### [212] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
*Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge Zhang*

Main category: cs.CL

TL;DR: 本文提出了CriticLean框架，通过强化学习提升对数学命题自然语言转形式代码的语义评估能力，构建了CriticLeanGPT评估模型和CriticLeanBench基准测试，并发布了含28.5万问题的FineLeanCorpus数据集。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注生成和编译成功率，忽视了生成的形式化表达是否真正反映了原问题的语义意图，评估阶段较为薄弱。

Method: 提出CriticLean框架，将评价者从被动验证者转变为主动学习组件；训练CriticLeanGPT模型进行语义忠实度评估；设计CriticLeanBench用于测评模型识别语义正误的能力。

Result: CriticLeanGPT显著优于现有强基线模型；FineLeanCorpus数据集具有领域丰富、难度广泛、高正确率特点，促进模型训练和评估。

Conclusion: 强化评价阶段对高质量形式化表达至关重要，CriticLean为未来形式数学推理研究提供了重要方法和资源。

Abstract: Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

</details>


### [213] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: 这篇论文研究了利用迁移学习和风格化数据增强提升英语新闻文本中主观性检测的效果，提出了基于GPT-4o的风格化增强方法，并取得了较好的结果。


<details>
  <summary>Details</summary>
Motivation: 提升英语新闻文本中主观和客观句子的分类效果，弥补现有方法在主观性检测上的不足。

Method: 对比了预训练编码器的微调与在相关任务上微调后转移学习的方法，引入GPT-4o进行受控的风格化数据增强，并使用同一模型进行生成数据的标签和风格一致性校正。

Result: 迁移学习的特定编码器优于通用编码器的微调，风格化数据增强显著提升模型鲁棒性，尤其在主观内容检测上效果突出。官方提交在24个参赛队伍中排名第16。

Conclusion: 结合编码器专业化和标签一致的数据增强是提升主观性检测效果的有效策略。

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [214] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
*Maximilian Heil,Aleksandar Pramov*

Main category: cs.CL

TL;DR: 本文针对涉及数量、比较和时间引用的数值声明，评估了自动事实核查中真实性预测的建模策略。


<details>
  <summary>Details</summary>
Motivation: 数值声明因涉及复杂的数量和时间信息，对自动事实核查系统构成独特挑战，需要探索有效的建模方法提升准确率。

Method: 使用QuanTemp数据集和自建证据检索管道，研究现代BERT在不同证据数量、右到左(R2L)分词及其组合对分类性能的影响。

Result: R2L分词和更长输入上下文窗口均未提升性能，强调证据质量为关键瓶颈。最佳系统在CheckThat!2025任务中获得宏平均F1得分0.57，排名前四。

Conclusion: 提高自动事实核查中数值声明的性能，需更多关注证据质量而非仅提升上下文长度或采用R2L分词。

Abstract: Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [215] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: 本文介绍了UQLM，一个利用不确定性量化技术检测大型语言模型幻觉的Python工具包。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成虚假或误导性内容（幻觉）影响下游应用的安全性和可信度。

Method: 采用最新的不确定性量化技术，通过多个基于不确定性的评分器计算回应级置信度得分（0至1）。

Result: 提供了一套现成的基于不确定性的幻觉检测工具，能够有效评估模型生成内容的可信度。

Conclusion: UQLM工具包可以轻松集成到各种应用中，提高大型语言模型输出的可靠性，缓解幻觉问题。

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [216] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
*Rui-Jie Zhu,Tianhao Peng,Tianhao Cheng,Xingwei Qu,Jinfa Huang,Dawei Zhu,Hao Wang,Kaiwen Xue,Xuanliang Zhang,Yong Shan,Tianle Cai,Taylor Kergan,Assel Kembay,Andrew Smith,Chenghua Lin,Binh Nguyen,Yuqi Pan,Yuhong Chou,Zefan Cai,Zhenhe Wu,Yongchi Zhao,Tianyu Liu,Jian Yang,Wangchunshu Zhou,Chujie Zheng,Chongxuan Li,Yuyin Zhou,Zhoujun Li,Zhaoxiang Zhang,Jiaheng Liu,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文综述了潜在推理技术，这是通过模型连续隐藏状态实现的多步推理方法，避免了对自然语言的依赖。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）虽提升了大语言模型的推理能力和解释性，但受限于自然语言表达，降低了模型的表达能力，潜在推理试图打破这一瓶颈。

Method: 论文回顾了潜在推理的基础神经网络层作用、多种实现方法（激活循环、隐藏状态传播、微调策略）及先进范式（如通过掩码扩散模型实现无限深度推理）。

Result: 综合分析了当前潜在推理的主要途径和技术特点，揭示其如何支持复杂、全局一致的推理过程。

Conclusion: 该综述明确了潜在推理的概念框架，推动该领域研究发展，并提供了最新资源的GitHub链接以促进社区交流。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, especially when guided by explicit chain-of-thought (CoT)
reasoning that verbalizes intermediate steps. While CoT improves both
interpretability and accuracy, its dependence on natural language reasoning
limits the model's expressive bandwidth. Latent reasoning tackles this
bottleneck by performing multi-step inference entirely in the model's
continuous hidden state, eliminating token-level supervision. To advance latent
reasoning research, this survey provides a comprehensive overview of the
emerging field of latent reasoning. We begin by examining the foundational role
of neural network layers as the computational substrate for reasoning,
highlighting how hierarchical representations support complex transformations.
Next, we explore diverse latent reasoning methodologies, including
activation-based recurrence, hidden state propagation, and fine-tuning
strategies that compress or internalize explicit reasoning traces. Finally, we
discuss advanced paradigms such as infinite-depth latent reasoning via masked
diffusion models, which enable globally consistent and reversible reasoning
processes. By unifying these perspectives, we aim to clarify the conceptual
landscape of latent reasoning and chart future directions for research at the
frontier of LLM cognition. An associated GitHub repository collecting the
latest papers and repos is available at:
https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [217] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
*Ayush Parikh,Hoang Thanh Thanh Truong,Jeanette Schofield,Maximilian Heil*

Main category: cs.CL

TL;DR: 本文针对CLEF 2025 CheckThat!任务4a的科学网络话语检测，提出了三种模型方法，团队排名第七，成绩优于基线。


<details>
  <summary>Details</summary>
Motivation: 需要识别推文中是否包含科学论断、科学研究引用或科学实体，以提升科学网络话语的自动检测能力。

Method: 采用三种方法：变换器微调、少量样本提示的大型语言模型(LLM)和基于前期实验设计的集成模型。

Result: 团队在比赛中获得第七名，宏平均F1分数达0.8611，高于基线的0.8375。

Conclusion: 多方法结合的模型在多分类科学话语检测任务中表现优异，具有实际应用潜力。

Abstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [218] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: 本文提出了针对大型语言模型（LLM）重排序任务的效率评估新指标E²R-FLOPs，解决了现有效率评估指标受硬件和运行环境影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM重排序效率评估指标依赖硬件和运行时间设置，难以准确反映模型大小及效率-效果的权衡，阻碍实用部署。

Method: 提出了基于计算量（FLOPs）的效率指标RPP（单位计算相关性指标）和QPP（单位计算吞吐量指标），并设计了无需实验即可预测FLOPs的估算器。

Result: 通过广泛实验验证了该指标在不同LLM架构上的有效性，揭示了效率与效果之间的权衡关系。

Conclusion: E²R-FLOPs指标为LLM重排序任务提供了硬件无关、可解释的效率评估方法，有助于推动该领域的实用性研究与优化。

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [219] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 引入了Agent KB，一个分层经验框架，通过Reason-Retrieve-Refine流程提升语言代理在复杂任务中的错误纠正和经验复用能力。


<details>
  <summary>Details</summary>
Motivation: 语言代理在处理复杂任务时难以有效纠错及跨域复用经验，且不能互相学习彼此的经验。

Method: 提出Agent KB框架，通过捕捉高层策略和详细执行日志形成共享知识库，实现跨代理知识转移，使用Reason-Retrieve-Refine管道进行问题解决。

Result: 在GAIA基准测试中，Agent KB将成功率提升最多16.28个百分点，Claude-3和GPT-4在不同任务上均显著提升表现，在SWE-bench代码修复任务中也有明显改进。

Conclusion: Agent KB为语言代理提供了一个模块化、框架无关的基础设施，促进代理从过往经验学习并将成功策略推广到新任务。

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


### [220] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 本文提出了一种名为TokenShapley的细粒度Token级归因方法，结合了Shapley值和KNN检索技术，显著提升了大语言模型生成内容中关键字的归因准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注句子级归因，难以满足用户对生成内容中特定关键字（如数字、年份、名字）的归因需求。

Method: TokenShapley通过结合Shapley值计算和基于KNN的上下文检索，利用预先构建的数据存储，实现对生成Token的重要性量化和细粒度归因。

Result: 在四个基准测试上，TokenShapley在Token级归因准确率上比现有最先进方法提升了11%-23%。

Conclusion: TokenShapley有效解决了大语言模型细粒度归因问题，提升了关键字级别的归因准确性，具备良好的应用潜力。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [221] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 由于数据污染，衡量大型语言模型的泛化能力具有挑战性。作者提出以用户行为预测代替知识检索和推理任务，作为衡量泛化能力的替代方法，并进行了实证测试。


<details>
  <summary>Details</summary>
Motivation: 传统的知识检索和推理任务不适合衡量大型语言模型的泛化能力，因为这些模型不是为特定任务训练的，且数据泄漏使得测试任务难以保证完全未知。

Method: 提出一个基于用户行为预测的理论框架，用于评估模型的泛化能力，并在电影和音乐推荐数据集上对GPT-4o、GPT-4o-mini及Llama-3.1-8B-Instruct进行测试。

Result: 实验结果符合框架预期，GPT-4o的表现优于GPT-4o-mini和Llama模型，然而所有模型仍有较大提升空间，尤其是Llama。

Conclusion: 用户行为预测是一种理论上合理、可扩展且稳健的泛化能力评价方法，有助于克服传统评估方法的数据污染问题，未来有望引导更有效的模型评估与改进。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [222] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种用于隐性性别歧视检测的自适应监督对比学习框架ASCEND，通过阈值对比学习机制优化文本嵌入，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效识别网络中隐性性别歧视内容，且易产生较多误判。

Method: 通过计算文本嵌入的余弦相似度，只有超过可学习阈值的样本对才视为正样本，联合对比损失和交叉熵损失进行训练，并利用词级注意力和情感、情绪、毒性特征增强文本表示。

Result: 在EXIST2021和MLSC数据集上，ASCEND相较于现有方法在多个任务上宏平均F1提升分别达到9.86%、29.63%和32.51%。

Conclusion: ASCEND有效捕捉隐性性别歧视的微妙信号，显著提升检测准确率，展现出强大的应用潜力。

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [223] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 本文提出了一种基于生成检索增强、提示工程和跨模态注意力融合的AI框架，用于预测远程学习中的学生辍学风险，实现了高准确率和解释性干预。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型难以捕捉学生非结构化互动中的情感和上下文因素，导致辍学预测不足。

Method: 通过构建基于知识库的生成检索增强BERT模型进行情感分析，利用提示工程识别学业压力，并通过跨模态注意力融合文本、行为和社会人口统计数据，提高预测效果。

Result: 在4423名学生的纵向数据集上，该框架达到89%的准确率和0.88的F1分数，较传统模型提升7%，假阴性减少21%。

Conclusion: 该框架不仅提升了辍学预测性能，还能生成可解释的干预策略，为全球教育系统提供了一种可扩展的风险缓解解决方案。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [224] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: 提出了LCDS系统，利用源映射和逻辑规则提升电子病历自动出院摘要的准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自动生成出院摘要存在信息编造和无法追溯来源的问题，同时电子病历数据量大且形式复杂，给生成内容的内容归因带来挑战。

Method: 通过计算电子病历与出院摘要文本的相似度构建源映射表，限制摘要内容范围；结合逻辑规则生成更可靠的银牌出院摘要；支持对生成内容进行来源归属，方便专家审查和反馈；最后利用专家修订的黄金摘要对模型进行增量微调。

Result: LCDS能够生成更准确、有来源支持且符合临床领域逻辑的出院摘要，提高了模型的可靠性和可审查性。

Conclusion: 结合文本相似度的源映射和逻辑控制策略，有效减少了大语言模型自动生成出院摘要的幻觉现象，提升了医用文本生成的实用价值和信任度。

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [225] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: MindFlow是面向电商的开源多模态大语言模型代理，显著提升了复杂查询处理能力和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂多模态电商场景下的能力有限，需提升其适应性和效率。

Method: 基于CoALA框架，MindFlow集成记忆、决策和行动模块，采用“MLLM-as-Tool”的模块化策略进行视觉-文本推理。

Result: 通过线上A/B测试和模拟消融实验，MindFlow在复杂查询处理、用户满意度提升及运营成本降低方面表现出显著优势，实际部署中实现93.53%的相对提升。

Conclusion: MindFlow作为首个开源多模态电商大语言模型代理，极大增强了模型在复杂场景下的应用能力，具有较高的实用价值。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [226] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoRA-Augmented Generation（LAG）的高效方法，用于选择和组合针对特定任务和领域的微调语言模型专家。该方法无需额外训练或数据输入，在每个token和层级上灵活调用专家，显著提升了知识密集型任务的表现，且能与其他方法如检索增强生成（RAG）兼容。


<details>
  <summary>Details</summary>
Motivation: 随着针对特定任务和领域的微调语言模型专家数量增加，如何高效地选择和结合这些专家成为重要问题。

Method: 提出了LAG方法，无需新训练或数据，能基于每个token和层过滤、检索并应用相应专家。

Result: 在多个知识密集型任务中，LAG在无数据条件下性能优于现有方法。

Conclusion: LAG不仅提升了数据无依赖环境下的专家模型应用效率和效果，还能兼容结合其它增强生成技术，适应多种应用场景。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [227] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: 本论文研究了大型语言模型中推理效率与结构化思维链的影响，通过图最短路径任务对比训练不同类型的推理轨迹，发现训练中使用较长但结构合理的推理轨迹有助于模型更好地泛化。


<details>
  <summary>Details</summary>
Motivation: 探究在大型语言模型中，推理过程中的计算分配及推理结构对模型泛化能力的影响，尤其是在系统性和递增式推理（结构化思维链）中的作用。

Method: 设计基于分层图最短路径的控制实验，训练解码器变换器模型，比较基于最优动态规划轨迹和包含回溯的更长有效轨迹的训练效果，使用定制分词器处理问答-轨迹三元组。

Result: 在相同训练令牌预算下，训练使用较长且有效的推理轨迹的模型在泛化到未见图时表现更好；而仅仅通过增加冗余无效轨迹无法获得类似性能提升。泛化能力与模型下一令牌预测的置信度正相关，表明连贯且局部递增的轨迹有助于优化训练信号。

Conclusion: 训练中采用长且结构合理的推理轨迹，可以提升大型语言模型的泛化能力，这支持推理应系统且递增形成结构化思维链的观点，有助于更有效的训练和推理。

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [228] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

TL;DR: EduCoder是一个专门针对教育对话逐句注释的工具，支持复杂的教育对话转录编码。


<details>
  <summary>Details</summary>
Motivation: 现有的一般文本注释工具难以处理教育对话中的多样化师生互动及复杂教学特征的编码需求。

Method: EduCoder允许研究人员协作定义复杂编码方案，支持分类和开放式注释，并结合课程目的等上下文信息，还提供多注释者的对比功能以提升数据可靠性。

Result: 该工具实现了对教育对话的高效、灵活注释，增强了注释的可靠性和研究的协作性。

Conclusion: EduCoder为教育领域提供了专门且开源的对话注释平台，有助于提升教育对话数据的分析质量和效率。

Abstract: We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [229] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

TL;DR: 本文提出InfoRidge框架，通过信息理论方法分析Transformer模型中层的任务相关信息分布，发现中间偏上的层含有最高的泛化信息。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在自然语言生成中表现优异，其各层内部如何生成及传播任务相关信息尚不清楚，特别是泛化能力的形成机制缺乏理解。

Method: 设计InfoRidge信息理论框架，通过计算隐藏层与任务输出的互信息，追踪训练中任务相关信息在各层的分布；引入可训练的残差缩放系数作为探针，评估各层在分布转移下的重要性。

Result: 发现各模型和数据集在训练过程中预测信息在中间偏上层达到峰值，形成“泛化脊”，而末层信息下降，反映了泛化与记忆的过渡；残差缩放系数显示在分布转移下模型更依赖脊层，弱化末层。

Conclusion: 研究揭示Transformer中间层在泛化能力中扮演关键角色，为理解其内部机制提供新视角，强调中间层对模型泛化的重要贡献。

Abstract: Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [230] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: 本文提出使用隐私配置文件，让用户通过简单的自然语言指令控制数据隐私，局部模型根据指令重写查询，只隐藏敏感信息，再发送给外部大语言模型，从而在隐私和性能间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通常通过商业API访问，用户不得不将数据暴露给服务提供商，缺乏对数据隐私的控制。

Method: 构建一个框架，利用本地模型根据用户提供的隐私配置文件重写查询，隐藏敏感内容后发送给外部模型；构建多语言PEEP数据集，其中包含标注了隐私内容的真实用户查询及合成隐私配置文件。

Result: 轻量级大语言模型能够部分遵循隐私指令，但仍存在持续的挑战和不足。

Conclusion: 现有模型在理解和执行用户隐私偏好方面仍需改进，进一步研究需要设计能够更好满足用户隐私需求的模型。

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [231] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: 本文提出了GeoFact-X多语言地理事实推理基准和BRIDGE训练方法，提升大语言模型在低资源语言推理任务中的表现和语言一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多语言推理中表现不足，尤其是低资源语言常被模型误判为英语推理，导致事实准确性和信任度下降。现有基准只关注最终答案，忽视了模型是否用目标语言进行推理。

Method: 构建包含英语、印地语、日语、斯瓦希里语和泰语的GeoFact-X基准，带有推理轨迹注释；提出BRIDGE方法，通过监督微调和基于语言一致性奖励的强化学习，提升模型推理在目标语言的表现；设计自动评价协议，利用大语言模型作为评审者评估答案正确性及推理语言一致性。

Result: 实验显示BRIDGE方法显著提升了多语言推理的语言一致性和推理准确率，有助于模型稳健地进行跨语言泛化。

Conclusion: 通过推理感知的多语言强化学习，能够有效弥合大语言模型在低资源语言推理上的差距，增强多语言任务的表现和可信度。

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [232] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出了一个名为CoPE的评价框架，用于衡量大型语言模型在多语言环境下对上下文知识和参数知识的使用，发现模型对后位信息的忽视问题，并设计了基于提示的改进方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型如何优先整合上下文知识和参数知识尚不清楚，需要系统的评估方法来分析模型的知识利用方式。

Method: 提出CoPE框架，利用多语言MultiWikiAtomic数据集分析大模型对上下文中不同位置信息的利用情况，尤其关注其对后位信息的忽视现象及CoT提示对上下文利用的影响，基于分析设计提示方法提升上下文知识利用。

Result: 发现大语言模型存在明显的位置偏见，忽略后位信息；推理模型和使用CoT提示的模型反而更少利用上下文；CoT提示导致召回率和回答长度下降，削弱了上下文基础。设计的CK知情提示在摘要任务中有效提升事实基础和减少幻觉。

Conclusion: 大型语言模型存在‘lost-in-the-later’现象，影响上下文知识的整合，合理设计提示方法能够改善模型对上下文的利用，提升生成质量和事实准确性。

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [233] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

TL;DR: 该论文基于900万条社交媒体数据，分析了美国堕胎权争议中性别和地区政治环境对公众讨论的影响。


<details>
  <summary>Details</summary>
Motivation: 当前研究虽关注堕胎的意识形态分歧，但鲜少探讨性别与地方社会政治环境如何共同影响公众话语。

Method: 利用推特上近千万条堕胎相关帖子，结合用户的推测性别、政治倾向和地理位置，量化分析性别对堕胎态度及情绪表达的调节作用。

Result: 发现性别在堕胎态度和情绪表达中起显著作用，尤其在保守地区，形成显著的性别差距。丹布斯意见草案泄露事件激发了以支持堕胎的女性为主的线上动员，主要集中在堕胎权受威胁的地区。

Conclusion: 堕胎话语不仅仅是意识形态的极化，更深受性别和地域的结构性影响，身份认同在制度性变革时政治表达中起核心作用。

Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [234] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: 本文介绍了PhoniTale，一种基于音近性的跨语言助记生成系统，用大语言模型生成助记符，帮助韩语学习者习得英语词汇。系统表现与人工助记符相当，并通过自动评测和短期记忆测试验证了有效性。


<details>
  <summary>Details</summary>
Motivation: L2学习中词汇习得困难尤甚，尤其是语言类型差异大且发音结构不匹配的英语和韩语。此前助记研究多针对英语母语者学其他语言，逆向研究较少。

Method: 提出PhoniTale系统，基于L1关键词序列的音近性检索，利用大语言模型生成助记符。使用自动指标和人工评价进行比较，并通过短期记忆测试衡量助记实用性。

Result: PhoniTale生成的助记符在质量上与人工助记符相当，优于之前自动生成方法。短期记忆测试表明系统生成的助记符有实际帮助。

Conclusion: PhoniTale在跨语言助记生成领域表现良好，但助记质量和方法仍有提升空间，未来研究应针对这些关键领域进行改进。

Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [235] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型（如ChatGPT）是否真正理解语言的语义，聚焦于词汇和句子层面的语义。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在语言生成和对话中表现优异，但其真实语义理解能力尚存在争议。

Method: 通过分析大型语言模型的内部机制和其语言表征，结合Frege和Russell的经典语义理论，深入研究模型的语义能力。

Result: 研究揭示了模型在语义理解方面的潜力，形成了关于大型语言模型语义能力的更细致认识。

Conclusion: 大型语言模型在某种程度上具备语义能力，但对语义理解的程度依然复杂且需进一步探索。

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [236] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: 提出了MODELCITIZENS数据集，注重多元身份背景和上下文对有害语言检测的影响，并发布相关微调模型提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有有害语言检测模型忽略了社区多样性和上下文，导致无法准确识别某些语境中的有害语言。

Method: 收集6800条社交媒体帖子，集成4万条多身份群体标注，利用大语言模型生成对话场景增强上下文，微调LLaMA和Gemma模型进行检测。

Result: 现有主流有害语言检测工具对该数据集表现欠佳，微调后的模型在内部分布测试中比GPT-o4-mini准确率提升5.5%。

Conclusion: 社区多样性和上下文对有害语言检测至关重要，基于此构建的数据集和模型更能实现包容性内容审核。

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [237] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文研究了两个临床自然语言处理的高难度任务：护士口述中的结构化报告和医生-患者咨询中的医疗订单提取，利用大型语言模型进行性能评估，并提出生成真实护士口述的智能流程，同时发布首个开放数据集支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在临床NLP任务中表现优异，但护士口述结构化报告和医疗订单提取因数据稀缺和敏感性而未得到充分研究，这两项任务的解决可显著减轻医疗文书负担，提高医疗服务效率。

Method: 利用私有和开源临床数据集，评估开源和封闭权重大型语言模型在两个任务中的表现，分析其优缺点，并设计一种智能流程生成真实且无敏感信息的护士口述，支持结构化临床观察提取。

Result: 提出的智能流程有效生成了真实的护士口述，有助于结构化信息提取，同时评估结果表明现有大型语言模型在这两项任务上具备良好潜力。并发布了首批开放数据集SYNUR和SIMORD，推动相关领域研究。

Conclusion: 本文通过提出创新方法和开放数据集，促进了护士口述结构化报告和医疗订单提取两项重要临床NLP任务的研究，有望减轻医疗人员文档负担，提升患者护理质量。

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [238] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为R2-LLMs的新型分层检索增强推理框架，通过双层检索和蒙特卡罗树搜索显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展方法依赖更高级模型的蒸馏数据，缺乏无需链式推理训练数据的有效推理增强手段。

Method: R2-LLMs通过粗层次抽取问题模板检索相似样本进行上下文学习，细层次在MCTS中检索中间解步骤并利用过程奖励模型评分优化推理。

Result: 在MATH500、GSM8K和OlympiadBench-TO数据集上，使用LLaMA-3.1-8B模型较基线最高提升16%的推理准确率。

Conclusion: 所提R2-LLMs框架有效提升了大语言模型的复杂推理能力，具备良好的泛化和可扩展性。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [239] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: 本文提出了Re5框架，通过自我评估和选择性修订，提高大语言模型对复杂指令的遵从性，同时保持生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升大语言模型遵守格式和指令方面存在高成本或过度修订导致质量下降的问题。

Method: Re5从用户指令中提取任务和约束，进行结构化评估避免错误积累，结合细粒度约束特定内容评估，进行选择性修订，最终利用高质量输出进行对齐调优。

Result: 实验显示Re5在少量数据情况下，指令遵从性能接近高性能模型GPT-4o-mini，且修订后回答质量提升至64.24%胜率。

Conclusion: Re5为在最小外部监督下高效有效提升指令遵守性和回答质量提供了可行方案。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [240] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一种反向知识蒸馏范式，让大型语言模型(LLM)从小型语言模型(SLM)学习，特别适用于文本匹配任务。


<details>
  <summary>Details</summary>
Motivation: 在文本匹配等任务中，微调的小模型通常能提供更有效的领域特定表示，但LLM具有丰富的语义理解能力，如何结合两者优势是研究重点。

Method: 使用LoRA将解码器式LLM重新解释为编码器-解码器结构，编码器生成压缩表示，解码器映射输出；通过边距感知对比学习(MCL)对齐LLM和SLM的相似度分数。

Result: 该方法仅需性能较好的SLM作为教师，显著提升了LLM的表现，并在金融和医疗数据集以及实际线上环境中验证其效果。

Conclusion: 所提出的反向知识蒸馏范式有效融合了小模型的领域特定能力与大模型的语义理解，提升了文本匹配任务中LLM的性能。

Abstract: Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [241] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: 本文提出了SARA框架，通过结合文本片段和语义压缩向量，提升了检索增强生成模型在有限上下文长度下的回答准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成模型在有限上下文长度和检索文档冗余问题下表现受限，纯压缩方法虽减少输入，但丢失了关键细节，影响事实准确性。

Method: SARA框架将上下文分为细粒度文本片段与紧凑的语义压缩向量两层表示，利用迭代证据选择模块动态重排上下文，提高上下文使用效率和答案正确率。

Result: 在9个数据集、5个开源大模型（Mistral、Llama、Gemma）上，SARA显著提升了答案相关性（+17.71）、正确性（+13.72）和语义相似度（+15.53）。

Conclusion: 结合文本和压缩向量的双层上下文表示是提升有限上下文下检索增强生成模型效果的有效策略。

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [242] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: ECom-Bench是首个针对电商客服领域多模态大型语言模型代理的评测框架，包含真实用户模拟和真实任务数据，任务难度高，先进模型表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专注于电商客服多模态能力评测的基准，需构建真实复杂场景的测试平台。

Method: 设计基于真实电商客户交流信息的动态用户模拟和包含多场景电商任务的数据集，模拟真实复杂业务场景。

Result: 先进模型如GPT-4o在此基准中仅达到10-20%通过率，表现有限，显示任务复杂性。

Conclusion: ECom-Bench为电商多模态客服领域提供了高难度标准化评测平台，代码数据将开源助力后续研究。

Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [243] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出Smoothie-Qwen方法，通过调整生成概率减少多语言大模型的语言混淆现象，无需重训且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型常在生成时偏向主导语言，导致语言混淆，影响模型的语言控制能力。

Method: Smoothie-Qwen是一种轻量级后处理方法，通过选择性调整token输出概率抑制不期望的语言生成。

Result: 应用到Qwen模型后，该方法使非期待中文输出减少超过95%，且在多语言任务中保持准确率。

Conclusion: Smoothie-Qwen提供了一种实用高效的解决方案，提高大模型语言可控性，增强其在全球应用中的可靠性。

Abstract: Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [244] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: 提出了DualDistill框架，通过多教师模型的互补推理策略蒸馏，训练出能动态选择最佳推理策略的Agentic-R1，提升了数学和逻辑推理任务的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前长链推理模型在数学推理表现出色但依赖自然语言，工具增强代理在算术上通过代码执行，但在复杂逻辑任务上表现欠佳，亟需结合多策略提升推理能力。

Method: 提出DualDistill微调框架，将多位教师模型的推理策略蒸馏至统一学生模型，训练Agentic-R1能够动态选择策略，用工具处理算术和算法问题，用文本推理处理抽象问题。

Result: 在多种任务上表现出更高准确率，包含计算密集型和标准基准，展示了多策略蒸馏在实现稳健高效推理上的有效性。

Conclusion: DualDistill框架通过多策略知识融合和动态策略选择，有效提升了模型推理的准确性和泛化能力，为复杂数学和逻辑推理任务提供了新的解决方案。

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [245] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: 该论文提出了DRAGON，一个动态的俄语检索增强生成（RAG）系统评测基准，基于不断更新的新闻语料库，实现全面评测检索器和生成器，支持自动生成问答并发布公共排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有多为英语的RAG评测资源不能满足俄语等其他语言动态和真实环境应用的需求。

Method: 构建基于动态更新的俄语新闻及公开文档的语料库，自动构造基于知识图谱的问题，涵盖四种核心问答类型，实现檢索和生成组件的全面评价，发布完整自动问答生成框架及评价脚本。

Result: 建立了首个动态俄语RAG评测系统DRAGON，支持多语言复用，已有公开排行榜促进社区参与与系统比较。

Conclusion: DRAGON填补了俄语RAG评测资源缺口，反映现实变化，为后续相关多语言RAG系统的开发与评测提供了重要工具和平台。

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [246] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种层级思维指令微调方法HIRAG，提高了检索增强生成模型在处理实时信息和领域特定问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）系统依赖大模型的上下文学习，但缺乏对RAG模型特定能力的深入研究，导致文档质量不稳定和检索系统不完善。

Method: 提出RAG模型应具备过滤、组合和专属推理三层次能力，设计HIRAG方法通过多层级链式思维的"思考后回答"策略进行指令微调。

Result: 实验显示HIRAG显著提升了模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的性能。

Conclusion: HIRAG方法有效增强了RAG模型对外部知识与内部知识的融合与推理能力，提高了开放书本考试能力和整体性能。

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [247] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: 论文提出了一种名为Omni-router Transformer的混合专家模型，通过在不同层之间共享路由器增强专家合作，显著提升了自动语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统的混合专家模型中，各层路由器独立选择专家，导致各层选择之间相关性弱，限制了多层专家间的协作和专业化。

Method: 提出共享路由器的Omni-router Transformer，在不同MoE层之间共享路由器，实现专家选择的协同，提高专家间的合作与专业化水平。

Result: 在大规模伪标注数据集和10个异域自动语音识别基准上，Omni-router Transformer训练损失更低，平均词错误率分别比密集模型和Switch Transformer降低11.2%和8.2%，展现更强的结构化专家使用及对多样数据的鲁棒性。

Conclusion: 共享路由器的Omni-router Transformer有效提升了多层混合专家模型的协作能力和专业化，显著增强了自动语音识别系统的性能和鲁棒性。

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [248] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
*Yujia Hu,Tuan-Phong Nguyen,Shrestha Ghosh,Moritz Müller,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文介绍了GPTKB v1.5，一个包含一亿条知识三元组的知识库，基于GPT-4.1构建，支持高级知识探索和查询。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然强大，但其事实知识理解有限，缺乏可供浏览和统计分析的工具。

Method: 采用GPTKB方法，利用大规模递归的语言模型知识实现方式构建知识库。

Result: 成功构建了一个高密度互联的100百万三元组知识库。

Conclusion: 大规模递归语言模型知识实现为系统分析语言模型知识和自动构建知识库提供了全新机会。

Abstract: Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

</details>


### [249] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
*Jing Yang Lee,Hamed Bonab,Nasser Zalmout,Ming Zeng,Sanket Lokegaonkar,Colin Lockard,Binxuan Huang,Ritesh Sarkhel,Haodong Wang*

Main category: cs.CL

TL;DR: 本文提出通过将相关文档集群转换为多轮多主题对话，合成对话数据增强大语言模型的多轮对话能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型预训练数据多为连续散文，缺乏多轮对话结构，导致模型在多轮对话任务中能力不足。

Method: 设计一个流水线流程，将多篇相关文档转化为扩展的多轮、多主题信息检索型对话，构建了包含73万多轮长对话的DocTalk语料库，对Wikipedia文章进行应用。

Result: 在预训练中加入DocTalk语料，使模型的上下文记忆和理解能力提升了最多40%，且基础性能未受影响。

Conclusion: 通过合成的多轮对话结构预训练数据，能有效提升大语言模型的多轮对话能力和理解力，具有实际应用价值。

Abstract: Large Language Models (LLMs) are increasingly employed in multi-turn
conversational tasks, yet their pre-training data predominantly consists of
continuous prose, creating a potential mismatch between required capabilities
and training paradigms. We introduce a novel approach to address this
discrepancy by synthesizing conversational data from existing text corpora. We
present a pipeline that transforms a cluster of multiple related documents into
an extended multi-turn, multi-topic information-seeking dialogue. Applying our
pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training
dialogue corpus consisting of over 730k long conversations. We hypothesize that
exposure to such synthesized conversational structures during pre-training can
enhance the fundamental multi-turn capabilities of LLMs, such as context memory
and understanding. Empirically, we show that incorporating DocTalk during
pre-training results in up to 40% gain in context memory and understanding,
without compromising base performance. DocTalk is available at
https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [250] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
*Anand A. Rajasekar,Praveen Tangarajan,Anjali Nainani,Amogh Batwal,Vinay Rao Dandin,Anusua Trivedi,Ozan Ersoy*

Main category: cs.CL

TL;DR: 本论文介绍了用于电商领域的对话式助理Flippi，利用大语言模型实现个性化、高效的购物体验。


<details>
  <summary>Details</summary>
Motivation: 解决电商产品种类繁多导致用户难以高效发现商品的问题，提升购物体验。

Method: 通过查询重构、意图识别、检索增强生成、命名实体识别和上下文简化等NLP技术，实现精准理解和个性化推荐。

Result: Flippi能准确提供产品信息、发现优惠、对比商品属性，提升用户决策质量和购物效率。

Conclusion: Flippi有效结合了在线购物便利性与实体店个性化服务，提升用户满意度和转化率，树立了数字市场的新标准。

Abstract: The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

</details>


### [251] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
*Amane Watahiki,Tomoki Doi,Taiga Shinozaki,Satoshi Nishida,Takuya Niikawa,Katsunori Miyahara,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文构建了基于基础形式本体的遮挡补全系统性分类基准，评估大规模视觉语言模型（LVLMs）在遮挡推理文本上的表现，发现模型整体表现接近人类，但在某些对象类别和日本语提示下准确率存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 探索大规模视觉语言模型在多模态任务中，对遮挡补全相关文本推理能力的表现，目前该领域尚未被充分研究。

Method: 基于基础形式本体，构建遮挡补全的系统分类基准，并利用该基准评估多个LVLMs在不同对象类别和语言提示下的推理准确率。

Result: 发现多数LVLMs整体性能与人类相近，但部分模型（如LLaVA-NeXT变体和Claude 3.5 Sonnet）在日本语提示时，对原图的准确率低于空白无视觉内容的刺激，表明语言能力存在短板。

Conclusion: LVLMs在遮挡补全文本推理上表现不一，且语言特定能力（如日语理解）影响模型推理准确性，提示需要进一步提升模型的多语言推理能力。

Abstract: One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.

</details>


### [252] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
*Tanvina Patel,Wiebke Hutiri,Aaron Yi Ding,Odette Scharenborg*

Main category: cs.CL

TL;DR: 本文探讨自动语音识别系统（ASR）中的偏见问题，比较不同的性能和偏见测量方法，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统存在针对不同说话者群体（如性别、年龄、口音）的偏见，且现有研究多关注检测和缓解，而缺少有效衡量性能和偏见的方法。

Method: 本文对比了多种文献和提出的性能及偏见衡量方法，使用多种偏见缓解策略，评估了荷兰语端到端ASR系统的表现。

Result: 发现单纯使用平均错误率不足以全面反映系统性能和偏见，需要结合其他指标综合评估。

Conclusion: 建议在报告ASR系统性能和偏见时，使用多种指标，以更准确反映系统在不同说话者群体中的表现及整体偏见水平。

Abstract: There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

</details>


### [253] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLMs）的虚拟受访者模拟框架，用于高效生成和验证心理测验题目，确保测验题目的结构效度，无需大量人力数据收集。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在心理特质评估中的应用，迫切需要一种可扩展且高效的测验题目生成方法，确保生成题目能够真实测量目标特质。

Method: 提出模拟不同中介变量（mediators）的受访者，通过多样化模拟响应来筛选稳健测量特质的测验题目，验证了该方法对三种心理特质理论（Big5, Schwartz, VIA）的有效性。

Result: 实验显示，采用中介变量生成和模拟框架能够有效识别高效度的测验题目；LLMs能够从特质定义生成合理的中介变量并模拟受访者行为。

Conclusion: 该研究为低成本、高效的心理测验开发提供了新方向，推动了对LLMs模拟人类行为能力的深入理解，并公开了数据集和代码以支持后续研究。

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [254] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
*Teodor-George Marchitan,Claudiu Creanga,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 本文介绍了Unibuc-NLP团队在SemEval 2025文本情感检测任务中的方法，主要使用大型语言模型，通过少量示例提示和微调进行多标签情感检测，并在不同语言子集上取得了不同水平的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决文本情感检测中的多标签识别挑战，促进跨语言情感检测技术的发展。

Method: 基于大型语言模型（Gemini、Qwen、DeepSeek）采用少量示例提示或模型微调的方法进行多标签情感检测。

Result: 在多标签情感检测的任务中，英文子集F1-macro得分0.7546，排名26/96；葡萄牙语（莫桑比克）子集得分0.1727，排名35/36；Emakhuwa子集得分0.325，排名第一。

Conclusion: 大型语言模型结合少样本学习和微调方法在多语言情感检测中表现良好，特别是在低资源语言Emakhuwa上取得领先成绩。

Abstract: This paper describes the approach of the Unibuc - NLP team in tackling the
SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion
Detection. We mainly focused on experiments using large language models
(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With
our final system, for the multi-label emotion detection track (track A), we got
an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36
teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams)
for the Emakhuwa subset.

</details>


### [255] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
*Sebastian Pohl,Max Ploner,Alan Akbik*

Main category: cs.CL

TL;DR: 该论文探讨了知识编辑评估方法的稳健性及其对编辑器排名的影响，发现不同评估指标和批量大小会导致排名变化，并指出字符串匹配评估方法存在误判问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑评估数据集使用不同方法评分，但其稳健性和公平性仍未充分研究，且编辑对模型整体能力的影响未充分关注。

Method: 通过比较不同评估指标、方法及编辑批量大小对知识编辑器排名的影响，同时在人类评估中分析字符串匹配评估方法的误判倾向。

Result: 不同评估方法和批量大小导致知识编辑器排名变化，同时字符串匹配评估易产生误判；编辑行为也影响一般语言理解任务表现。

Conclusion: 评估指标选择和方法设计对编辑器排名有显著影响，字符串匹配方法存在不足，需改进评估体系以公平准确评价编辑效果。

Abstract: Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

</details>


### [256] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: 本文提出了一种针对多模态错误信息检测的持续学习方法DAEDCMD，解决了模型遗忘和环境变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态错误信息检测方法依赖离线数据训练，无法适应不断出现的新事件，导致性能下降。

Method: 利用基于狄利克雷过程的专家混合结构隔离事件特定参数防止遗忘，并通过学习连续时间动态模型预测未来环境分布。

Result: 在多种多模态错误信息检测基线和持续学习方法中，DAEDCMD表现出持续且显著的优越性能。

Conclusion: DAEDCMD有效缓解了持续学习中的遗忘和环境变化问题，提升了多模态错误信息检测的实用性和鲁棒性。

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


### [257] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
*Sandeep Mishra,Anubhab Mandal,Bishal Santra,Tushar Abhishek,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 该论文针对聊天系统中的文本自动补全（Ghosting）问题进行了全面研究，比较了多种方法并提出了新的动态早停策略。


<details>
  <summary>Details</summary>
Motivation: 随着聊天系统如ChatGPT的普及，Ghosting技术变得非常重要，但在NLP/ML领域缺乏系统研究和基准测试。

Method: 使用四个公开对话数据集，比较了基于trie、n-gram和深度学习的方法，并引入了基于熵的动态早停策略。

Result: n-gram和trie方法在已见前缀上表现和效率优于深度学习，神经模型在未见查询上表现更佳，加入对话上下文显著提升Ghosting质量。

Conclusion: 统计模型和深度学习各有优势，结合对话上下文可提升性能，研究成果和资源已公开。

Abstract: Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

</details>


### [258] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文介绍了开源的FActScore评估框架OpenFActScore，用于评价大语言模型生成文本的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 原FActScore依赖闭源商业模型，限制了透明性和可重复性，需一个支持开源模型的事实性评价框架。

Method: 通过原子事实生成（AFG）提取文本中独立事实，结合原子事实验证（AFV）对每条事实进行知识源比对。实现支持任意Hugging Face模型。

Result: 在原FActScore基准上测试多款开源模型，开源模型表现接近闭源系统，Gemma表现最佳，整体结果与原实验相关系数高达0.99。

Conclusion: OpenFActScore提升了评价透明度、可重复性和成本效益，是一个强大的开源事实性评价工具。

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [259] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
*Ehud Reiter*

Main category: cs.CL

TL;DR: ACL领域几乎没有关注NLP系统实际影响的研究，大部分论文仅侧重于指标评估。


<details>
  <summary>Details</summary>
Motivation: 当前ACL社区很少重视对NLP系统实际世界影响的评估，影响了技术的推广和应用。

Method: 通过对ACL Anthology中论文的结构化调查，统计包含实际影响评估的论文比例及其表现方式。

Result: 仅约0.1%的论文包含实际影响评估，且大多数评估粗略，更多关注指标评估。

Conclusion: 若能认真评估NLP技术的实际影响，将促进其更实用化和更快推广。

Abstract: The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [260] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: RabakBench是一个针对新加坡多语言环境的安全性基准测试，涵盖Singlish、中文、马来语和泰米尔语，包含超过5000个带有详细安全标签的数据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型及其安全分类器在低资源语言上的表现较差，原因在于训练数据和评测基准有限。为此需要一个针对新加坡多语言独特背景的安全评测基准。

Method: 通过三阶段流程构建RabakBench：第一阶段生成对抗示例，第二阶段采用多标签半自动标注结合人类判断，第三阶段进行高保真翻译确保语言细节和毒性信息的保留。

Result: 在四种语言和六个细粒度安全类别下评估11个流行的安全分类器，结果显示性能显著下降。RabakBench为东南亚多语言场景提供了强健的安全评测，有效支持低资源环境。

Conclusion: RabakBench不仅促进多语言安全评测，还提供了低资源环境中构建本地化安全数据集的可复现框架，数据集和代码已公开。

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [261] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
*Minghang Zhu,Shen Gao,Zhengliang Shi,Jiabao Fang,Pengjie Ren,Zhaochun Ren,Zhumin Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 提出了一种自我进化的语言模型训练方法，通过多级原则生成和基于原则的实例生成，利用小规模语言模型高效生成训练数据，从而减少碳排放和数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 传统依赖大型语言模型扩展数据集的训练方法虽然减轻了人工标注负担，但存在高碳排放和闭源模型数据泄露风险。

Method: 先用大型模型总结任务完成原则（多级原则生成），再用小型模型基于这些原则生成大量训练数据（基于原则的实例生成）。

Result: 实验表明该方法显著提升了模型性能，相较直接用小型模型生成数据的效果更好，同时大幅降低碳排放。

Conclusion: 该自我进化方法有效提升训练效率和环保性，降低风险，为语言模型的数据扩增提供了新的途径。

Abstract: A common training approach for language models involves using a large-scale
language model to expand a human-provided dataset, which is subsequently used
for model training.This method significantly reduces training costs by
eliminating the need for extensive human data annotation. However, it still
faces challenges such as high carbon emissions during data augmentation and the
risk of data leakage when we use closed-source LLMs. To address these issues,
we propose a self-evolution method for language models. First, we introduce the
Multi-level Principle Generation, which enables a large-scale model to
summarize task-completion principles based on a small amount of task data.
Then, we propose the Principle-based Instance Generation, in which a
smaller-scale language model uses these task principles to generate a large
amount of data. This data is then used for model training. Experimental results
show that our proposed method significantly improves model performance compared
to directly using a smaller-scale language model to generate data.
Additionally, since we only use the large-scale language model to generate the
task-completion principles, the carbon emissions associated with training the
model are greatly reduced.

</details>


### [262] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
*Nicholas Popovič,Ashish Kangen,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的全自动合成数据生成与上下文学习管线，用于文档级实体和关系抽取，解决了标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有文档级实体和关系抽取在零样本或少样本情境下，缺乏大规模高质量标注语料，限制了任务性能提升。

Method: 利用推理优化语言模型，结合合成数据生成和基于检索的上下文学习，动态获取相关示例，构建无需人工标注的高质量示范数据库。

Result: 生成了包含约5千篇维基百科摘要、5.9万个实体和3万个关系三元组的合成数据集，并在DocIE共享任务中零样本评测。结果表明，文档级联合实体和关系抽取依然具有较大挑战性。

Conclusion: 基于LLM的自动合成数据生成与上下文学习方法有效缓解标注稀缺问题，但文档级联合抽取任务仍需进一步研究以提升性能。

Abstract: Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

</details>


### [263] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
*Youmna Farag,Svetlana Stoyanchev,Mohan Li,Simon Keizer,Rama Doddipatla*

Main category: cs.CL

TL;DR: 本文提出了一种利用零样本链式提示的多阶段条件性失败恢复框架，有效提升了具身智能体在复杂任务中的执行成功率。


<details>
  <summary>Details</summary>
Motivation: 具身智能体在执行复杂任务时易发生失败，亟需有效的失败恢复机制以提升任务成功率。

Method: 设计了一个包含四个错误处理阶段的恢复框架，利用大语言模型（LLMs）的推理能力，在任务执行中三阶段处理错误，执行后进行反思，结合环境上下文分析问题并制定解决策略。

Result: 在TEACH数据集的TfD基准测试中，所提方法相比无错误恢复的基线模型提升11.5%，且击败了当前最佳模型19%的表现。

Conclusion: 该多阶段错误恢复框架能显著提升具身智能体的任务执行效果，证明了结合LLMs推理能力进行条件性错误处理的有效性。

Abstract: Embodied agents performing complex tasks are susceptible to execution
failures, motivating the need for effective failure recovery mechanisms. In
this work, we introduce a conditional multistage failure recovery framework
that employs zero-shot chain prompting. The framework is structured into four
error-handling stages, with three operating during task execution and one
functioning as a post-execution reflection phase. Our approach utilises the
reasoning capabilities of LLMs to analyse execution challenges within their
environmental context and devise strategic solutions. We evaluate our method on
the TfD benchmark of the TEACH dataset and achieve state-of-the-art
performance, outperforming a baseline without error recovery by 11.5% and
surpassing the strongest existing model by 19%.

</details>


### [264] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: 本文提出了训练数据的记忆难度与数据熵呈线性关系的“熵-记忆定律”，并基于此定律引入区分训练与测试数据的新方法。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）会记忆训练数据的一部分，有时会完整复现这些内容，但如何量化训练数据的记忆难度尚未深入研究。

Method: 通过在开源模型OLMo上的实证实验，发现数据熵与记忆得分线性相关；进一步分析随机字符串的低经验熵，提出熵-记忆定律，并基于此实现区分训练和测试数据的Dataset Inference技术。

Result: 发现训练数据的记忆难度可以通过数据熵来刻画，验证了熵-记忆定律，并成功应用于训练数据与测试数据的区分。

Conclusion: 数据熵是衡量语言模型记忆训练数据难度的重要指标，基于该指标可有效推动模型隐私保护和数据安全领域的研究。

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [265] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: 本文综述了提示调优作为一种参数高效的语言模型适配方法，主要类别为直接提示学习和迁移学习。


<details>
  <summary>Details</summary>
Motivation: 解决在保持语言模型固定的情况下，高效调整模型以适应特定任务的需求。

Method: 本文将提示调优方法分为直接提示学习（包括优化方法、编码器方法、分解策略和专家混合框架）和迁移学习（包括通用迁移、编码器方法、分解策略），并分析其设计、创新与优缺点。

Result: 通过比较不同框架，揭示了各方法的优势与不足，指出了计算效率和训练稳定性的挑战。

Conclusion: 未来应着重提升训练鲁棒性和扩展应用范围，以解决现有方法中的关键问题。

Abstract: This survey reviews prompt tuning, a parameter-efficient approach for
adapting language models by prepending trainable continuous vectors while
keeping the model frozen. We classify existing approaches into two categories:
direct prompt learning and transfer learning. Direct prompt learning methods
include: general optimization approaches, encoder-based methods, decomposition
strategies, and mixture-of-experts frameworks. Transfer learning methods
consist of: general transfer approaches, encoder-based methods, and
decomposition strategies. For each method, we analyze method designs,
innovations, insights, advantages, and disadvantages, with illustrative
visualizations comparing different frameworks. We identify challenges in
computational efficiency and training stability, and discuss future directions
in improving training robustness and broadening application scope.

</details>


### [266] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: NeoBabel是一个支持六种语言的多语言文本生成图像框架，实现了高性能、高效率和包容性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成主要以英语为中心，造成其他语言使用者的数字鸿沟，且使用翻译管道导致语义偏差和文化不匹配。

Method: 通过大规模多语言预训练和高分辨率指令微调训练NeoBabel模型，同时扩展英文基准测试至多语言版本。

Result: NeoBabel在多语言基准测试中表现优异，超越同行多语言大模型，同时模型体积更小，且推出两项多语言评测指标。

Conclusion: 多语言能力不仅不妨碍性能，反而促进了生成AI的鲁棒性、效率和文化适配性，推动了包容性AI研究。

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [267] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了Code Triangle框架，对大语言模型的编程能力进行系统评估，发现其解决方案缺乏多样性和稳健性，通过引入人类编辑内容和模型组合策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成上取得显著进展，其真实编程能力和在多维度上的表现尚未被充分探究。

Method: 设计Code Triangle框架，从编辑分析、代码实现和测试用例生成三方面评估大语言模型，并在竞赛编程基准上进行大量实验。

Result: 发现模型在三维度上能自我一致但缺少人类程序员的多样性和稳健性，错误多因训练数据偏差和推理迁移有限所致，通过引入人类编辑内容、多样测试用例及模型组合显著提升性能和稳健性。

Conclusion: 揭示了大语言模型认知中的一致性和不一致性，提出利用其进行自我反思和改进为未来开发更强编码模型的潜在方向。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [268] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
*Wei Shen,Jiangbo Pei,Yi Peng,Xuchen Song,Yang Liu,Jian Peng,Haofeng Sun,Yunzhuo Hao,Peiyu Wang,Yahui Zhou*

Main category: cs.CL

TL;DR: Skywork-R1V3通过强化学习后训练，有效提升视觉语言模型的推理能力，实现了多模态推理领域的领先表现。


<details>
  <summary>Details</summary>
Motivation: 如何将文本领域大语言模型的推理能力转移到视觉任务中，从而提升视觉语言模型的推理性能。

Method: 采用精心设计的强化学习后训练框架，无需额外继续预训练，通过连接模块实现跨模态对齐，并用关键推理标记的熵作为训练过程中检查点选择的指标。

Result: Skywork-R1V3在MMMU测试中表现大幅提升，从64.3%提升到76.0%，达到入门级人类水平，且38B参数模型能媲美顶级闭源模型。

Conclusion: 强化学习后训练是推动开放源代码视觉语言模型推理能力显著提升的有效方法，Skywork-R1V3为多模态推理带来重要进展。

Abstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.

</details>


### [269] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
*Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge Zhang*

Main category: cs.CL

TL;DR: 本文提出了CriticLean框架，通过强化学习优化批评者阶段，以提升数学命题到形式化代码翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有工作关注生成和编译成功率，但忽视了评估形式化表达是否忠实原始语义的重要阶段。

Method: 引入CriticLeanGPT，结合监督微调和强化学习训练，专注语义精准评估，并设计CriticLeanBench基准测评模型区分语义正确与错误的形式化表达。

Result: 所训练的CriticLeanGPT在CriticLeanBench上显著优于强基线；构建了包含28.5万问题的FineLeanCorpus数据集，领域丰富、难度广泛且高质量。

Conclusion: 优化批评者阶段对于生成可靠的形式化表达至关重要，CriticLean为未来形式化数学推理改进提供了重要参考。

Abstract: Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

</details>


### [270] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: 本文通过迁移学习和风格化数据增强，提高了英语新闻文本中主观性检测的分类性能。


<details>
  <summary>Details</summary>
Motivation: 提升主观性与客观性句子分类的准确度，探索迁移学习与数据增强的效果。

Method: 对比预训练编码器的微调与相关任务编码器的迁移学习，使用GPT-4o生成风格化的句子改写，并通过同模型修正生成文本以保证标签和风格一致性。

Result: 迁移学习的特定编码器优于一般编码器微调，且精心设计的数据增强显著提升了模型对主观内容的识别能力。

Conclusion: 结合编码器专业化和标签一致的数据增强方法，有助于提升主观性检测性能。

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [271] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
*Maximilian Heil,Aleksandar Pramov*

Main category: cs.CL

TL;DR: 该论文评估了针对包含数量和时间信息的陈述自动事实核查的建模策略，发现证据质量比输入长度和分词方式对性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 自动事实核查系统在处理包含数字、比较和时间参考的陈述时存在独特挑战，因此需要优化建模策略以提升预测准确性。

Method: 利用QuanTemp数据集，构建证据检索管线，使用ModernBERT模型研究了证据数量与输入上下文窗口长度、右到左（R2L）分词方式及其组合对真假判断性能的影响。

Result: 发现R2L分词和更长上下文窗口均未提升自然语言推理中的数值任务性能，证据质量是性能瓶颈。最终系统达到宏平均F1值0.57，在CheckThat! 2025任务中排名前四。

Conclusion: 提升事实核查数值陈述真实性的关键在于提高证据质量，而非简单增加证据数量或改变分词方式。

Abstract: Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [272] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: 本文介绍了一个名为UQLM的Python工具包，用于通过不确定性量化技术检测大型语言模型的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成虚假或误导性内容的问题影响下游应用的安全性和可信度，因此需要有效的幻觉检测方法。

Method: 提出了基于最先进不确定性量化技术的多种评分方法，计算回复的置信度分数（0到1）以检测幻觉。

Result: 开发出UQLM工具包，提供开箱即用的基于不确定性量化的幻觉检测方案，易于集成和使用。

Conclusion: UQLM提高了大型语言模型输出的可靠性，为幻觉检测提供了便捷有效的解决方案。

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [273] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
*Rui-Jie Zhu,Tianhao Peng,Tianhao Cheng,Xingwei Qu,Jinfa Huang,Dawei Zhu,Hao Wang,Kaiwen Xue,Xuanliang Zhang,Yong Shan,Tianle Cai,Taylor Kergan,Assel Kembay,Andrew Smith,Chenghua Lin,Binh Nguyen,Yuqi Pan,Yuhong Chou,Zefan Cai,Zhenhe Wu,Yongchi Zhao,Tianyu Liu,Jian Yang,Wangchunshu Zhou,Chujie Zheng,Chongxuan Li,Yuyin Zhou,Zhoujun Li,Zhaoxiang Zhang,Jiaheng Liu,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文综述了隐式推理在大型语言模型中的应用，探讨了其通过隐藏状态完成多步推理的机制以及相关方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统链式思维推理依赖自然语言表达，限制了模型的表达能力，隐式推理通过隐藏状态完成推理，有望突破这一瓶颈。

Method: 本文全面回顾了隐式推理的方法，包括基于激活的递归、隐藏状态传播、通过微调压缩或内化显式推理轨迹，以及利用掩码扩散模型实现无限深度隐式推理。

Result: 通过总结各种隐式推理方法，本文阐明了神经网络层作为推理计算基础的作用以及层级表示如何支持复杂变换，同时介绍了最新的推理范式。

Conclusion: 本文统一了隐式推理的理论框架，展望了未来大型语言模型认知研究的新方向，并提供了相关资源库便于学术交流和进一步研究。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, especially when guided by explicit chain-of-thought (CoT)
reasoning that verbalizes intermediate steps. While CoT improves both
interpretability and accuracy, its dependence on natural language reasoning
limits the model's expressive bandwidth. Latent reasoning tackles this
bottleneck by performing multi-step inference entirely in the model's
continuous hidden state, eliminating token-level supervision. To advance latent
reasoning research, this survey provides a comprehensive overview of the
emerging field of latent reasoning. We begin by examining the foundational role
of neural network layers as the computational substrate for reasoning,
highlighting how hierarchical representations support complex transformations.
Next, we explore diverse latent reasoning methodologies, including
activation-based recurrence, hidden state propagation, and fine-tuning
strategies that compress or internalize explicit reasoning traces. Finally, we
discuss advanced paradigms such as infinite-depth latent reasoning via masked
diffusion models, which enable globally consistent and reversible reasoning
processes. By unifying these perspectives, we aim to clarify the conceptual
landscape of latent reasoning and chart future directions for research at the
frontier of LLM cognition. An associated GitHub repository collecting the
latest papers and repos is available at:
https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [274] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
*Ayush Parikh,Hoang Thanh Thanh Truong,Jeanette Schofield,Maximilian Heil*

Main category: cs.CL

TL;DR: 本文介绍了DS@GT团队在CLEF 2025 CheckThat!任务4a中针对科学网络话语检测的三种建模方法，并以0.8611的宏平均F1分数获得第7名。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决多分类任务，判断推文中是否包含科学主张、科学研究或出版物引用，及科学实体提及，以提升科学信息的自动识别。

Method: 采用了三种方法：变换器微调、少样本大模型提示和结合之前实验设计的集成模型。

Result: 团队获得了宏平均F1分数0.8611，优于DeBERTaV3基线的0.8375，排名第7。

Conclusion: 通过多种建模方法的尝试，研究有效提升了科学话语检测的性能，为相关任务提供了实用方案。

Abstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [275] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: 本论文针对大语言模型（LLMs）在检索重排任务中的高计算需求，提出了硬件无关的效率评估指标E²R-FLOPs，从而更准确衡量模型的效率与效果的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM重排序器效率的评价依赖于延迟和计算次数等受硬件影响的指标，难以准确反映模型的计算效率和效果权衡。

Method: 提出了基于每百亿次浮点运算（PetaFLOP）的排序相关指标（RPP）和查询吞吐指标（QPP），并构建了易解释的浮点运算量估算器，支持无需运行模型即可估算计算资源。

Result: 通过E²R-FLOPs指标对多种架构的LLM重排序器进行了全面评估，深入分析了效率与效果的折中关系。

Conclusion: 提出的硬件无关指标有效解决了现有评估指标的不足，推动研究社区更合理地关注LLM在信息检索中的性能与计算资源平衡问题。

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [276] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: Agent KB通过层次结构经验框架和Reason-Retrieve-Refine流程，实现跨代理的知识共享和错误纠正，显著提升了复杂任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 语言代理在处理复杂任务时难以有效纠错和跨领域复用经验，且传统代理缺乏共享学习机制。

Method: 引入Agent KB框架，捕捉高层策略与执行日志，构建共享知识库，利用新颖的Reason-Retrieve-Refine管线促进跨代理经验传递。

Result: 在GAIA基准测试中，成功率提升最高达16.28个百分点；Claude-3和GPT-4在复杂任务中的表现显著提高；在SWE-bench代码修复任务上也有明显提升。

Conclusion: Agent KB为代理提供了一个模块化、框架无关的基础设施，支持代理从过去经验中学习并将成功策略推广到新任务。

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [277] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 本文将图神经网络中过度平滑现象与无序系统中的安德森局域化进行类比，提出参与度指标量化过度平滑，并探讨减少信息传播中的无序性以缓解过度平滑。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络深度增加，节点表示变得同质化，导致特征失去区分性，严重影响模型性能，需要深入理解和解决过度平滑问题。

Method: 通过类比安德森局域化，定义参与度指标以量化过度平滑现象，分析深层传播中节点特征同质化的机理，系统梳理安德森局域化与过度平滑的关系，并提出减少传播无序性以缓解过度平滑的潜在方法。

Result: 理论分析表明，过度平滑可以理解为低频模式扩展和高频模式局域化的现象。参与度指标有效描述了这一过程。减少信息传播的无序性有望缓解过度平滑问题。

Conclusion: 本文提供了过度平滑的物理类比视角及理论框架，开辟了基于信息传播无序控制的新思路，为解决深层图神经网络过度平滑提供了理论支持和潜在方向。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [278] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种基于全局统计的外生输入预处理方法，通过去冗余和增强全局上下文感知能力，提升了时间序列预测的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于transformer的时间序列预测方法通过结合外生输入提升性能，但存在冗余信息和固定回溯窗口限制捕捉长期依赖的问题。

Method: 本文通过对外生输入进行白化处理，降低冗余并增强其对长期趋势的感知能力，且不增加回溯窗口长度，将改进的外生输入与内生输入结合用于预测。

Result: 在四个基准数据集上，所提方法在性能上显著超越11个基线模型，实现了最先进水平。

Conclusion: 所提方法有效缓解了外生输入的冗余问题，增强了长期依赖捕捉能力，成为时间序列预测中结合外生输入的强有力方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [279] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 本文提出了一种利用可解释人工智能（XAI）技术进行深度神经网络（DNN）压缩的新方法，通过计算参数重要性进行剪枝和混合精度量化，实现了显著的模型压缩和准确率提升。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在性能卓越的同时，存在计算和内存资源消耗大的问题，迫切需要高效的压缩技术。XAI技术可以解释DNN内部机制，有助于更精细地进行模型压缩。

Method: 利用基于梯度的XAI方法Layer-wise Relevance Propagation (LRP)计算DNN中权重的重要性分数，剪除重要性为负或零的权重，然后根据权重重要性分数采用混合精度量化；重要权重用更多位数，低重要权重用更少位数量化。

Result: 该方法使DNN模型大小减少64%，且准确率相比现有基于XAI的压缩方法提高42%。

Conclusion: 结合XAI计算权重重要性进行有针对性的剪枝和混合精度量化，是一种有效的DNN模型压缩方法，可在大幅减小模型体积的同时提升性能。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [280] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图神经网络的物理驱动机器学习框架P-DivGNN，用于微观尺度局部应力场的重构。


<details>
  <summary>Details</summary>
Motivation: 微观局部应力场的准确预测对于断裂分析和局部疲劳准则的设定至关重要，传统有限元方法计算复杂且耗时，需寻找有效替代方法。

Method: 通过将周期性微观结构表征为图结构，结合消息传递图神经网络，并在训练中引入物理约束及周期性边界条件，实现局部应力场的预测。

Result: 在考虑线性和非线性超弹性材料响应及不同几何形状时，模型能准确预测局部应力场，且在非线性情况下相比有限元模拟具有显著的计算加速效果。

Conclusion: P-DivGNN有效结合物理信息和图神经网络技术，提升了局部微观应力场模拟的效率和精度，适合大规模多尺度应用。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [281] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经元传递函数变化速率（神经速度）的动态训练方法NeVe，用于调整学习率和定义停止准则，以优化神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 现有超参数调优方法多依赖监控验证损失，需持出数据，且效率有限。

Method: 引入神经速度概念，通过测量神经元传递函数变化速率动态调整学习率和停止准则，且可通过网络噪声采样神经速度，无需用验证集。

Result: 实验表明神经速度可作为有效指标促进训练收敛，提升训练优化效率。

Conclusion: 神经速度为神经网络训练的关键指标，能高效指导超参数调节与训练停止判断，减少对验证集的依赖。

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [282] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件图神经网络(cGNN)的数据驱动模型，用于模拟软组织在虚拟环境中的变形和受力，解决了高变形性软组织模拟的复杂难题。


<details>
  <summary>Details</summary>
Motivation: 软组织具有高度可变形性，传统方法依赖分割、网格划分及刚度参数估计，难以精准模拟软组织变形及力反馈以提升医疗虚拟环境中的沉浸感。

Method: 设计了条件图神经网络(cGNN)模型，输入为表面点及施力位置，预测点的变形和受力。先利用质量-弹簧仿真进行预训练，再用实验收集的软组织表面跟踪数据进行微调，提高模型泛化能力。

Result: 模型对30 mm变形预测的距离误差为0.35±0.03 mm，对7.5 N力的绝对误差为0.37±0.05 N，展示了准确的预测能力。

Conclusion: 此数据驱动方法有效应对软组织模拟难题，不仅适用于医疗模拟，也有望在其他需真实软组织模拟的领域应用。

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [283] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: 该论文首次将无数据神经网络方法应用于资源约束项目调度问题（RCPSP），提出将调度约束转化为可微目标函数的数学框架，实现基于梯度优化和GPU加速。


<details>
  <summary>Details</summary>
Motivation: 尽管无数据神经网络已应用于最大独立集问题，但尚未扩展到RCPSP领域，存在应用空白。

Method: 通过平滑松弛和自动微分技术，将RCPSP的离散调度约束转为可微目标，利用密集时间网格表示实现内存高效，并支持GPU并行计算。

Result: 当前正在PSPLIB基准（J30、J60、J120）上进行实现和实验，实验结果将在后续版本报告。

Conclusion: 该工作填补了无数据神经网络在项目调度领域的空白，展示了基于神经网络的新型调度算法框架的潜力。

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [284] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架HEPO，它通过最大化策略改进而非策略不变，解决了启发式奖励导致的奖励黑客问题，提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，利用启发式奖励编码人类先验知识可以帮助任务优化，但不合理的启发式奖励经常导致性能下降和资源浪费，现有方法依赖策略不变性理论效果有限且表现欠佳。

Method: 提出Heuristic Enhanced Policy Optimization（HEPO）框架，基于最大化策略改进的目标设计，有效利用启发式奖励并避免奖励黑客问题，提升策略性能。

Result: HEPO在标准基准测试中表现优异，即使是不擅长设计奖励的非专家也能通过HEPO获得良好策略表现，显著减少了人工设计奖励的工作量。

Conclusion: HEPO作为一种即插即用的优化方法，有效利用启发式奖励提升了强化学习策略的表现，降低了对人类先验奖励设计的依赖，展示了较强的实用价值。

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [285] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: 该论文提出了一种基于因果推理的基础模型，通过双编码器和结构化对比学习将物理信号与仪器效应分离，显著提升了结构化时间序列数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在结构化时间序列数据中，由于观测数据同时包含真实物理现象和测量仪器引入的系统性扭曲，导致模型难以泛化，尤其是在多仪器环境。

Method: 设计了一个因果驱动的双编码器架构，利用自然存在的观测三元组进行结构化对比学习，分别学习物理信号和仪器效应的隐含表示。

Result: 在模拟天文时间序列数据（如NASA的TESS任务数据）上，该方法在低数据量条件下下游预测任务表现显著优于传统单隐空间基础模型。

Conclusion: 该模型支持基础模型的关键能力，如少样本泛化和高效适应，强调将因果结构编码到表征学习中对于结构化数据的重要性。

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [286] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 本论文比较了监督微调（SFT）和强化微调（RFT）两种持续后训练范式对知识保留的影响，发现在持续学习多模态任务时，RFT能有效避免灾难性遗忘并提升模型的泛化能力，而SFT则表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前持续后训练主要关注数据重放、模型扩展或参数正则化，忽视了学习范式的根本作用，因此研究SFT和RFT对知识保留的不同影响。

Method: 基于Qwen2.5-VL-7B-Instruct模型，在七个多模态任务集上比较SFT与RFT的表现，分析其对知识遗忘和模型能力变化的影响，同时提出基于rollout的实例过滤算法提升RFT性能。

Result: SFT导致灾难性遗忘和模型能力下降，RFT不仅保留了先前知识，还提升了模型在标准基准上的表现。隐式正则化是RFT的主要优势，显式机制作用不大。

Conclusion: 强化微调（RFT）因其隐式正则化效果显著，是持续后训练中更优越且稳定的学习范式，适合多模态大模型的持续适应。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [287] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 本文提出了一种名为PT-LiRPA的新框架，通过结合LiRPA方法的过度逼近与采样方法，紧致神经网络输出的线性界限，提高形式化验证的效率和置信度。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiRPA的形式化验证方法存在计算开销大且边界松散的问题，迫切需要一种既能保证验证严谨性又能减少计算成本的方法。

Method: PT-LiRPA结合过度逼近技术和采样估计中间可达集，通过概率化方法显著收紧神经网络输出的上下线性界限，提升验证工具的效率和精准性，同时保证验证结果的概率性正确性。

Result: 在国际神经网络验证竞赛等标准测试集中，PT-LiRPA的验证器使得鲁棒性证书提升了3.31倍和2.26倍，且在传统方法失败的挑战性实例中也能以不低于99%的置信度给出答案。

Conclusion: PT-LiRPA是一种有效且实用的神经网络形式化验证框架，大幅提升了验证性能和结果的置信度，特别适用于处理复杂和高难度的验证任务。

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [288] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: AXLearn是一个高性能、可扩展的大型深度学习训练系统，强调模块化设计和异构硬件支持。


<details>
  <summary>Details</summary>
Motivation: 为了实现深度学习模型训练的高效扩展和简化异构硬件上的开发工作，设计一种具备高模块化和低复杂度的系统。

Method: AXLearn采用严格封装的软件组件接口，支持模块组合和快速实验，并提出通过代码行数复杂度来量化模块化，展示系统复杂度随组件规模保持不变。

Result: AXLearn在不同组件扩展时保持代码复杂度恒定，实现了如RoPE功能的高效集成，同时性能与现有顶尖训练系统相当。

Conclusion: AXLearn凭借其模块化设计和对异构设备的支持，实现了高效、灵活的大规模深度学习训练，显著降低了系统复杂度，提高了开发效率。

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [289] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 本论文针对因果相关潜变量的鲁棒判别表示学习问题，利用干预数据强化模型以应对干预分布变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了干预所导致的因果独立关系，导致在干预数据上预测性能差异大，尤其当干预样本有限时。

Method: 识别性能差异与干预因果模型所诱导的独立性条件的强相关性；为线性模型推导干预数据比例的充分条件；提出RepLIn算法，显式约束干预下的统计独立性。

Result: 在合成数据和图像、文本真实数据集（面部属性分类、毒性检测）上验证了RepLIn算法，展示其对连续和离散潜变量干预分布偏移的鲁棒性提升效果，且具备良好扩展性。

Conclusion: 通过显式利用因果干预信息强制潜变量表示的独立性，RepLIn有效缓解干预样本稀缺问题，提高模型在干预分布上的预测性能和鲁棒性。

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [290] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: 本文提出两种基于深度学习的新型模型（EmissionNet和EmissionNet-Transformer）用于预测农业源的N2O排放，改善传统物理模型在捕捉复杂污染物交互上的不足。


<details>
  <summary>Details</summary>
Motivation: 农业排放导致的空气污染对环境和公共健康有重要影响，但传统基于物理的空气质量预测模型难以有效捕捉复杂的非线性污染物相互作用。

Method: 设计并评估两种新颖的深度学习模型EmissionNet和EmissionNet-Transformer，利用卷积和基于Transformer的架构提取高分辨率排放数据中的时空依赖关系。

Result: 通过评估，所提出的模型在捕捉农业N2O排放的时空特征方面表现优异，优于传统物理模型。

Conclusion: 新型深度学习架构有效提升了农业排放N2O的预测性能，为空气质量预测提供了更准确的工具。

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [291] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: 该论文提出了一种针对公司财务报告欺诈的多目标攻击方法MVMO，能够显著提高欺诈成功率，使公司在约50%的情况下能将收益虚增100-200%，同时降低欺诈得分15%。


<details>
  <summary>Details</summary>
Motivation: 坏行为者尤其是陷入困境的公司有动力操纵财务报告以隐藏困境并获取私利，现有攻击方法难以满足多重反向目标的需求。

Method: 提出最大违反多目标(MVMO)攻击，调整攻击者搜索方向，提高攻击成功的满足率达20倍。

Result: 实验证明MVMO在约50%情况下能同时大幅虚增收益与降低欺诈评分。

Conclusion: 通过与律师和专业会计师合作，模型建立在现实可行的欺诈情景基础上，MVMO有效提升财务欺诈攻击的成功率和隐蔽性。

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [292] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: 本文提出了一种统一的分布式多步强化学习框架Horizon-DQN，专注于解决延迟且稀疏奖励问题，在2048游戏中显著提升了得分表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在延迟和稀疏奖励环境中难以有效赋予动作价值，如2048游戏中的局部最优困境。

Method: 提出了Horizon-DQN，结合分布式学习、决斗网络、噪声网络、优先经验重放等技术，并与DQN、PPO、QR-DQN进行了对比。

Result: Horizon-DQN在2048游戏中取得了最高分18.21K，扩展后更是达到41.828K，显著优于其他算法。

Conclusion: 分布式多步目标极大提升了稀疏奖励场景下的强化学习性能，未来可结合模型规划和课程学习进一步提升。

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [293] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: 本文提出了一种基于前向-后向表示（FB）的零样本强化学习探索策略，通过最小化FB表示的后验方差，提升探索效率和样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本强化学习算法在采集数据时依赖其他探索方法，未充分利用FB表示进行高效探索。

Method: 设计了一种从FB表示自然产生的探索策略，目标是最小化FB表示的后验方差以降低不确定性。

Result: 实验证明，该探索策略显著提升了FB算法的样本效率，优于其他探索方法。

Conclusion: 利用FB表示本身进行探索能更有效地学习最优策略，提高零样本强化学习算法的性能。

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [294] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: 本文研究线上凸优化中的动态遗憾问题，将动态遗憾最小化转化为函数空间的静态遗憾问题，利用再生核希尔伯特空间（RKHS）构造，实现了最优的动态遗憾界，并扩展到非线性损失情况。


<details>
  <summary>Details</summary>
Motivation: 现有动态遗憾最小化方法多仅适用于线性损失，且动态与静态遗憾的转换有限，亟需一种通用方法能处理任意损失函数并提供优良的动态遗憾界。

Method: 通过将比较任意比较器序列的问题视为在函数空间中与固定比较器函数竞争的问题，构建适当的RKHS函数空间，完成从动态遗憾到静态遗憾的归约，从而获得更广泛和精细的遗憾界。

Result: 在此框架下，重现了线性损失下最优动态遗憾界$\mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$，并获得了新的无尺度和方向自适应遗憾界。同时，适用于任意损失序列，在指数凹和不适当线性回归中得到复合遗憾界。算法实际可计算。

Conclusion: 本文提出的基于RKHS的动态到静态遗憾归约方法不仅恢复和提升了已有在线优化动态遗憾的界限，还实现了对非线性损失的适用性，具有重要理论推动及实际意义。

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [295] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 提出了无训练的扩散引导框架SDG，利用代理随机最优控制目标和Stein变分推断来校正近似后验，提高扩散模型采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有随机最优控制虽有效但计算昂贵，训练无关方法准确性不足，需结合两者优势提升扩散模型引导效果。

Method: 构建代理随机最优控制目标，推导值函数理论界限，利用Stein变分推断最小化KL散度对近似后验进行校正，设计新的运行代价函数实现低密度区域有效引导。

Result: 在分子生成等困难任务上，SDG显著优于传统无训练引导方法，表现出更高的采样质量和引导可靠性。

Conclusion: 该方法为无训练扩散模型引导提供了理论基础及有效机制，兼具准确性与效率，具备广泛应用潜力。

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [296] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: 该文提出了一种名为Ex-HiDeNN的可解释层次深度学习神经网络方法，结合符号回归，从有限数据中高效发现准确且可解释的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 当前利用数据驱动方法构建复杂函数关系虽有进展，但如何高效发现准确且解释性强的闭式表达式仍具挑战。

Method: 提出了两步算法Ex-HiDeNN，结合可分性检查器，采用节约资源、分离性强且可扩展的神经网络架构与符号回归进行表达式发现。

Result: Ex-HiDeNN在多个基准测试及三个工程实例中表现优异，误差显著低于传统符号回归及参考方法。

Conclusion: Ex-HiDeNN在表达式发现方面具备卓越性能，且可解释性强，未来有望扩展应用领域并克服当前限制。

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [297] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络的校园建筑占用率和建筑间动态流动预测模型，结合Wi-Fi日志和使用时间表，保护隐私且效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 通过利用Wi-Fi数据结合建筑使用时间表，准确预测校园内人员流动，提升场景建模效率且保证隐私安全。

Method: 构建以建筑为节点的图结构，采用图卷积与LSTM结合的GCLSTM模型学习复杂流动模式，实现基于数据驱动的流量估计。

Result: 模型在多伦多都会大学的实际Wi-Fi数据上测试，性能显著优于多层感知器和线性回归等传统预测方法。

Conclusion: 提出的集成GCLSTM架构有效捕捉建筑间人流动态，具备较高预测精度并保障用户隐私，适合实际校园智能管理应用。

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [298] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多层蒙特卡洛（MLMC）压缩方案，利用有偏压缩器构建统计无偏估计，兼顾有偏和无偏方法的优势，提升分布式学习中的通信效率。


<details>
  <summary>Details</summary>
Motivation: 分布式学习中通信开销成为瓶颈，传统梯度压缩方法在有偏和无偏压缩器之间权衡效率和理论保证。

Method: 引入多层蒙特卡洛压缩方案，结合常用的Top-k和逐位压缩器，提出增强版本并设计自适应变体。

Result: 在分布式深度学习任务中验证了该方法的有效性。

Conclusion: 该方法有效弥合了有偏和无偏压缩技术的差距，提升了分布式学习的通信效率和性能。

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [299] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的用户增长营销优化方法，通过直接建模关键业务指标的提升效果，提升活动效果并降低成本。


<details>
  <summary>Details</summary>
Motivation: 用户增长是消费互联网公司的核心战略，传统方法难以高效优化营销投入与用户参与，需新方法提升营销的ROI。

Method: 利用深度学习从历史实验学习，通过软最大门控联合优化聚合损失函数，直接优化关键业务指标的提升效果，并处理复杂业务约束。

Result: 该方法在与R-learner和Causal Forest等先进技术比较中，性能提升超过20%，在成本效益和实际应用中表现优异。

Conclusion: 提出的深度学习优化算法在多种产品及情境中表现出高度灵活性和优越性能，已成功实现全球部署，具备广泛应用价值。

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [300] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的新方法，通过构建神经增强朴素贝叶斯层，实现对多因素异质性治疗效应的建模和排序。


<details>
  <summary>Details</summary>
Motivation: 随着异质性治疗效应评估应用的普及，治疗操作空间从二元变量扩展到包含连续治疗强度和离散治疗分配的复杂策略，亟需有效的建模方法。

Method: 作者从基本原理出发，推导了多政策变量纳入个体及平均治疗效应函数的公式，设计了一个神经增强朴素贝叶斯层来满足贝叶斯假设，并将其集成于深度学习框架中处理连续和离散治疗变量，直接对聚合治疗效应函数进行排序。

Result: 在公开数据集上，所提方法表现出显著的性能提升，验证了其在多因素异质性治疗政策学习中的有效性。

Conclusion: 本文构建了一个通用的深度学习异质治疗政策框架，能够灵活处理复杂治疗因素，为相关领域的HTE估计提供了强有力工具。

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [301] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: 本文提出了一种基于元学习的因果推断模型MACE-TNP，用于在结构不确定性较大的情况下进行贝叶斯模型平均，多于传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 在缺乏因果结构先验知识时，单一结构推断容易过于自信，需要一种能够处理结构不确定性的贝叶斯方法，但传统计算代价高。

Method: 提出了一种端到端的元学习模型——MACE-TNP，训练其预测贝叶斯模型平均的干预后验分布，避免昂贵计算。

Result: 实验证明MACE-TNP优于强贝叶斯基线方法。

Conclusion: 元学习为近似复杂贝叶斯因果推断提供了灵活且可扩展的范式，适用于未来更复杂场景。

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [302] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: 该论文提出了一种名为InterpoLL的方法，通过插值大多数类样本的表示，引入少数类样本的特征，减弱模型对捷径的依赖，提高了模型在少数类样本上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的经验风险最小化（ERM）方法使模型过度依赖于输入与标签之间的捷径相关性，导致模型在少数类样本上泛化能力不足。现有方法存在针对性强、调参困难、计算复杂且无法改善学习表示的问题。

Method: 提出InterpoLL方法，通过在表示空间中插值大多数类样本与少数类样本的特征，减弱捷径特征的影响，使模型能够学习对少数类和多数类均有效的特征。

Result: 在多项自然语言理解任务上，InterpoLL在不损失多数类样本准确率的前提下，显著提升了少数类样本的泛化性能，效果优于ERM和最新捷径缓解方法。该方法适用于多种模型架构，包括编码器、编码器-解码器和仅解码器架构。

Conclusion: InterpoLL是一种通用且高效的捷径缓解策略，通过特征插值得到更鲁棒的表示，提升了模型对少数类样本的适应能力，具有广泛的应用前景。

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [303] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 本文提出了一种针对图神经网络（GNN）的硬件故障攻击方法——渐进式翻转位故障攻击（GBFA），通过逐层选择易受攻击的权重位进行bit翻转，显著降低模型预测准确率。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器在提升GNN性能中被广泛采用，但对硬件故障攻击的安全问题缺乏关注，攻击者可能通过注入故障修改权重参数，导致GNN输出误分类。

Method: 提出GBFA方法，首先通过Markov模型预测各层执行顺序，实现对特定层的攻击；然后通过梯度排序搜索该层内易被攻击的权重位，逐渐翻转这些位以最小修改量破坏模型性能。

Result: 在Cora和PubMed数据集上的多种GNN模型（如GraphSAGE）验证显示，GBFA显著降低预测准确率，如在Cora数据集最后一层仅翻转单个位即可导致准确率下降17%。

Conclusion: GBFA的层感知攻击策略有效揭示了GNN模型对硬件故障攻击的易感性，强调在安全防护设计中需关注不同网络层的脆弱性。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [304] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: 本文首创性地理论分析了结合跳跃连接和图稀疏化的图卷积网络（GCNs）的学习动力学和泛化性能，并验证稀疏有效邻接矩阵对泛化精度的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究在理论上缺乏对跳跃连接和图稀疏化联合使用时GCNs泛化性能的分析。

Method: 提出理论框架分析使用跳跃连接和图稀疏化的GCNs的学习动态和泛化，定义稀疏有效邻接矩阵$A^*$，探讨不同层的稀疏需求。

Result: 发现泛化性能依赖于$A^*$是否保留关键边，跳跃连接导致不同层对稀疏化的敏感度不同，一层的偏差比第二层影响更大。

Conclusion: 跳跃连接影响GCNs的稀疏化需求，其理论分析首次揭示了跳跃连接在保持泛化性能中的作用，实验验证了理论有效性。

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [305] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: 该论文提出了一种名为LSC-GNN的新型图神经网络，通过引入外部“干净”边约束潜在空间，改善了噪声边对图神经网络的影响，在多种基准数据集和异构图中表现优异。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在存在噪声边时性能下降，如何有效利用外部“干净”边来提升模型鲁棒性是该工作的动机。

Method: 设计了两个编码器，一者基于完整图（包含目标图和外部边），另一者基于不含目标图潜在噪声边的正则化图，通过惩罚两个编码器潜在表示的差异来避免过拟合噪声边。

Result: 在基准数据集和蛋白质-代谢物异构图上，LSC-GNN优于传统及抗噪声图神经网络，显著提升了预测性能和模型解释性。

Conclusion: LSC-GNN通过潜在空间约束，有效抵抗了图中的噪声边干扰，提升了图神经网络在噪声关系结构下的表现和可解释性。

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [306] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: 该论文提出了一种结合辅助信息的负载估计框架，用于通过可穿戴传感器收集的步态数据评估搬运负载，提高了泛化能力和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法单纯依赖负载步态与手部负载的直接映射，导致泛化能力和预测准确性不足，且携带方式信息难以获取限制了模型性能。

Method: 引入辅助信息（无负载基线步态和携带方式），结合深度潜变量模型、时间卷积网络和双向交叉注意力机制，融合负载与无负载步态数据，设计无须推断时携带方式标签的条件负载估计模型。

Result: 实验基于实测惯性测量单元数据，显示引入辅助信息显著提升了负载估计准确性，并强调了显式融合机制优于简单特征拼接的重要性。

Conclusion: 该框架有效整合辅助信息提升了负载估计性能，解决了携带方式信息获取难题，具备较强的实用价值和推广潜力。

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [307] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 本文提出多任务预演算法，通过在一个任务上的经验预先学习未执行但可进入的其他任务，支持快速适应和泛化。


<details>
  <summary>Details</summary>
Motivation: 人类虽能执行无限多任务，但同时只能关注少数任务，假设人类利用一个任务的经验预学习其他未执行任务的解决方案。

Method: 提出多任务预演算法，通过在一个任务的经验进行反事实模拟（预演）学习预测性表征，支持后续任务的快速适应。

Result: 多任务预演在小型网格世界和部分可观测的2D Minecraft环境中，比传统方法更好地预测了人类对未执行任务的泛化能力，并促进人工智能代理在新环境中转移学习。

Conclusion: 多任务预演为人类如何跨多个任务进行反事实学习和泛化提供了可扩展理论，赋予人工代理此能力可显著提升其多任务环境中的表现。

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [308] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文综述了大规模语言模型（LLMs）中训练数据记忆现象的现状，探讨了其驱动因素、检测方法、影响及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 深入理解LLMs的记忆机制，揭示其行为、隐私风险以及学习与记忆的界限。

Method: 综合了近期相关研究，分析了数据重复、训练动态、微调等影响因素，评估了基于前缀提取、成员推断和对抗提示等检测技术，并讨论了法律伦理问题和缓解措施。

Result: 系统整理了影响记忆现象的关键因素和检测方法，总结了隐私风险及法律伦理影响，提出了数据清洗、差分隐私和去学习等缓解策略。

Conclusion: 该综述为理解LLMs记忆现象提供了全景视角，强调在降低有害记忆与保持模型性能之间的权衡，并指出未来研究方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [309] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: 本文提出了一种基于近端策略优化（PPO）的无模型强化学习方法，用于原位训练衍射光学处理器，提高了训练速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统光学计算中的衍射层优化受到硬件缺陷、噪声和误差的影响，且现有方法收敛慢、性能不稳定，难以有效利用有限测量数据。

Method: 采用基于PPO的无模型强化学习方法，直接在物理系统上训练，通过高效复用测量数据和限制策略更新，实现更快、更稳定的收敛。

Result: 该方法在多种任务中（如能量聚焦、全息图生成、像差校正和光学图像分类）均表现出更好收敛性和性能，并消除了对系统建模的需求。

Conclusion: 该原位强化学习策略能处理复杂反馈动力学下的物理系统和光学系统，实现快速准确的训练，具备良好可扩展性。

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [310] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: 本文提出了一种统一的傅里叶谱变换器网络，结合谱方法和注意力神经网络，解决偏微分方程长期预测问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法和机器学习方法对复杂动力系统的长期预测效果有限，亟需高精度且高效的预测模型。

Method: 将偏微分方程转化为谱空间的常微分方程，利用高精度数值解算器生成训练数据，并用Transformer网络预测谱系数的演化。

Result: 在二维不可压Navier-Stokes方程和一维Burgers方程实验中，方法在有限训练数据下实现了高精度长期预测，优于传统数值和机器学习方法。

Conclusion: 该谱傅里叶变换器框架对未知数据具备良好泛化能力，为复杂动力系统的实时预测与控制提供了有前景的方法。

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [311] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 本文比较了基于特征和基于GAN的示范学习方法，分析了奖励函数结构及对策略学习的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨不同示范学习方法中奖励结构对策略学习效果的影响及其适用场景。

Method: 分析特征方法提供密集且解释性强的奖励，适合高保真运动模仿；GAN方法通过隐式分布监督实现可扩展性和灵活适应，但训练不稳且奖励粗糙。

Result: 两种方法各有优势与局限，最新进展强调结构化运动表示，改善切换平滑性和任务集成。

Conclusion: 方法选择应基于具体任务需求，如保真度、多样性、解释性和适应性，兼顾算法权衡与设计考量。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [312] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文针对强化学习中的奖励欺骗问题进行了大规模实证研究，提出了自动检测方法并在多环境多算法中验证。


<details>
  <summary>Details</summary>
Motivation: 奖励欺骗严重威胁自主智能体的实际部署，现有检测和缓解方法有限，需要系统的研究和有效工具。

Method: 分析了15个强化学习环境和5种算法的15247个训练回合，设计了覆盖六类奖励欺骗的自动检测算法，并在三个应用场景中进行了验证。

Result: 检测框架在多环境中达到78.4%的准确率和81.7%的召回率，计算开销小于5%。奖励密度和目标一致性显著影响欺骗频率。缓解方法在控制场景中将欺骗率降低了54.6%。

Conclusion: 本文提供了系统的检测与缓解方法，数据和工具公开，促进强化学习安全研究，但实际应用仍面临概念漂移和对抗性适应等挑战。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [313] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 本文提出了一种名为Uncertainty-Aware RL (UARL)的新型强化学习框架，通过不直接与目标环境交互来提高策略的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署强化学习策略面临分布偏移、安全性和政策优化直接交互不可行等挑战，现有方法通过直接与目标域交互提高鲁棒性存在安全隐患。

Method: UARL利用多个评论家评价策略不确定性，结合逐步环境随机化技术，在模拟环境中针对高不确定性区域进行迭代优化，实现无须直接交互的策略适应和OOD检测。

Result: UARL在MuJoCo基准测试和四足机器人上表现出可靠的OOD检测能力、提升的性能和更高的样本效率，优于基线方法。

Conclusion: UARL框架有效提升了强化学习策略在真实世界环境中的安全性和泛化能力，避免了直接目标域交互带来的风险。

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [314] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: 本文综述了图学习的发展及其在机器学习中的重要性，涵盖了其在可扩展性、时间动态、多模态、生成模型、可解释性及责任AI等方面的最新进展。


<details>
  <summary>Details</summary>
Motivation: 图学习能够有效建模复杂的非欧几里得关系，克服传统机器学习的局限，支持药物发现、欺诈检测等实际应用，但面临可扩展性、泛化能力等挑战。

Method: 系统回顾了图学习的关键技术，包括大规模图处理、动态时间依赖、多模态数据融合、生成模型、可解释方法及伦理问题，如隐私和公平性。

Result: 总结了图学习领域的最新技术进展和实际应用，指出了跨AI范式整合的新兴主题，展望了未来研究方向。

Conclusion: 通过全面综述，推动研究者理解和应用图学习技术，促进该领域的可持续和负责任发展。

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [315] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: 本文提出了收敛特征定理(FACT)，揭示了在带非零权重衰减下神经网络权重收敛时的自洽关系，并基于该定理设计了FACT-RFM算法，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习理论中的核心挑战是理解神经网络如何学习和表示特征。

Method: 证明了FACT，建立权重矩阵与输入特征及梯度之间的关系，并基于此修改递归特征机(RFM)构建FACT-RFM算法。

Result: 通过实验证明神经网络特征满足FACT，还展示了FACT-RFM在表格数据上表现优良，并能模拟真实训练中的多种特征学习现象。

Conclusion: FACT揭示了神经网络特征学习的内在规律，FACT-RFM提供了一种有效且解释性强的学习算法，促进了特征学习理论与实际应用的结合。

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [316] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: 该论文利用可穿戴惯性传感器和深度学习模型，实现了犬类步态的神经性和骨科性异常的高精度分类。


<details>
  <summary>Details</summary>
Motivation: 临床上神经性和骨科性步态异常难以区分，现有方法不足，需借助惯性传感器和深度学习提升诊断准确性。

Method: 使用惯性传感器数据，优化传感器配置和深度学习模型结构，完成多类别（健康/骨科/神经）及二类别（健康/非健康）分类任务。

Result: 在含29只犬的数据集上，方法在多类别分类中达96%准确率，在二类别分类中达82%准确率，体现了良好的泛化能力。

Conclusion: 基于惯性传感器的深度学习模型能为犬类步态的神经性及骨科性异常诊断提供有效且客观的辅助，具有临床应用潜力。

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [317] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种智能客户端与专家模型动态匹配的系统设计，旨在提升联邦学习中混合专家架构大规模模型的训练效率与通信效率。


<details>
  <summary>Details</summary>
Motivation: 目前联邦学习与混合专家架构结合存在系统层面挑战，尤其是如何在异构客户端资源与复杂专家协调间实现高效分配尚无量化方案。

Method: 提出包含动态适应评分、全局专家负载监控及客户端容量描述的智能客户端-专家匹配系统设计。

Result: 该设计有助于实现更大规模、更高效更鲁棒的训练机制，减少收敛所需通信轮数。

Conclusion: 通过解决系统性匹配和负载均衡问题，促进大规模联邦MoE模型在边缘计算中的高效部署与通信节省。

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [318] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 本文提出了AutoTriton，一种基于强化学习的Triton编程模型，自动优化高性能深度学习内核。


<details>
  <summary>Details</summary>
Motivation: 深度学习内核开发需在硬件上权衡内存、并行性及硬件优化，传统方法需手动调参，门槛高，效率低。

Method: AutoTriton通过监督微调获得Triton编程基础知识，随后采用组合规则和执行奖励的Group Relative Policy Optimization强化学习算法进一步提升性能。

Result: 在TritonBench和KernelBench的五个评测通道上，AutoTriton实现了与主流大型模型（如Claude-4-Sonnet、DeepSeek-R1-0528）相当的性能。

Conclusion: 强化学习可有效自动生成高性能内核，推动了深度学习系统核心组件自动化优化，为构建更高效的AI系统奠定重要基础。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [319] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文提出了MobileGUI-RL框架，通过在线环境训练视觉GUI代理，提升了任务执行的可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉GUI代理大多在离线环境中训练，依赖预先收集的数据，导致模型对特定UI模板过拟合，缺乏对未知环境的适应能力。

Method: 提出MobileGUI-RL框架，包含自我探索生成的任务课程和改进的GRPO算法，引入轨迹感知优势和复合奖励，平衡任务成功率与执行效率。

Result: 在三个在线移动代理基准测试中，MobileGUI-RL实现了性能的一致提升，验证了其有效性。

Conclusion: 通过在线训练和任务课程设计，MobileGUI-RL显著提升了视觉GUI代理的泛化能力和操作效率，具有较强的应用前景。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [320] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: 本论文提出了一种基于部分卸载的双层无人机辅助边缘计算架构，协调不同高度无人机的计算与中继能力，优化系统延迟和能耗，确保任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有无人机辅助卸载策略难以有效协调异构计算资源及适应动态网络环境，限制了车辆网络中计算密集型和延迟敏感应用的性能。

Method: 设计双层架构，结合高空无人机的中继能力和低空无人机的计算支持，建立联合延迟与能耗优化模型，将问题转化为马尔可夫决策过程，并基于软演员-评论家算法提出分层卸载方案，实现全局连续动作和局部优先调度的解耦。

Result: 仿真结果显示该方案在任务完成率、系统效率和收敛速度方面优于多种基线方法，具备良好的鲁棒性和动态环境适应性。

Conclusion: 所提双层无人机辅助架构有效协调异构资源，实现了动态车辆环境下计算卸载的性能提升，具备较强的实用潜力。

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [321] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 本文提出了一种基于多层感知机的天气预测模型WeatherMixer和一种新型模型并行方案Jigsaw，实现了高效、大规模的气象预测训练。


<details>
  <summary>Details</summary>
Motivation: 传统气象预测模型受限于加速器内存和I/O带宽，难以处理高空间分辨率和长预报时间的海量数据。

Method: 设计了线性复杂度的WeatherMixer模型和结合域并行与张量并行的Jigsaw并行方案，减少了内存冗余，提高了计算通信效率。

Result: 在256 GPU上训练获得9和11 PFLOPs的峰值性能，分别达到理论峰值的23%和28%，并实现了68%和72%的扩展效率，显著优于无模型并行的51%。

Conclusion: WeatherMixer结合Jigsaw模型并行技术有效突破了内存和带宽瓶颈，实现了高精度和大规模气象预测训练，提升了模型性能和扩展性。

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [322] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: 该研究结合深度学习图像配准与物理信息正则化，预测心脏组织的生物力学属性并进行疾病分类，显示出高准确率。


<details>
  <summary>Details</summary>
Motivation: 心脏病具有高发病率和致死率，亟需准确及时的诊断方法。

Method: 利用Neo-Hookean材料的能量应变模型进行心脏组织形变建模，通过深度学习优化变形场，保证物理和生物力学一致性，结合多种分类算法进行疾病分类。

Result: 在ACDC数据集上，图像分割Dice系数分别达0.945、0.908和0.905，分类准确率训练集98%、测试集100%。

Conclusion: 该方法通过可解释人工智能提高诊断准确性和透明度，有助于个性化心脏病患者的有效治疗。

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [323] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: 本文提出了一种结合时间序列预测和基于线性规划的通量平衡分析(FBA)方法来预测动态网络中图结构的变化，克服了现有方法对顶点不变的假设。


<details>
  <summary>Details</summary>
Motivation: 现有图预测方法假设连续图之间顶点不变化，这在实际动态网络中不成立，限制了预测能力。

Method: 结合时间序列预测技术和适应性的通量平衡分析(FBA)，利用FBA处理增长图的各种约束，实现动态图结构的有效预测。

Result: 在合成数据集（基于偏好连接模型）和真实数据集（UCI Message、HePH、Facebook、Bitcoin）上的实证评估，验证了方法的有效性。

Conclusion: 提出的方法有效突破了顶点不变限制，提升了动态图预测的精度和适用范围，对于异常检测等应用具有重要意义。

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [324] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出了一个新的物理推理基准ABench-Physics，用以评估大型语言模型（LLMs）在物理领域的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在物理领域的能力尚未充分探索，现有基准测试难度有限且评价方式不全面，无法充分反映模型的物理建模能力。

Method: 设计了包含静态和动态两部分的问题集，分别为400个高难度静态题目（Phy_A）和100个带自动变体生成的动态题目（Phy_B），要求模型给出精确的数值答案，严格限定格式和误差范围。

Result: 对多种先进的大型语言模型进行测试，发现其在物理推理尤其是对动态变体的泛化能力上存在显著差距和不足。

Conclusion: ABench-Physics作为一个具有挑战性和诊断性的评测框架，有助于推动大型语言模型在科学推理领域的发展。

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [325] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: 本文提出了Soup-Adapter，通过训练多个具有不同超参数的独立适配器并对其输出进行平均，提升了小样本领域适应中基础模型的性能和鲁棒性，同时解决了超参数调优困难和分布变化适应问题。


<details>
  <summary>Details</summary>
Motivation: 解决小样本领域适应中缺乏大规模验证集导致超参数调优困难，以及模型对测试时分布偏移的鲁棒性不足。

Method: 训练多个独立适配器，使用多样化超参数，平均其输出形成集成模型。该集成模型可通过参数连接重新表示为单个适配器，称为Soup-Adapter。同时首次将CLIP适配器技术用于DINOv2并进行直接比较。

Result: 集成模型在性能和对分布变化的鲁棒性上均优于任一单独适配器，且对关键超参数残差比敏感度明显降低。

Conclusion: Soup-Adapter方法有效解决了小样本领域适应中的超参数调优和分布变化鲁棒性问题，同时简化了模型复杂度，提升了CLIP和DINOv2适配器的实用性。

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [326] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: 本文提出了一种基于概念的机械可解释性分析框架及交互式工具BAGEL，通过构建知识图谱全局分析神经网络内部语义概念的传播和交互，揭示模型决策机制与数据偏差影响。


<details>
  <summary>Details</summary>
Motivation: 传统的概念可解释方法多集中于局部预测解释，缺乏对模型全局行为和机制的理解，需要一种能够系统分析模型内部语义表示及其流动的全局框架。

Method: 构建一个模型无关的框架，定量分析语义概念在不同层的表达情况，揭示潜在的神经回路和信息流；开发可视化平台BAGEL，以结构化知识图展示概念与类别关系，支持用户互动探索。

Result: 成功实现了对模型内部语义概念的全局解剖，能够识别伪相关关系并帮助理解模型泛化能力及其对数据偏差的敏感性。

Conclusion: 该框架扩展了概念解释方法到机械可解释领域，提升了对深度学习模型决策机制的理解和信任度，对模型调试和改进具有重要作用。

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [327] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: 本文研究了公平域泛化问题，旨在同时最小化目标域的期望风险和公平性违规，提出了基于互信息的理论界限及Pareto优化框架PAFDG，实现优越的效用-公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 当前域泛化方法忽略公平性，而公平性方法忽略域变迁，导致公平性在未知域中难以保持。作者希望同时解决域泛化和算法公平性的问题。

Method: 提出基于互信息的期望风险和公平违规上界，为方法设计提供理论支持；设计了PAFDG框架，通过Pareto优化实现公平与效用的权衡。

Result: 在真实的视觉和语言数据集上，PAFDG在效用-公平性权衡上优于现有方法。

Conclusion: 通过理论分析与实践验证，PAFDG有效桥接了域泛化与算法公平性，实现了在未知域中公平且性能优越的模型。

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [328] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: 本论文提出一种联邦学习框架，利用原型和轻量级适配器模块解决统计异质性和通信开销问题，同时实现模型内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据协同训练中常遇通信负担大及因客户端数据分布不同带来的统计异质性问题。

Method: 通过原型嵌入和轻量级适配器模块替代完整模型权重传输，客户端局部调整模型以对齐类别原型，同时减小通信量并保证模型全局一致性和解释性。

Result: 在真实视网膜眼底图像数据集上进行分类实验，结果显示该方法在提供可解释能力的同时，分类准确率优于基线算法。

Conclusion: 该框架有效缓解联邦学习中的统计异质性和通信负担，提升了模型性能和可解释性，适合实际临床场景应用。

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [329] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络(PINNs)的混合方法，用以提升电力系统状态估计的准确性和鲁棒性，特别是在故障和网络攻击条件下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统在状态估计和实时监控中面临响应速度和准确性的挑战，尤其是在故障状态或网络攻击时。

Method: 通过将物理定律嵌入神经网络结构，采用PINNs方法对电力传输网的状态进行估计。

Result: 实验结果显示，该方法在未见过的数据子集上准确率提高了83%，在全新数据集上性能提升65%，并在数据篡改攻击下比同等神经网络准确率高出93%。

Conclusion: 基于物理信息的神经网络方法显著增强了电力系统状态估计的精度和安全性，具备优于传统机器学习模型的能力。

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [330] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: 提出了一种将表格数据转化为图结构并使用图自编码器生成通用嵌入的方法，用于无预定义任务的下游分析，实验表明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 工业数据库中表格数据应用任务多样且常未预先定义，需一个通用的表格数据嵌入方法以支持多种下游任务。

Method: 将表格数据转化为图结构，利用图自编码器生成实体嵌入，再聚合为每行数据的嵌入；该方法支持相似实体的新样本无额外训练即可嵌入。

Result: 在真实数据集上，该方法较现有通用表格数据嵌入技术表现更优。

Conclusion: 本文提出的图结构和图自编码器结合的两步生成嵌入策略，实现了任务无关的表格数据嵌入，提升了下游任务的效果。

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [331] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为D2C的扩散数据集凝缩框架，通过选择和增强方法，大幅减少训练数据量，实现更快更高质量的扩散模型训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练资源消耗极大，需大量数据和时间，亟需从数据角度减少训练负担。

Method: 提出D2C框架，先通过扩散难度评分和间隔采样选择紧凑多样子集，再附加丰富语义和视觉信息增强子集条件信号。

Result: D2C在多种数据集和模型上均显著加速训练并降低数据需求，使用仅0.8%数据在40k步内达到FID 4.3。

Conclusion: D2C有效压缩扩散模型训练所需数据，极大提升训练效率且保持生成质量，开拓了扩散模型数据凝缩的新方向。

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [332] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: 本文研究了犬类心脏病听诊数据中标签噪声问题，提出通过多专家意见减少标签噪声，并采用XGBoost等算法提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 标签噪声严重影响兽医领域AI模型训练的分类性能，需探索减少标签噪声的方法以提升诊断准确率。

Method: 利用140条犬类心音记录数据，结合多专家评价筛选出70条高质量数据，通过标签噪声减少扩展训练样本，并训练AdaBoost、XGBoost和随机森林三种分类器。

Result: 应用标签噪声减少后，所有分类算法表现显著提升，XGBoost在心脏杂音轻度、中度及剧烈分类中的敏感性和特异性均大幅提高。

Conclusion: 减少标签噪声对于提升犬类心脏杂音检测的机器学习分类准确率至关重要，多专家评估结合先进算法是有效途径。

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [333] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出将Adam优化器视为类似符号的优化器，简化了其收敛性分析，并首次在较弱假设下证明了其收敛速度为O(1/T^{1/4})，优于之前的理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有Adam的理论分析复杂且依赖强假设，收敛性证明难以验证和推广，亟需一种更简洁且有效的分析方法。

Method: 将Adam重构为符号型优化器形式，通过该新视角简化收敛证明，基于一般$p$-仿射方差和$(L_0,L_1,q)$-平滑性假设，推导其收敛率。

Result: 证明Adam在弱假设条件下达到最优收敛率$O(1/T^{1/4})$，且与模型维度和稳定参数$B5$无关，同时揭示了动量在收敛中的重要作用。

Conclusion: 新理论视角不仅简化了收敛性分析，还为动量重要性提供理论支持，且给予实际学习率调整的指导，有助于理论与实践的结合。

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [334] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: KnowIt是一个用于深度时间序列模型构建与解释的灵活Python工具包。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析工具限制较多，难以适应多样化任务，且解释性不足。

Method: KnowIt通过定义统一接口，解耦数据集、网络架构和解释技术，实现灵活导入数据与自定义模型。

Result: 该框架支持多样时间序列数据建模与多角度解释，易于扩展和定制。

Conclusion: KnowIt致力于成为深度时间序列建模与知识发现领域的可信平台，推动该领域发展。

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [335] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: 本文介绍了Kamae，一个开源Python库，用于在生产推荐系统中实现训练与推理环境中一致的特征预处理，通过将PySpark预处理流程转换为等效的Keras模型，避免了重复逻辑和数据偏移风险。


<details>
  <summary>Details</summary>
Motivation: 生产推荐系统中，特征预处理必须在训练和推理环境中保持一致，但通常需要在线下和线上环境中重复实现相同逻辑，这增加了工程工作量并引入了数据偏移的风险。

Method: 提出了Kamae库，通过将PySpark的预处理管道翻译成Keras模型，提供一套可配置的Spark转换器和估计器，这些组件对应于Keras层，实现了机器学习生命周期中一致的端到端预处理。

Result: Kamae框架在实际案例中得到了验证，包括MovieLens数据集和Expedia的排序学习管道，证明了其实用性。

Conclusion: Kamae有效桥接了线下和线上预处理逻辑差异，简化了工程实现并降低了数据偏移风险，提升了推荐系统的生产效率和可靠性。

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [336] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: 本文提出了一种在高维低样本量环境下使用多视图中间融合技术的通用学习方法，通过构造多视图并验证其有效性，解决了维度远大于样本量的问题。


<details>
  <summary>Details</summary>
Motivation: 高维低样本量（HDLSS）环境中，特征维度远远超过样本数量，现有方法面临挑战，迫切需要新的有效学习方法。

Method: 作者提出三种视图构造方法，将高维特征向量拆分成更小的子集，每个子集作为不同的视图，利用多视图中间融合技术进行学习，无需预设固有视图。

Result: 实验在不同模型和任务中广泛验证了所提方法的有效性和泛化能力，显示出良好的表现。

Conclusion: 该方法为多视图中间融合学习在HDLSS环境下的普适优势奠定了基础，促进未来相关研究。

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [337] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: EdgeCodec是一种为风力涡轮叶片气压数据设计的端到端神经网络压缩器，实现高压缩率和低重构误差，支持实时运行和动态码率调整。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮叶片上的传感器产生大量气压数据，需要高效压缩以减少无线传输能耗，延长传感器运行时间。

Method: 设计一个非对称自编码器体系结构，结合鉴别器和残差向量量化器进行训练，以最大化压缩效率，并能动态调整码率。

Result: 实现2560:1到10240:1的压缩率，重构误差低于3%，在GAP9微控制器上实时运行，码率11.25至45比特/秒，动态调整。

Conclusion: EdgeCodec在最高压缩率下，可将无线数据传输能耗降低2.9倍，显著延长传感器单元的使用寿命。

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [338] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 该论文提出了一种结合轻量级数值代理和卷积神经网络的局部-全局卷积神经网络（LGCNN）方法，用于高效模拟具有异质性流动条件的地下水热输运过程。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在科学与工程领域受限于有限或低质量训练数据，难以准确预测复杂的地下水流和热传输过程。传统数值模拟计算成本高，纯数据驱动模型对对流过程的预测鲁棒性差。

Method: 提出LGCNN模型，将全局轻量数值代理与局部卷积神经网络结合，分别模拟运输过程和水流速度及热扩散过程。通过在随机输入场景下分析，并在德国慕尼黑地区实际地下水数据切片中训练，实现对更大区域预测的无须再训练能力。

Result: LGCNN成功模拟了城市规模的复杂地下水温度场，包括多点注入形成的热羽流，展示了对异质地下水流场和热扩散过程的准确建模和良好的泛化能力。

Conclusion: 该方法有效提高了地下水热输运模拟的计算效率和预测精度，解决了传统数值模拟和纯数据驱动模型的缺陷，具有广泛应用潜力。所有数据和代码已开源保证可复现性。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [339] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: 本文研究了结构化状态空间模型（SSM）在资源受限设备上的量化感知训练（QAT）方法及其对模拟内存计算硬件的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对SSM的量化感知训练虽有所探讨，但缺乏针对模拟内存计算芯片等边缘硬件实施细节的分析。

Method: 通过量化感知训练，分析模型规模与数值精度的关系，提升对模拟噪声的鲁棒性，并结合结构剪枝技术，将SSM部署至忆阻式模拟内存计算平台。

Result: QAT可使SSM复杂度显著降低最多两个数量级，同时增强模型对模拟噪声的抵抗力和支持结构剪枝，提升计算效率。

Conclusion: 量化感知训练结合结构剪枝有效促进了SSM在模拟内存计算边缘设备的高效部署，显著优化了计算资源利用率。

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [340] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 本文提出了一种利用潜在空间中推理链条嵌入（CoRE）进行无标签自我评估的方法，从而提升大规模推理模型的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型在推理过程中存在过度思考的问题，导致推理步骤冗余且效率低。如何让模型自主评估自身推理过程的正确性成为关键挑战。

Method: 提出Chain-of-Reasoning Embedding（CoRE）来表示推理过程的隐藏状态，通过分析CoRE的几何特性发现冗余推理表现为循环波动。基于此，设计无训练、无标签的自我评估框架CoRE-Eval，动态判断推理是否可提前结束。

Result: 在数学推理基准（GSM8K、MATH-500、AIME）和不同规模模型（7B至32B）上测试，CoRE-Eval将推理链长度缩短13.7%至33.2%，同时提升结果准确率约10%，32B模型在AIME上达到70.0%的准确率。

Conclusion: 利用潜在空间的推理链嵌入实现无标签自我评估，有效减少冗余推理步骤，提升模型推理效率和准确性，为大规模推理模型的自我监控提供有效途径。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [341] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: 提出了一种基于子空间近似Hessian的零阶优化方法ZO-SAH，通过选择二维子空间拟合二次多项式估计Hessian，显著加速了无梯度信息情况下的优化收敛。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法主要依赖一阶近似，而加入二阶曲率信息虽有助于加速收敛，但高昂的函数评估成本限制了其实际应用。

Method: ZO-SAH方法在随机选择的二维子空间中拟合二次多项式估计Hessian矩阵的二阶系数，并通过周期性切换子空间策略复用函数评估，降低查询成本。

Result: 在八个基准数据集（包括逻辑回归和深度神经网络训练任务）上的实验表明，ZO-SAH相比现有零阶方法实现了显著更快的收敛速度。

Conclusion: ZO-SAH通过在低维子空间中有效估计二阶信息，显著提高了零阶优化的效率，为无梯度优化提供了实用且高效的解决方案。

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [342] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: 本文提出了一种结合多种矩阵分解技术的层次链接预测框架，用于从材料科学文献中推断隐含关联，促进跨学科探索。


<details>
  <summary>Details</summary>
Motivation: 科学文献网络和知识图谱通常大规模、稀疏且噪声多，存在缺失的节点链接，亟需有效方法揭示潜在关联，推动科研发现。

Method: 融合分层非负矩阵分解（HNMFk）、布尔矩阵分解（BNMFk）和逻辑矩阵分解（LMF），构建三层主题树，利用BNMFk与LMF的集成方法结合离散可解释性和概率评分。

Result: 构建了基于73种过渡金属二硫族化合物的文献主题树，模型成功预测并验证了材料与超导主题间的隐含关联，指示了新的跨学科假设。

Conclusion: 所提框架能有效发掘科学文献中隐性连接，尤其适合多元文献资源的交叉分析，并通过交互式平台促进人机协同科学发现。

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [343] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: 本文探讨卷积层中步幅引起的混叠现象及其对数值稳定性和统计泛化的影响，提出了基于框架理论的方法来分析和抑制混叠，确保Parseval稳定性。


<details>
  <summary>Details</summary>
Motivation: 卷积层中使用步幅会引入混叠，影响数值稳定性和模型的泛化能力，目前缺乏针对混叠及其对稳定性的系统分析。

Method: 采用框架理论对1D卷积核中的混叠进行建模，导出稳定性界估计和Parseval稳定性的特征，设计两个高效优化目标抑制混叠，推导随机卷积核下混叠效果的期望和方差表达式。

Result: 提出的方法能有效促进Parseval稳定性，通过系统地抑制混叠提高数值稳定性，解析了初始化时混叠的基本行为。

Conclusion: 基于框架理论的混叠分析方法为设计稳定且具有良好泛化能力的卷积神经网络提供了理论支持和实用优化目标。

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [344] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: 提出了一种部分优化深度可微逻辑门网络连接的新方法，通过在门输入连接子集上使用概率分布选择连接，显著减少逻辑门数量且提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少深度可微逻辑门网络中连接数量，提高模型效率和性能。

Method: 利用概率分布部分优化每个门输入的连接，选择最优连接后确定门类型。

Result: 在Yin-Yang、MNIST和Fashion-MNIST数据集上，优化连接的LGN性能优于固定连接LGN，且逻辑门使用量减少到1/24。

Conclusion: 实现了高效且性能优越的可部分训练布尔逻辑网络，推动深度可微布尔逻辑的发展。

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [345] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 该论文研究了差分设计技术在选择性状态空间架构Mamba中的应用，提出了一种新颖的差分机制，显著提升了模型的检索能力和性能，缓解了注意力过度分配问题。


<details>
  <summary>Details</summary>
Motivation: 序列模型如Transformer和RNN常常将过多注意力分配给无关上下文，导致中间表示噪声增多、模型能力下降，包括幻觉生成、长期依赖和检索能力减弱以及鲁棒性降低。尽管差分设计已在Transformer中得到验证并改善了其表现，尚不清楚此技术能否迁移到效率更高的Mamba架构。

Method: 对差分设计方法进行适配与修改，设计出适合Mamba的新型差分机制，并在语言建模基准上进行实证验证。

Result: 新提出的差分机制相比于原始Mamba模型提升了检索能力和整体性能。

Conclusion: 精心设计的差分机制有效缓解了Mamba模型中过度注意力分配的问题，提升了模型性能和能力，验证了差分设计方法在状态空间模型上的可行性和效果。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [346] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: 本文介绍了联想记忆（如Hopfield网络）作为完全递归神经网络的优雅模型，重点介绍其信息存储能力及其与先进AI架构（如Transformer和扩散模型）的联系。


<details>
  <summary>Details</summary>
Motivation: 近年来联想记忆因其新颖的理论结果和与先进AI架构的关联而受到关注，这为理解传统AI网络计算提供了新的视角。

Method: 本文采用现代语言和方法，结合拉格朗日方法对联想记忆网络进行数学推导，并提供编程实践代码。

Result: 通过拉格朗日新形式设计了强大分布式模型，提升了表示学习能力，推动新架构设计。

Conclusion: 本文以教程形式系统介绍了联想记忆，强调理论与实践结合，为理解和设计现代神经网络提供了理论基础和应用指导。

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [347] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: 该论文提出了一种利用神经网络优化有固定位置的挤压天线（PAs）激活方案，以最大化无线通信速率的方法。


<details>
  <summary>Details</summary>
Motivation: 现代无线通信系统需要灵活、高效且低成本的天线技术，挤压天线由于其动态控制电磁波的特性成为研究热点。

Method: 将天线激活问题建模为组合分数0-1二次规划问题，利用神经网络结合空间特征和信号结构，直接从数据中学习激活策略，并考虑用户位置不确定性进行训练和评估。

Result: 仿真结果表明所提模型在最大化通信速率方面表现有效且具有鲁棒性。

Conclusion: 该研究证明了基于神经网络的数据驱动方法在复杂的天线激活优化问题中具有较好的性能和实用潜力。

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


### [348] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 本文探讨图神经网络（GNN）中因网络深度增加引发的过度平滑问题，提出了参与度作为量化指标，并通过与安德森局域化的类比揭示其机制。


<details>
  <summary>Details</summary>
Motivation: 深层GNN中节点特征趋同，导致表示能力下降，需理解和缓解过度平滑问题。

Method: 通过将过度平滑与物理中安德森局域化现象类比，引入参与度指标，理论分析信息传播中的无序性对过平滑的影响。

Result: 发现过平滑对应低频模式扩展和高频模式局域，揭示信息传播中的无序性是影响过平滑的关键因素。

Conclusion: 减少信息传播中的无序性可缓解过度平滑，提供了新的理解和潜在改进方向。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [349] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 提出一种通过全局统计白化外生输入以减少冗余，并增强其对长期模式感知的时间序列预测方法，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的时间序列预测模型虽能利用外生输入提升性能，但面临冗余信息和固定窗口导致的长期依赖捕捉能力不足的问题。

Method: 通过对外生输入进行全局统计白化处理，减少冗余并增强对长时间趋势的感知，结合内生输入进行建模，无需增加回溯窗口长度。

Result: 在四个基准数据集上，该方法超过11个基线模型，取得了最新的性能表现。

Conclusion: 本方法有效提高了利用外生输入的时间序列预测能力，是一种稳健且高效的替代方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [350] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释人工智能（XAI）的深度神经网络（DNN）压缩方法，通过计算权重的重要性评分，实现剪枝和混合精度量化，有效减小模型大小并提升准确率。


<details>
  <summary>Details</summary>
Motivation: DNN模型虽性能优异，但计算资源和内存需求高，限制了其在边缘设备上的应用。利用XAI理解DNN内部结构，有助于更高效地压缩模型。

Method: 采用基于梯度的Layer-wise Relevance Propagation（LRP）计算权重重要性，剪除负或零评分的参数，且对权重进行混合精度量化，重要性高的使用更多比特位，重要性低的使用更少比特位。

Result: 模型大小减少64%，准确率相较于现有基于XAI的压缩方法提升了42%。

Conclusion: 结合XAI技术的DNN压缩方法能够在显著减小模型体积的同时提升模型性能，证明了XAI在模型压缩领域的有效性。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [351] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的图神经网络框架P-DivGNN，用于多尺度模拟中微观尺度的局部应力场重建，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 多尺度模拟中需要精确获取微观结构的局部应力场，以支持断裂分析和疲劳准则定义，但传统有限元模拟计算代价高昂。

Method: 将周期性微观结构表示为图结构，结合消息传递图神经网络，并在训练中引入物理约束以保证局部应力场的平衡状态，同时利用周期性图表示实现周期边界条件。

Result: 在处理线性及非线性超弹性材料响应时，P-DivGNN能准确预测局部应力场，且在非线性超弹性情况下相比有限元模拟显著加快计算速度。

Conclusion: 所提物理信息图神经网络方法有效且高效，尤其适合大规模多尺度材料模拟中的局部应力场预测。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [352] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经元速率的新动态训练方法NeVe，用以自动调整学习率和停止训练，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统超参数调优依赖于验证损失，存在对验证集需求高及调整不灵活的问题。

Method: 提出神经元速率指标，衡量神经元传递函数变化速率，通过神经元速率动态调整学习率并定义停止条件，且可通过网络噪声采样，无需验证集。

Result: 实验证明神经元速率作为训练收敛指标有效，可高效优化神经网络训练过程。

Conclusion: 神经元速率是优化训练超参数的关键指标，NeVe方法显著提升了训练的自适应性和效率。

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [353] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件图神经网络的数据驱动软组织模拟方法，准确预测组织变形和作用力，提升虚拟环境中软组织仿真效果。


<details>
  <summary>Details</summary>
Motivation: 软组织高变形性和触觉反馈集成对软组织虚拟仿真提出了挑战，现有方法依赖分割、网格划分和刚度估计，力反馈精度不足。

Method: 设计条件图神经网络模型，输入表面点和力施加位置，预测变形点和作用力。利用转移学习，先用质量弹簧模拟训练，再用实验数据微调。

Result: 模型预测变形误差为0.35±0.03毫米，力预测误差为0.37±0.05牛顿，表现出较高的仿真精度和泛化能力。

Conclusion: 提出的方法有效解决了软组织虚拟仿真的复杂性，适用于医学和其他需要真实软组织仿真的领域。

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [354] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: 本文首次提出了资源约束项目调度问题（RCPSP）的无数据神经网络方法，通过将调度约束转化为可微目标，利用光滑松弛和自动微分实现基于梯度的优化和GPU加速。


<details>
  <summary>Details</summary>
Motivation: 尽管无数据神经网络已被应用于最大独立集问题，但尚无研究将其扩展至RCPSP，本文填补这一空白。

Method: 提出完整数学框架，将离散调度约束转化为可微目标，采用光滑松弛和自动微分技术，支持GPU并行计算。

Result: 正在PSPLIB基准测试集（J30、J60、J120）上进行实验，结果将在论文更新版中详细报告。

Conclusion: 该方法为RCPSP引入了无数据神经网络新范式，推动项目调度领域从传统序列算法向并行优化转变。

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [355] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习方法HEPO，旨在利用启发式奖励提升策略表现，同时避免传统方法中因政策不变性导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，利用启发式奖励来编码人类先验知识虽重要，但现有方法往往需要大量人为调整且表现欠佳。

Method: HEPO框架基于最大化策略改进的实用目标，有效结合启发式奖励，规避了传统方法中的奖励操纵问题。

Result: HEPO在标准基准测试中表现优越，即使启发式设计粗糙也能实现良好性能，降低了人工设计奖励的工作量。

Conclusion: HEPO为强化学习中利用启发式奖励提供了实用且有效的新途径，显著提升了策略优化效果并减少了对人类专业知识的依赖。

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [356] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: 本文提出一种因果基础模型，通过双编码器和结构化对比学习，将物理信号与仪器效应解耦，实现对结构化时间序列数据的有效建模。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型难以区分真实物理现象与测量仪器引入的系统性失真，导致模型泛化能力受限，特别是在多样化仪器数据环境中。

Method: 设计因果驱动的双编码器架构，利用自然存在的观测三元组，通过结构化对比学习训练模型，分别学习物理信号和仪器效应的隐藏表示。

Result: 在仿真的天文时间序列数据（例如NASA的TESS任务数据）上，模型在低样本数情况下的下游预测任务中显著优于传统单潜空间基础模型。

Conclusion: 引入因果结构到表示学习，有助于模型实现少样本泛化和高效适应，对结构化时间序列数据建模具有重要意义。

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [357] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 本文比较了监督微调（SFT）和强化微调（RFT）两种持续后训练(CPT)范式对多模态大型语言模型知识保持的影响，发现RFT能有效防止灾难性遗忘并提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前CPT研究主要关注数据重放、模型扩展和参数正则化，忽视了微调范式对知识保留的影响，本研究旨在填补这一空白。

Method: 基于Qwen2.5-VL-7B-Instruct模型，在七个多模态任务上对比SFT和RFT的表现，分析其对知识遗忘和泛化能力的影响，提出基于rollout的实例过滤算法提升RFT的稳定性和效率。

Result: 实验表明SFT导致严重灾难性遗忘和泛化能力下降，而RFT能够保持甚至增强模型对先前知识和通用标准测试的表现。隐式正则化是RFT防止遗忘的关键机制。

Conclusion: 强化微调作为持续后训练中的一种鲁棒范式，优于监督微调，有助于持续学习并保护模型知识和泛化能力。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [358] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 本文提出了PT-LiRPA框架，结合线性松弛和采样方法，有效紧化神经网络输出的上下界，提高了形式验证的效率和鲁棒性证书的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于线性松弛的方法在计算神经网络可达集时松弛较大，导致验证工具开销大且结果不够准确。

Method: PT-LiRPA结合LiRPA的过度逼近技术和采样方法，利用估计的可达集来紧化输出的线性上下界，同时保证验证的概率正确性。

Result: 在国际神经网络验证竞赛等标准基准测试中，PT-LiRPA验证器可使鲁棒性证书提高3.31倍和2.26倍，且能高置信度（99%）处理其他方法失败的困难实例。

Conclusion: PT-LiRPA提供了一个计算开销低、结果更紧且带概率保证的神经网络形式验证新框架，在复杂验证任务中表现优异，推动了神经网络安全验证技术的发展。

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [359] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: AXLearn是一种模块化且支持异构硬件的大规模深度学习训练系统，具有高性能和易扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了应对深度学习模型规模增大及硬件多样性，设计一个模块化且能高效支持异构硬件的训练系统。

Method: 通过严格封装的软件组件接口实现模块化设计，提出基于代码行数复杂度的量化模块化方法，以保持系统扩展时复杂度不增高。

Result: AXLearn在支持复杂功能（如RoPE）时只需少量代码，且性能与现有领先系统持平。

Conclusion: AXLearn通过模块化设计和对异构硬件的支持，实现了高效、易扩展的深度学习训练系统，并提供了开发和运营的宝贵经验。

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [360] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 本文提出了一种名为RepLIn的训练算法，通过利用干预数据中的因果独立性条件，学习对干预分布转变具有鲁棒性的判别性表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用干预所引起的因果独立关系，导致训练的表示在观察和干预数据上的预测性能差异大，尤其是当干预样本有限时表现较差。

Method: 首先识别性能差异与表示遵守干预因果模型独立条件的强相关性；其次对于线性模型，推导出干预数据比例的充分条件以保证独立性有助于降低干预数据误差；最后提出RepLIn算法，通过显式施加干预节点与非后代节点间的统计独立性来训练模型。

Result: 在合成数据、面部属性分类和毒性检测的真实图像及文本数据集上验证了RepLIn的有效性，显示其可扩展至拥有多个因果节点的图，且能提升对连续及离散潜变量干预分布转移的鲁棒表示学习。

Conclusion: 利用因果独立性条件显式约束干预数据中的表示学习，可以有效缩小预测性能在观察与干预数据间的差异，提升模型对干预分布变化的适应能力，实现更鲁棒的判别表示。

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [361] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: 本论文提出两种基于深度学习的新型架构，用于预测农业氮氧化物排放，旨在改进传统物理模型对复杂非线性污染物交互的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的空气质量预测模型难以准确捕捉农业排放中复杂的非线性污染物相互作用，影响预测效果。

Method: 本文设计两种新型深度学习模型EmissionNet和EmissionNet-Transformer，利用卷积和Transformer架构提取排放数据的时空依赖关系。

Result: 通过对比流行架构，所提模型在高分辨率农业排放数据预测中表现出较优的性能。

Conclusion: 提出的深度学习方法有效提升了农业N2O排放预测的准确度，为污染治理与空气质量管理提供了有力工具。

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [362] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: 论文提出了一种针对财务报表造假的多目标攻击方法MVMO，显著提高了攻击成功率，使恶意公司能大幅提高盈余并掩盖诈欺风险。


<details>
  <summary>Details</summary>
Motivation: 恶意企业有动机通过操纵财务报告隐藏困境并获得个人利益，现有攻击方法难以处理其具有相互矛盾目标的数据。

Method: 提出最大违背多目标（MVMO）攻击，动态调整攻击者搜索方向，以同时满足数据中的多目标约束。

Result: MVMO攻击比传统攻击找到的满足攻击多达20倍，在约50%案例中实现盈余膨胀100-200%，骗取欺诈评分降低15%。

Conclusion: 该方法在与法律和会计专业人士合作下，保证攻击模型真实反映实际财务造假手段，显著提升了对造假检测的挑战。

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [363] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: 本文提出了一种统一的分布式多步强化学习框架H-DQN，显著提升了2048游戏的得分表现。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏且延迟奖励环境中难以有效归因，导致策略局部最优。2048游戏就是此类挑战的典型。

Method: 基于Gym-2048环境，作者开发并比较了DQN、PPO、QR-DQN和集成多种技术的Horizon-DQN，后者结合了分布式学习、多步目标、噪声网络等先进技术。

Result: H-DQN在最大分数上远超其他方法，达到18.21K，进一步扩展可达41.828K，并能生成4096方块。

Conclusion: 分布式多步目标显著提升了稀疏奖励环境下的强化学习效果，未来可以结合模型规划和课程学习进一步提升性能。

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [364] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: 本文提出基于前向后向表示（FB）的方法，利用其本质进行探索策略设计，以最小化表示的不确定性，从而提升零样本强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本强化学习方法（如FB）与探索策略脱节，依赖外部探索算法，导致学习效率不高。

Method: 设计了一种基于FB表示后验方差最小化的探索策略，主动减少表征的不确定性，实现更有效的数据采集。

Result: 实验表明，该方法在样本复杂度上较其他探索策略有显著提升。

Conclusion: FB表示不仅适合无奖励强化学习，也应被用于指导探索，从而提升学习效率和性能。

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [365] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: 本文研究了在线凸优化中的动态遗憾问题，通过构造再生核希尔伯特空间（RKHS）将动态遗憾最小化归约为静态遗憾问题，提出了适用于任意损失序列的新方法并实现了计算可行的算法。


<details>
  <summary>Details</summary>
Motivation: 传统动态到静态遗憾归约多限于线性损失，而如何设计适用于任意损失函数且具有更好动态遗憾保证的方法尚未解决。

Method: 将与任意比较序列竞争的问题转化为与固定比较函数竞争的静态遗憾问题，在RKHS中构造合适的函数空间以实现归约，并利用RKHS的再生性质保证算法的可计算性。

Result: 恢复了线性损失下的最优动态遗憾界，提出了无量纲且方向自适应的动态遗憾界，同时扩展到任意损失序列，获得了指数凸和不恰当线性回归问题中的界限。

Conclusion: 本文提出的基于RKHS的动态到静态遗憾归约方法突破了线性损失的限制，具有广泛适用性和理论保证，同时能在无限维空间中实现实际可计算的算法。

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [366] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 本文提出了一种基于代理随机最优控制目标的无训练扩散引导框架，即Stein扩散引导（SDG），以提高扩散模型的指导效果。


<details>
  <summary>Details</summary>
Motivation: 随机最优控制（SOC）虽然能微调扩散模型，但计算成本高，不适合快速采样；现有训练免疫方法虽方便但可能误差较大，导致指导不可靠。

Method: 本文通过提出代理随机最优控制目标，结合斯坦变分推断纠正近似后验，实现了对扩散后验的有效校正，并引入新的运行代价函数，增强低密度区域引导能力。

Result: 在分子生成等复杂任务中，SDG显著优于传统的训练免疫引导方法，展示了其效果与潜力。

Conclusion: SDG成功融合了SOC与训练免疫方法的优势，为扩散模型快速且可靠的引导提供了新的理论与实践路径。

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [367] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出了一种名为Ex-HiDeNN的可解释层级深度神经网络，结合符号回归，从有限数据中高效发现闭式表达式，在多个基准测试和工程应用中表现出色，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 虽然数据驱动科学和计算发展迅速，但从复杂数据中高效发现可解释且准确的闭式表达式仍具挑战。

Method: 提出了Ex-HiDeNN方法，一种结合符号回归的层级深度神经网络，包含一个可分离性检查器，通过两步算法从有限观测数据中发现闭式表达式。

Result: 在多个基准问题和工程应用中，Ex-HiDeNN展示了卓越的近似能力，误差远小于参考数据和传统符号回归方法。

Conclusion: Ex-HiDeNN是一种高效、可扩展且具有解释性的闭式表达式发现方法，优于现有技术，且指出了当前限制及未来的发展方向。

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [368] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: 本文提出了一种基于图结构的神经网络，用于动态预测校园建筑物的占用情况和建筑间的人员流动，利用Wi-Fi日志数据和建筑使用时间表。


<details>
  <summary>Details</summary>
Motivation: 通过Wi-Fi数据直接估计人员流动，避免假设用户行为并保护隐私，提高校园流量预测的准确性。

Method: 构建图卷积结合LSTM网络（GCLSTM），以建筑为节点，通过边连接，建模复杂的交通流动模式。

Result: 在多伦多都会大学真实数据上，GCLSTM模型显著优于传统的多层感知机和线性回归模型。

Conclusion: 集成的GCLSTM模型有效提升了校园人员流动及占用预测的准确性，展现了其在实际应用中的潜力。

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [369] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 本文提出了一种新的多级蒙特卡洛(MLMC)压缩方案，通过利用有偏压缩器构建无偏估计，有效融合了有偏与无偏压缩方法的优点，显著减少分布式学习中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 分布式学习中的通信开销严重制约了算法效率，而现有梯度压缩技术在有偏压缩的经验效率和无偏压缩的理论保证之间存在权衡。

Method: 引入多级蒙特卡洛(MLMC)压缩方案，利用有偏压缩器构造统计无偏估计，并应用于如Top-k和位级压缩器，并提出自适应版本以提升性能。

Result: 通过在分布式深度学习任务中的实证验证，展示了该方法在减少通信成本的同时提升压缩性能和学习效果。

Conclusion: 该方法有效桥接了有偏与无偏压缩技术的差距，提升了分布式学习的通信效率和模型性能，具备广泛应用潜力。

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [370] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的用户增长营销优化方法，通过直接建模业务指标提升，优化用户选择和奖励分配，显著提升营销效果并降低成本。


<details>
  <summary>Details</summary>
Motivation: 互联网消费企业需要通过优化高成本的营销活动，实现用户增长和最大化用户参与度。为此，提出一种更有效的营销策略优化方法。

Method: 利用深度学习模型从历史实验中学习，直接建模关键业务指标的提升，采用软最大门控联合优化参数，处理复杂业务约束。

Result: 该方法在对比R-learner和因果森林等先进技术时，表现出至少20%的性能提升，证明了其成本效益和实际影响力。

Conclusion: 提出的深度学习方法不仅效果优越且灵活，可广泛应用于多产品场景的优化分配，且已在全球范围内成功部署验证。

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [371] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: 本文提出了一种方法，通过神经增强朴素贝叶斯层在深度学习框架中整合多因素治疗策略，实现对异质性治疗效果的直接排序和优化。


<details>
  <summary>Details</summary>
Motivation: 随着异质性治疗效果（HTE）在科学及工业中的广泛应用，治疗行动空间从简单的二元变量扩展至包含多个因素的结构化治疗策略，亟需有效方法整合和分析多变量治疗政策。

Method: 从理论出发，本文推导了多治疗策略变量纳入个体及平均治疗效果的函数形式，构建神经增强朴素贝叶斯层整合多个遵循朴素贝叶斯假设的因素，并应用于连续治疗强度、离散治疗分配及聚合治疗效果函数的直接排序。

Result: 所提出的方法在公共数据集上表现优秀，证明了该泛化深度学习异质治疗政策框架提升异质性治疗效果估计和排序性能的有效性。

Conclusion: 本文提出的通用框架成功融合多因素治疗政策，有助于精准识别和排序异质治疗效果，推动深度学习应用于复杂治疗策略的研究与实践。

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [372] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: 本文提出了一种基于元学习的端到端模型MACE-TNP，用于贝叶斯模型平均因果推断，解决了因果结构海量增长导致的计算难题，提升了干预效应的预测性能。


<details>
  <summary>Details</summary>
Motivation: 因果结构未知且可支持多种模型，直接计算贝叶斯模型平均因果推断计算量巨大且不可行，且传统方法容易过度自信。

Method: 利用元学习构建MACE-TNP模型，训练其预测贝叶斯模型平均的干预后验分布，避免了昂贵的计算。

Result: 实验证明MACE-TNP在预测干预效果上优于强大的贝叶斯基线方法。

Conclusion: 元学习为逼近复杂的贝叶斯因果推断提供了灵活且可扩展的范式，有望在更复杂场景中应用。

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [373] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: 该论文提出了一种名为InterpoLL的插值学习方法，通过在多数类样本的表示中融合少数类中的关键特征，减弱模型对数据捷径的依赖，从而提升模型对少数类样本的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的经验风险最小化方法导致模型过度依赖数据中的短路关联，这种依赖在少数类样本中表现为泛化能力下降。而现有的短路缓解方法存在针对性强、难调优、计算开销大且未能有效提升表征学习的问题。

Method: InterpoLL方法通过在多数类样本的表示中插值包含少数类内含有缓解捷径相关特征的表示，弱化模型依赖捷径，从而学习到对多种样本都具有预测力的特征。

Result: 在多个自然语言理解任务上，InterpoLL相较于经验风险最小化和最先进的短路缓解方法，在少数类样本泛化能力上均有提升，且未影响多数类样本的准确性。

Conclusion: InterpoLL方法适用于多种模型结构（编码器、编码器-解码器、仅解码器），具有广泛的适用性，有效改善了模型对少数样本的泛化能力，解决了捷径依赖问题。

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [374] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 本文提出了一种针对图神经网络(GNN)的硬件级渐进式位翻转攻击(GBFA)，通过有针对性地翻转关键层中少量权重位，显著降低模型预测准确率。


<details>
  <summary>Details</summary>
Motivation: 当前虽有多种硬件加速器支持GNN，但相关的安全性问题，尤其是硬件故障注入攻击未被充分关注。研究其脆弱性有助于提升GNN系统的安全防护能力。

Method: 提出GBFA方法，采用两步策略：第一步利用马尔可夫模型预测层执行顺序以定位攻击目标层；第二步基于梯度排名在该层中选择易受攻击的权重位进行逐步位翻转，最大程度影响模型性能。

Result: 在Cora和PubMed数据集上的多种GNN模型节点分类任务中，GBFA显著降低了预测准确率。例如，在Cora数据集上仅翻转GraphSAGE最后一层的单个位，即使准确率下降了17%。

Conclusion: GBFA证明了GNN对硬件故障注入攻击的脆弱性，强调采用层感知的攻击策略对于提高攻击效果的重要性，为未来设计更安全的硬件GNN系统提供了参考。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [375] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 该调研比较了基于特征和基于GAN的示范学习方法，分析了它们在奖励函数结构及策略学习上的不同表现。


<details>
  <summary>Details</summary>
Motivation: 了解两种主流示范学习方法在奖励函数设计上的差异及其对策略学习的影响，帮助研究者根据任务需求选择合适的方法。

Method: 对比分析了基于特征的方法和基于GAN的方法的奖励信号设计、训练稳定性、泛化能力以及结构化动作表示的作用。

Result: 基于特征的方法奖励信号密集且可解释，适合高保真动作模仿，但泛化能力有限；基于GAN的方法训练灵活且适应性强，但奖励信号粗糙且易不稳定。两者最新进展都强调结构化动作表示的重要性。

Conclusion: 两种方法各有优劣，选择应基于任务对保真度、多样性、可解释性和适应性的具体要求，避免简单二分，促使更有原则的算法选择。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [376] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: 本文首次理论分析了跳跃连接下图卷积网络（GCNs）结合图稀疏化的学习动态和泛化性能，指出当稀疏矩阵保留关键边时泛化性能得以保持，并发现不同层的跳跃连接对稀疏化要求不同。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，跳跃连接和图稀疏化各自提升GCNs性能，但缺乏结合两者的泛化理论分析。

Method: 提出使用稀疏有效邻接矩阵A*的框架，理论分析GCNs在跳跃连接与图稀疏化条件下的学习动态和泛化误差，并区分不同层稀疏化对性能的影响。

Result: 证明了泛化准确度接近于依赖A*的目标函数的最高准确度，指出跳跃连接导致各层对稀疏化的不同敏感度。

Conclusion: 跳跃连接配合合理图稀疏化能保持GCNs的泛化性能，首个揭示了跳跃连接对稀疏化层次需求的理论特征，实验验证了理论结果。

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [377] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 提出了一种名为不确定性感知强化学习（UARL）的新框架，通过无须直接与目标环境交互的方式，实现了在多样化条件下的安全训练和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 现实环境中部署强化学习策略面临分布转移、安全隐患和直接交互不可行的问题，现有方法安全性不足。

Method: UARL利用多个评估器构建不确定性量化，通过逐步环境随机化模拟高不确定性状态，提升策略泛化能力，无需目标域的直接交互。

Result: 在MuJoCo基准和四足机器人测试中，UARL表现出更可靠的OOD检测能力、性能提升及更高的样本效率。

Conclusion: UARL有效解决了现实环境中策略部署的安全与效率问题，提升了强化学习策略的鲁棒性和泛化能力。

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [378] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: LSC-GNN模型通过引入外部“干净”链接约束嵌入，有效应对图神经网络中的噪声边问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在噪声边的存在下表现不佳，需引入机制减少对噪声边的过拟合。

Method: 设计两个编码器分别训练完整图和排除噪声边的正则化图，限制两者潜在表示的偏差，从而引导模型避免过拟合噪声边。

Result: 在多组基准测试中，LSC-GNN在含有中度噪声的图数据上优于传统及抗噪声GNN，并成功拓展到异构图，提升了蛋白质-代谢物网络预测性能。

Conclusion: LSC-GNN在含噪声的图结构中提升了预测准确性和模型可解释性，具有较大应用潜力。

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [379] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: 利用机器学习提升手持物料搬运中的负载估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接映射负载步态到手持负载，泛化能力和预测准确性受限。

Method: 结合辅助信息（静止步态和携带方式），采用深度潜变量模型、时序卷积网络和双向交叉注意力机制，利用领域知识设计模型以消除推断时对携带方式标签的依赖。

Result: 利用真实IMU数据实验证明引入辅助信息显著提升准确性，并且显式融合机制优于简单特征拼接。

Conclusion: 通过引入辅助信息和复杂融合机制，所提模型有效提升了基于步态的负载估计，增强了适用性和预测性能。

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [380] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 本文提出了多任务预演算法，通过对未执行任务的反事实模拟学习预测性表示，帮助人类和人工智能在多任务环境中更快适应和推广。


<details>
  <summary>Details</summary>
Motivation: 人类虽然能接触无限多任务，但通常只能同时进行少数任务，假设人类通过一个任务的经验预先学习其他未执行但可访问任务的解决方案。

Method: 提出多任务预演（Multitask Preplay）算法，将一个任务的经验作为起点，利用反事实模拟学习可支持快速适应的预测性表示。

Result: 实验显示该算法在网格世界和部分可观测的2D Minecraft环境中，更好地预测人类如何泛化至未执行任务，并使人工智能能迁移行为至新环境。

Conclusion: 多任务预演为人类多任务学习和泛化提供了可扩展理论，赋予人工智能此能力显著提升多任务环境下性能。

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [381] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文综述了大型语言模型中的数据记忆现象，分析其驱动因素、检测方法及缓解策略，并探讨其技术、隐私与伦理影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在表现出卓越能力的同时存在训练数据的记忆问题，带来隐私风险和模型行为理解的挑战。

Method: 综合近期研究，探讨训练数据重复、训练动态、微调等对记忆的影响，评估前缀提取、成员推断、对抗提示等检测方法，并分析数据清洗、差分隐私、训练后遗忘等缓解策略。

Result: 明确了记忆现象的关键驱动因素及有效检测手段，揭示了其技术和伦理层面的复杂影响，并总结了缓解记忆的多种途径及其挑战。

Conclusion: 本文全面总结了大型语言模型记忆方面的研究进展，强调在减少有害记忆与保持模型效用间需要权衡，并指出未来研究的重要方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [382] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: 本文提出了一种基于近端策略优化的无模型强化学习方法，用于现场训练衍射光学处理器，实现了更快更稳定的收敛，提高了多种光学任务的性能。


<details>
  <summary>Details</summary>
Motivation: 衍射光学网络在高效信息处理方面具备潜力，但现有的优化方法难以应对硬件固有误差、噪声和对准偏差，且收敛缓慢且不稳定。

Method: 采用无模型强化学习中的近端策略优化算法，在物理系统上直接进行训练，充分利用有限的测量数据并限制策略更新，提高收敛速度和稳定性。

Result: 在多种任务实验中，包括定向能量聚焦、全息图成像、像差校正和光学图像分类，验证了该方法比现有方法表现出更好的收敛速度和性能。

Conclusion: 该方法无需系统先验知识，能自然适应实际系统中的各种未知误差，实现更快速准确的训练，为复杂反馈驱动的光学及物理系统提供了可扩展的训练框架。

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [383] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合傅里叶谱方法与Transformer网络的统一框架，实现了对偏微分方程的高精度长期预测，优于传统数值方法和机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 旨在整合经典谱方法的高精度数值求解与基于注意力机制的神经网络优势，解决偏微分方程的长期预测问题。

Method: 通过将原始偏微分方程转换为谱常微分方程，用高精度数值求解器生成训练数据，采用Transformer网络预测谱系数演化。

Result: 在二维不可压Navier-Stokes方程和一维Burgers方程上验证，模型在有限训练数据下实现高度准确的长期预测，优于传统方法。

Conclusion: 该谱Transformer框架具备良好泛化能力，为复杂动力系统的实时预测与控制提供了新的有效范式。

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [384] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文通过大规模实证研究分析强化学习中奖励黑客行为，提出自动检测多种奖励黑客类别的方法，检测准确率达78.4%，并验证了奖励密度和目标一致性对黑客行为的影响，进而开发缓解技术减少奖励黑客发生。


<details>
  <summary>Details</summary>
Motivation: 强化学习系统中的奖励黑客行为威胁自主代理部署安全，现有系统缺乏系统性检测和缓解方法，亟需深入研究和解决。

Method: 分析15个强化学习环境、5种算法共15,247个训练回合，实现六类奖励黑客自动检测算法，并通过控制实验研究奖励函数性质对黑客频率的影响，同时设计缓解策略并验证效果。

Result: 检测框架达78.4%精准率和81.7%召回率，计算开销低于5%；缓解方法在控制场景中最高减少54.6%奖励黑客，验证了奖励密度和目标一致性显著影响黑客频率。

Conclusion: 提出的检测与缓解方法有效支持强化学习安全研究，但实际部署仍面临概念漂移、误报成本及对抗适应等挑战，相关资源已开源以促进研究社区发展。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [385] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: 图学习作为机器学习和人工智能的重要分支，经历了从图论方法到图神经网络的发展，已广泛应用于药物发现、欺诈检测等领域，但仍面临可扩展性、泛化性等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习难以有效建模复杂的非欧几里得关系，图学习通过捕捉这种关系以更好地支持现实世界应用。

Method: 综述了图学习在可扩展架构、动态图建模、多模态学习、生成模型、可解释性和责任AI等方面的最新技术与方法。

Result: 提出了高效处理大规模图、动态时序依赖、多模态数据融合、生成新图样本及提升模型解释性的方法，同时探讨了隐私、公平性等伦理问题。

Conclusion: 图学习具有广阔应用前景，但需解决可扩展性、泛化性、异质性、可解释性和可信度等问题，未来发展将融合更多AI范式，并关注伦理责任。

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [386] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: 本文提出了特征收敛定理（FACT），揭示了神经网络权重在有权重衰减训练下的收敛特性，并基于此设计了新的学习算法FACT-RFM，在表格数据上表现优异并能捕捉特征学习的多种现象。


<details>
  <summary>Details</summary>
Motivation: 深度学习理论中的关键挑战是理解神经网络如何学习和表示特征。

Method: 证明了特征收敛定理（FACT），给出了权重收敛时的自洽方程。通过调整递归特征机（RFM）使其满足FACT，设计了新算法FACT-RFM。

Result: 实验证明神经网络的特征在收敛时满足FACT，且FACT-RFM在表格数据性能优异，能模拟神经网络训练中的各种特征学习行为。

Conclusion: FACT定理为理解和设计神经网络特征学习提供了理论工具，FACT-RFM展现了其应用潜力和现实价值。

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [387] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: 本文提出了一种基于惯性传感器和深度学习的犬类步态分析方法，用于区分神经性和骨科性步态异常。


<details>
  <summary>Details</summary>
Motivation: 现有临床经验不足以准确区分犬类神经性与骨科性步态障碍，需开发客观有效的辅助诊断工具。

Method: 利用惯性传感器采集29只犬的步态数据，采用深度学习模型优化传感器配置和评估方案，实现多分类（健康/骨科/神经）和二分类（健康/非健康）任务。

Result: 提出的方法在多分类任务中准确率达到96%，二分类任务中准确率达到82%，且具有良好的泛化能力。

Conclusion: 基于惯性传感器的深度学习模型能有效区分犬类神经性和骨科性步态异常，具备实用的临床辅助诊断潜力。

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [388] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习与专家混合模型结合面临的系统挑战，提出了一种智能的客户端与专家动态匹配系统设计。


<details>
  <summary>Details</summary>
Motivation: 现有大型联邦MoE模型训练效率低，缺乏动态且量化的客户端-专家匹配策略，难以兼顾客户端异构资源和系统负载均衡。

Method: 设计了包含动态适应评分、全局专家负载监控和客户端容量分析的智能匹配系统，优化训练过程。

Result: 通过系统性策略，实现了更高效、可扩展且收敛轮次更少的联邦MoE训练机制。

Conclusion: 该方法提升了大型联邦MoE模型在边缘计算环境中的部署可行性和通信效率。

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [389] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 本文提出了AutoTriton，一种基于强化学习的Triton程序自动调优模型，通过有监督微调和强化学习提升GPU内核性能，实现与主流大模型相当的表现。


<details>
  <summary>Details</summary>
Motivation: GPU内核开发需要手动调优关键参数，过程繁琐且效率低下，阻碍了性能优化和技术推广。

Method: AutoTriton先通过有监督微调学习Triton编程知识，再使用基于规则和执行反馈的奖励函数进行增强学习，采用GRPO算法优化策略。

Result: 在TritonBench和KernelBench测试中，AutoTriton表现出媲美大型模型如Claude-4-Sonnet的性能，验证了各模块设计的重要性。

Conclusion: 强化学习方法在自动生成高性能内核方面显示出巨大潜力，为构建更高效的AI系统奠定了基础。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [390] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文提出了MobileGUI-RL，一个在线训练的GUI智能体框架，通过自我探索生成学习任务，提升了智能体在移动界面的自动导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的GUI智能体多数依赖离线预收集轨迹训练，导致难以扩展、过拟合特定UI模板，且在未知环境中表现脆弱。

Method: 提出MobileGUI-RL框架，包括(1)通过自我探索和筛选生成可学习任务课程，(2)采用改进的GRPO算法适配GUI导航，利用轨迹感知优势和综合奖励平衡任务成功率与执行效率。

Result: 在三个人机在线移动代理基准测试中，MobileGUI-RL表现出持续的性能提升，验证了该方法的有效性。

Conclusion: MobileGUI-RL通过在线环境训练和任务课程生成，显著增强了GUI智能体的泛化能力和执行效率，推动了移动端自动化操作的研究进展。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [391] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: 提出了一种基于双层无人机辅助边缘计算架构的部分卸载方案，以协调异构计算资源，优化系统延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 当前无人机辅助卸载策略在协调异构计算资源和适应动态网络条件方面存在不足。

Method: 设计了由高空无人机中继能力和低空无人机计算支持组成的双层架构，构建联合优化模型，通过软演员批评算法分层优化卸载和轨迹规划，并采用优先级机制进行局部调度。

Result: 仿真实验表明该方法在任务完成率、系统效率和收敛速度上优于多个基线方案，表现出强鲁棒性和适用性。

Conclusion: 所提双层无人机辅助部分卸载架构有效整合异构资源，提升了动态车联网中的边缘计算性能。

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [392] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出了ABench-Physics，一个专门针对物理领域设计的语言模型评测基准，包含静态和动态两个部分，专注于评估模型的物理推理与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在物理领域的推理和建模能力尚未充分探索且评测基准不足，现有测试多为选择题且难度有限，缺少动态考察物理建模的能力。

Method: 设计了两部分组成的基准：静态的400道高难度物理题和动态的100道题目，通过自动变化引擎考察模型在变化条件下的鲁棒性，要求模型给出精确数值答案并遵守严格格式与容差限制。

Result: 在多种先进大语言模型上的评测显示出显著的性能差距，尤其是在动态条件下的泛化能力存在限制，暴露了模型在物理推理上的不足。

Conclusion: ABench-Physics为推动语言模型在科学推理方面的提升提供了一个具有挑战性且诊断性强的评测框架，促进物理领域能力的发展。

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [393] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 提出了基于多层感知机的WeatherMixer架构及Jigsaw模型并行方案，解决了高分辨率大规模气象预测中的计算和内存瓶颈，实现了高效的多GPU训练和优异的扩展性能。


<details>
  <summary>Details</summary>
Motivation: 目前高分辨率、中长期气象预测涉及大规模神经网络和海量数据，受限于加速器的内存和I/O带宽，训练效率低下，需要新的模型架构和并行策略来提升性能。

Method: 设计了WeatherMixer模型，其计算复杂度随输入规模线性增长，结合新颖的Jigsaw模型并行方法，融合领域与张量并行，消除内存冗余，提升训练扩展性和效率。

Result: 在256 GPU集群下训练，达到9和11 PFLOPs峰值性能，分别为理论峰值的23%和28%；相较无模型并行，训练扩展效率提升至68%和72%，显著优于51%的基线表现。

Conclusion: WeatherMixer与Jigsaw并行方案有效解决了大规模气象预测中模型训练的内存及通信瓶颈，显著提升了训练性能和扩展效率，为高分辨率气象预报提供了强有力的技术支持。

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [394] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: 本文提出结合深度学习图像配准和物理约束的心脏组织运动生物力学特性预测及疾病分类方法，在ACDC数据集上取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 心脏疾病诊断需要准确及时的方法，传统图像配准不足以捕捉生物力学特性，需结合物理知识提高诊断解释性和准确度。

Method: 采用Neo-Hookean材料的能量应变模型，结合深度学习图像配准与物理信息正则化，优化形变场保证物理一致性，提取局部应变特征用于疾病分类，测试多种分类算法并进行特征选择。

Result: 在ACDC数据集上，左室腔Dice得分0.945，右室腔0.908，心肌0.905。分类准确率训练集达98%，测试集达100%。

Conclusion: 该方法结合可解释人工智能，提高心脏疾病诊断的准确性和透明度，有助于个性化和有效的临床治疗。

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [395] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: 提出了一种结合时间序列预测与调整后的流量平衡分析（FBA）方法来预测动态图结构，解决了顶点变化限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有图预测方法通常假设连续图的顶点不变，限制了对动态变化的网络的建模能力。

Method: 将时间序列预测方法与适应性调整的流量平衡分析（FBA）结合，后者通过线性规划引入适合增长图的约束。

Result: 在合成数据（优先连接模型）及实际数据集（UCI Message、HePH、Facebook、Bitcoin）上的实验证明了该方法的有效性。

Conclusion: 提出的方法有效解决了动态图中顶点变化问题，提高了图结构预测的准确性，适用于动态网络分析和异常检测。

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [396] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: 本文提出了一种通过训练多个独立适配器并平均其输出的方法，以提升少样本领域适配中基础模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对少样本领域适配中难以进行超参数调优和模型对分布偏移不够鲁棒的问题寻求解决方案。

Method: 训练多个超参数多样的独立适配器并将它们的输出进行平均，实现性能提升和鲁棒性增强，并通过参数串联将集成模型简化为单一适配器，称为Soup-Adapter。

Result: 集成适配器在性能和应对测试数据分布偏移上优于单一适配器，且对关键超参数残差比例不敏感。

Conclusion: Soup-Adapter有效解决了少样本领域适配中的超参数调优困难和模型鲁棒性问题，是首次将CLIP适配器风格技术应用于DINOv2并与CLIP进行直接比较的研究。

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [397] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: 本文提出了一种结合图结构的交互式工具BAGEL，扩展概念基可解释性方法，用于全局解析深度学习模型的语义属性及其传播机制。


<details>
  <summary>Details</summary>
Motivation: 当前概念解释方法多聚焦于局部神经网络预测，缺乏对模型全局行为及潜在机制的深入分析。

Method: 通过系统量化语义概念在模型各层的表示，揭示潜在电路和信息流，并以结构化知识图形式可视化，辅助探索概念类别关系与数据偏差。

Result: 构建了模型无关、可扩展的框架和名为BAGEL的可视化平台，实现了对深度模型语义属性传播与偏差的全局分析。

Conclusion: 该框架提升了对深度学习模型泛化能力及偏差机理的理解，有助于增强模型的可信度和解释力。

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [398] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: 本文研究了领域泛化和算法公平性结合的问题，提出了一个新的公平领域泛化（FairDG）框架PAFDG，利用信息论方法设计，并通过帕累托优化平衡效用与公平性，在视觉和语言数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前领域泛化方法多关注降低目标域的风险，但忽视公平性；现有公平方法多不考虑领域变化，公平性无法泛化到新域。本文旨在解决这两者结合的问题，实现公平且稳健的领域泛化。

Method: 本文基于多类分类和多组敏感属性，推导了基于互信息的风险和公平性上界，为算法设计提供信息论视角指导。提出PAFDG框架，利用帕累托优化权衡效用与公平性，实现公平领域泛化。

Result: 在真实视觉和语言数据集上的实验显示，PAFDG在效用和公平性的权衡上优于现有方法，表现出更好的泛化能力和平衡效果。

Conclusion: 本文首次系统研究了公平领域泛化问题，基于信息论提出理论界限，并设计帕累托优化框架，有效提升了模型在多变域上的公平性和效用。该方法为公平领域泛化提供了新的思路和实践方案。

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [399] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习框架，利用原型和轻量级适配器模块减小通信开销并解决统计异质性，提升模型泛化能力和解释性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信负担重和客户端数据分布异质性大的挑战，需要有效压缩模型参数并提升模型泛化能力和可解释性。

Method: 通过使用原型表示替代完整模型权重，并采用轻量级适配器模块作为本地模型的压缩代理，客户端通过对齐类嵌入与原型并调整适配器，实现模型本地优化和全局结构共享。

Result: 在真实视网膜眼底图像数据集上进行实验，展示了模型固有的解释能力和分类任务的准确率优于基线算法。

Conclusion: 所提方法有效减小通信开销，提升联邦学习中模型的泛化能力和解释性，适用于具有非独立同分布数据的多客户端环境。

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [400] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（PINNs）的电力系统状态估计方法，提高了响应速度和准确性，尤其在故障和网络攻击条件下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统在状态估计和实时监测方面面临响应速度和准确性挑战，特别是在故障或网络攻击情况下。

Method: 通过将物理定律嵌入神经网络架构，PINNs提升了输电网状态估计的准确性和鲁棒性。

Result: 实验表明该方法比传统机器学习模型在未知数据上的准确率高83%，在新数据集表现提升65%，并在数据篡改攻击下准确率提升93%。

Conclusion: 物理信息神经网络在电力系统状态估计中显著提升了准确性和安全性，具备广泛应用潜力。

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [401] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: 本文提出了一种基于图自动编码器的通用表格数据嵌入框架，用于无监督生成表格数据的嵌入表示，以支持多种下游任务如回归、分类和异常检测。


<details>
  <summary>Details</summary>
Motivation: 工业数据中表格数据占据重要地位，且下游应用任务多样且常不可预设，需一种任务无关的通用表示方法。

Method: 将表格数据转化为图结构，利用图自动编码器生成实体嵌入，再聚合得到每条数据样本的嵌入，无需针对新样本额外训练。

Result: 实验表明该方法在真实数据集上相比其他通用表格嵌入技术有更优表现。

Conclusion: 该框架实现了任务无关的通用表格数据嵌入，能够有效支持多种下游分析任务，提升了工业表格数据的分析能力。

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [402] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: 该论文提出了一种扩散数据集浓缩方法D2C，通过选取和增强子集，大幅减少数据量，实现扩散模型的高效训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练资源消耗巨大，现有方法未解决数据集缩减问题，亟需从数据角度降低训练成本。

Method: 设计了D2C框架，包含“选择”（用扩散难度分数和区间采样选子集）和“附加”（丰富子集语义视觉信息）两个阶段。

Result: 多种数据集规模和模型架构实验表明，该方法提升训练速度100倍，使用0.8%数据仅40k步即可达到高视觉质量，FID 4.3。

Conclusion: D2C有效实现扩散数据集浓缩，极大降低训练成本且保持生成质量，为扩散模型训练提供新思路。

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [403] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: 本文研究了兽医领域中犬类心脏杂音数据中标签噪声的问题，通过多专家意见减少标签噪声，采用三种分类算法提升分类性能，XGBoost表现最佳，显著提高了心脏杂音检测的灵敏度和特异性。


<details>
  <summary>Details</summary>
Motivation: 标签噪声严重影响犬类心脏杂音AI模型的训练效果，亟需通过专家多重评估减少噪声以提升诊断准确率。

Method: 收集140条犬类心脏音频，多个专家对心脏杂音强度进行标注，筛选70条高质量数据构建低噪声数据集。利用心脏单次心跳周期扩充训练集，训练AdaBoost、XGBoost和随机森林三种分类器，比较其性能，重点分析标签噪声减少的影响。

Result: XGBoost分类器在降低标签噪声后性能显著提升，轻度心脏杂音敏感度从37.71%提升到90.98%，特异性从76.70%提升到93.69%；中度杂音敏感度从30.23%提升至55.81%，特异性提升至97.19%；重度杂音敏感度和特异性也大幅提升。

Conclusion: 通过多专家评估减少标签噪声显著提升了犬类心脏杂音的检测准确率，XGBoost表现最佳，表明减少标签噪声对提升兽医心脏病AI诊断模型至关重要。

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [404] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文将Adam优化器重新解释为一种符号类优化器，简化了其收敛性分析，并首次在较弱假设下证明了其最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有对Adam优化器收敛性的理论分析复杂且需强假设，难以验证和扩展。

Method: 提出将Adam视为符号类优化器，简化收敛性证明，并基于广义p-仿射方差和平滑性条件进行分析。

Result: 在弱假设条件下证明Adam达到了1/T^{1/4}的最优收敛率，且不依赖模型维度和数值稳定参数ε。

Conclusion: 重新解释Adam的动量作用促进收敛，为实际调参提供理论指导，促进理论与实践结合。

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [405] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: KnowIt是一个灵活的深度时间序列建模与解释框架，提供Python工具包，支持用户构建和解释复杂时间序列模型。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列数据分析缺少灵活且易扩展的深度学习模型及解释工具，KnowIt旨在填补这一空白，促进时间序列知识发现。

Method: KnowIt通过解耦数据集定义、深度神经网络架构和解释技术，构建模块化接口，支持导入新数据集、自定义架构及多种可解释性范式，实现实时建模与解释。

Result: KnowIt作为一个Python工具包，实现了灵活性、扩展性和模块化设计，用户可方便地对复杂时间序列数据构建强大模型并解释其行为。

Conclusion: KnowIt为时间序列深度学习模型的构建和解释提供了一个开放且可信的平台，促进该领域的发展和应用，未来将继续完善和推广。

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [406] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: Kamae是一个开源Python库，实现了PySpark数据预处理管道向Keras模型的转换，确保训练和推理环节预处理一致。


<details>
  <summary>Details</summary>
Motivation: 生产推荐系统中，训练和推理环境需统一的数据预处理逻辑，避免重复编码和数据分布漂移风险。

Method: 设计配置化Spark转换器和估计器，并映射到Keras层，支持预处理流程的端到端一致性。

Result: 在MovieLens数据集和Expedia的排序学习管道中验证了框架的有效性。

Conclusion: Kamae有效桥接了离线和在线预处理差异，简化工程实现，提升系统一致性。

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [407] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: 本文提出了一种通用的多视角中融合学习方法，解决高维低样本问题。


<details>
  <summary>Details</summary>
Motivation: 高维低样本环境中，传统学习方法难以有效处理维度远大于样本数的数据特征。

Method: 通过设计三种视角构建方法，将高维特征划分为多个子集，利用多视角中融合技术进行学习。

Result: 在多种模型和学习任务上的广泛实验验证了该方法的有效性和泛化能力。

Conclusion: 多视角中融合学习在高维低样本环境下表现优异，为该领域的进一步研究奠定了基础。

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [408] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: EdgeCodec是一种针对风力涡轮叶片气压数据的端到端神经压缩器，实现高压缩率和低重构误差，支持实时运行和动态码率调节，显著降低无线传输能耗。


<details>
  <summary>Details</summary>
Motivation: 提高风力涡轮叶片气压数据的压缩效率和传输能耗，延长传感器设备的使用寿命。

Method: 采用不对称自动编码器结构，结合鉴别器训练和残差矢量量化器，支持位速率动态调整，并在GAP9微控制器上实时运行。

Result: 实现2560:1-10240:1压缩率，重构误差低于3%，码率11.25-45bps，最高模式下无线传输能耗降低2.9倍。

Conclusion: EdgeCodec有效提升气压数据压缩效率，支持实时动态码率调整，显著节能，适合边缘传感器设备应用。

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [409] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 本论文提出了一种本地-全局卷积神经网络（LGCNN），用于模拟复杂的异质地下水流动与热传输过程，可有效处理大规模、多点热泵注入的城市级地下温度场预测问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在科学与工程中的实际应用中因训练数据有限或质量低下而受到限制；地热地下水流动过程的数值模拟计算量大，且数据驱动模型难以准确捕捉敏感的对流过程。

Method: 设计了一种结合轻量级数值代用模型（全局）与卷积神经网络（局部）的LGCNN模型，实现对地下水速度和热扩散过程的有效建模。该模型利用真实地下结构数据训练，并能适应更大范围的数据。

Result: LGCNN模型成功模拟了慕尼黑地区包含百个热泵注入点的复杂地下温度场，且在随机输入场景下表现稳定，可直接扩展到更大区域而无需重新训练。所有数据、代码和训练模型均已公开。

Conclusion: 本文提出的LGCNN方法实现了对复杂地下水流与热传输过程高效、准确的模拟，克服了数据驱动模型预测对流过程的难题，具有良好的推广和应用潜力。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [410] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: 本文研究了量化感知训练（QAT）在结构化状态空间模型（SSM）上的应用，提升了模型在模拟内存计算硬件上的效率和鲁棒性，显著减少了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: SSM在长序列处理中的优势及其低资源需求使其适合边缘设备，但现有研究未充分考虑QAT在专用边缘硬件（如模拟内存计算芯片）上的应用影响。

Method: 通过量化感知训练（QAT）降低模型数值精度，提高对模拟噪声的鲁棒性，结合结构剪枝技术，优化SSM模型尺寸和性能，最终在忆阻器类模拟内存计算硬件上部署实现。

Result: QAT将SSM的复杂度降低了两个数量级，提升了模型对模拟噪声的耐受性，并使结构剪枝成为可能，显著提高了计算效率。

Conclusion: 量化感知训练结合结构剪枝能有效优化SSM模型，促进其在模拟内存计算芯片等资源受限边缘设备上的实际应用。

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [411] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 本文提出了CoRE方法，通过隐藏状态的轨迹检测推理过程中的冗余步骤，提升大规模推理模型的推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型存在“过度思考”问题，即推理步骤冗余，导致推理效率低下。如何让模型在无外部标签的情况下自我评估推理过程的正确性是关键。

Method: 提出Chain-of-Reasoning Embedding(CoRE)，通过分析推理隐藏状态的几何属性识别循环冗余的推理步骤，设计无监督、无训练的自我评估框架CoRE-Eval，实现动态推理终止。

Result: 在数学推理基准（GSM8K、MATH-500、AIME）和不同规模模型（7B至32B）上，CoRE-Eval减少推理链长度13.7%至33.2%，提升答案准确率约10%，在32B模型上AIME准确率达到70%。

Conclusion: CoRE及CoRE-Eval有效解决大规模推理模型的过度思考问题，通过无标签自我评估显著提升推理效率和性能。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [412] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: 本文提出了基于子空间的近似Hessian方法（ZO-SAH），通过在随机选取的二维子空间内拟合二次多项式，估计函数的Hessian矩阵，实现了零阶优化中的加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法多依赖于一阶近似，虽然二阶信息理论上能显著加快收敛速度，但高昂的函数评估代价限制了其实用性。

Method: ZO-SAH方法在随机选择的二维子空间中拟合二次多项式，提取二阶系数作为Hessian近似；并通过周期性切换子空间策略，复用函数评估，减少查询次数。

Result: 在八个基准数据集上，包括逻辑回归和深度神经网络训练任务，ZO-SAH表现出比现有零阶方法显著更快的收敛速度。

Conclusion: ZO-SAH成功利用子空间内二阶信息，在节约函数评估成本的前提下，提升了零阶优化的效率和实用性。

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [413] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: 本文提出了一种结合多种矩阵分解方法的分层链接预测框架，用于从大规模、稀疏且噪声较多的科学文献网络中推断缺失或潜在的节点关系，辅助材料科学领域的跨学科发现。


<details>
  <summary>Details</summary>
Motivation: 科学文献网络通常包含大量缺失的链接，尤其是在复杂材料领域，如何有效推断这些缺失关系以发现潜在知识是该研究的动机。

Method: 结合分层非负矩阵分解(HNMFk)、布尔矩阵分解(BNMFk)与逻辑矩阵分解(LMF)，自动模型选择，构建材料与主题之间的三层主题树，并通过BNMFk与LMF的集成方法实现离散可解释性与概率评分的融合。

Result: 方法成功从实验文献库中识别出材料与主题（如超导、电能存储、摩擦学等）的聚类，并预测出缺失或弱连接的潜在关联，如成功预测被移除的超导相关文献的关联，验证了模型的有效性。

Conclusion: 该框架能够发现科学文献中的隐藏关联，为跨学科材料科学研究提供新假设，并通过交互式仪表盘支持人机协同科学发现。

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [414] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: 本文分析了卷积层中步幅引入的混叠现象及其对数值稳定性和泛化能力的影响，提出了基于框架理论的混叠描述和Parseval稳定性的优化方法。


<details>
  <summary>Details</summary>
Motivation: 步幅卷积固有的混叠会影响神经网络的数值稳定性和统计泛化能力，但混叠及其稳定性影响缺乏系统分析。

Method: 利用框架理论分析1D卷积核中的混叠，推导稳定性界限和Parseval稳定性的特征，设计两种高效优化目标来抑制混叠以促进Parseval稳定性，并对随机卷积核混叠效应给出期望和方差的解析表达。

Result: 提出的优化目标有效促进Parseval稳定性，提供了针对短卷积核设计的稳定性估计，并揭示了初始化阶段混叠行为的基本性质。

Conclusion: 基于框架理论的混叠分析和优化方法为确保步幅卷积层的数值稳定性和良好泛化提供了理论和实践支持。

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [415] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: 本文提出了一种用于深度可微逻辑门网络连接部分优化的新方法，通过概率分布选择最佳连接，显著提高了性能并减少了逻辑门数量。


<details>
  <summary>Details</summary>
Motivation: 解决传统逻辑门网络连接固定、效率低下的问题，提升网络性能和训练效率。

Method: 利用概率分布在每个门输入处选择子集连接，再选取最高价值连接和门类型进行训练。

Result: 优化后的逻辑门网络在Yin-Yang、MNIST和Fashion-MNIST数据集上性能优于固定连接的LGNs，且逻辑门数量大幅减少。训练所有连接时，用8000个简单逻辑门即可在MNIST数据集上达到98%以上准确率；相比标准全连接LGNs，门数量减少24倍且性能更优。

Conclusion: 所提出的方法为全可训练布尔逻辑网络提供了新的方向，实现了高效且性能优越的逻辑门网络训练。

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [416] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 本文研究了差分设计技术在基于选择性状态空间层的Mamba架构中的应用，通过改进差分机制提升了模型在语言建模中的检索能力和整体表现。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型如Transformer和RNN存在对无关上下文分配过多注意力的问题，导致性能下降。差分设计被发现可以缓解这一问题，但其在Mamba架构中的效果尚未明确。

Method: 作者提出了一种针对Mamba的新型差分机制，并对架构进行了细致修改，以适应差分设计，并在语言建模任务中进行了实证验证。

Result: 实验结果表明，改进后的Mamba模型在检索能力和整体性能上优于原始Mamba，有效缓解了过度分配注意力的问题。

Conclusion: 差分设计虽对Transformer有效，但直接应用于Mamba效果有限，需专门设计。本文提出的差分机制为Mamba带来性能提升，验证了差分设计在新架构中的适用性。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [417] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: 本文介绍了联想记忆模型（如Hopfield网络），探讨其在信息存储和检索中的作用及其与现代AI架构（如Transformer和扩散模型）的关系。


<details>
  <summary>Details</summary>
Motivation: 重新审视联想记忆网络，发掘其在现代AI体系中的应用和理论价值。

Method: 通过新的拉格朗日方法构建分布式模型，并结合手把手的数学推导和代码实例，深入讲解联想记忆网络。

Result: 展示了联想记忆网络的信息存储能力及其对设计新型AI架构的启发。

Conclusion: 联想记忆网络为理解和设计先进AI模型提供了理论工具和实践指导，具有重要研究价值。

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [418] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: 本论文提出了一种通过神经网络优化波导中固定位置Pinching天线的激活选择，以最大化通信速率的方法，并考虑用户位置不确定性，验证了模型的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的发展，需求更加灵活、高效且成本低廉的天线技术，Pinching天线因其能够动态控制电磁波传播而受到关注。如何在波导中优化这些天线的激活以提升通信性能是关键问题。

Method: 将天线激活问题建模为组合分数0-1二次规划，并利用不同复杂度的神经网络结构，直接从数据中学习激活策略，结合空间特征和信号结构。并将用户位置不确定性纳入训练和评估过程，模拟实际部署场景。

Result: 仿真结果表明，提出的神经网络模型在最大化通信速率方面表现出良好的效果和较强的鲁棒性。

Conclusion: 通过神经网络优化Pinching天线激活选择，考虑实际用户位置不确定性，能够有效提升无线通信系统性能，验证了该方法的实用价值。

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [419] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 本文通过基于二进制决策图的符号搜索方法，利用单核CPU和大内存，生成了89.6GB的四子棋查找表，实现了强解。


<details>
  <summary>Details</summary>
Motivation: 虽然四子棋已通过搜索方法数学求解，但实现强解（查找表形式）被认为不可行，本文旨在突破这一限制。

Method: 采用基于二进制决策图的符号搜索，实现了在标准7×6棋盘上的高效强解生成，包含胜负平评估和alpha-beta搜索以确定最快胜利或最慢失败的走法。

Result: 在单核CPU和128GB内存支持下，47小时内成功生成89.6GB的查找表，开放源代码实现相关功能。

Conclusion: 本文证明了基于符号搜索的二进制决策图方法能够生成实用的强解查找表，推动了四子棋AI的发展。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [420] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出了Chat2SPaT方法，利用大语言模型将模糊的交通信号控制计划描述转换为精确的信号阶段和时序结果，实现结构化信号控制计划管理。


<details>
  <summary>Details</summary>
Motivation: 预设时段的交通信号控制方案创建和更新耗费大量人工，且同一路口需多方案管理，工作重复繁琐，亟需简化信号控制方案管理流程。

Method: 设计Chat2SPaT流程，通过精心设计的提示词，运用大语言模型理解用户半结构化描述，输出json格式的相位序列和属性。结合Python脚本处理交通信号控制细节，生成完整控制方案，并支持交互式多轮编辑。

Result: 在包含300多个信号控制方案描述的测试集中，Chat2SPaT在中英文方案生成准确率均超过94%。

Conclusion: Chat2SPaT为交通信号方案管理提供了首个基于大语言模型的评测基准和易用工具，推动智能交通系统领域语言模型的更精准多样化应用。源代码和数据集均已开源。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [421] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 本文证明了在给定条件下模糊分类聚合函数必须为加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 研究多对象多类型模糊分类聚合函数的性质，以确定满足特定条件的函数形态。

Method: 利用数学证明方法，针对连续个体分类，分析满足最优性、独立性和零一致性的聚合函数形式。

Result: 证明了当对象数量大于等于3，类型数量在2到对象数量之间时，任何满足条件的模糊分类聚合函数必定是加权算术平均。

Conclusion: 在多对象多类型的模糊分类聚合问题中，加权算术平均是唯一满足给定优化与一致性条件的聚合函数形式。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [422] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++是一种针对市政和跨司法管辖区法律规则建模的泊义逻辑图扩展，支持更丰富的节点和边类型，增强了规则的表达能力与推理功能。


<details>
  <summary>Details</summary>
Motivation: 现有法律知识图模型在表达法律义务、例外和层级关系时存在局限，需要一个更丰富且结构化的模型来支持复杂法律规则的语义建模和推理。

Method: 提出了OLG++扩展，增加空间、时间、群体、防御性和逻辑分组等节点与边类型，支持带上下文条件、优先级和复杂触发规则的结构化推理，结合属性图查询实现法律问答。

Result: 通过食品商业法规示例展示了OLG++的表达能力，支持复杂法律义务和例外的建模及查询，且相较LegalRuleML和先前模型，具有更高的表达力和原生支持多类语义结构。

Conclusion: OLG++显著提升了法律法规的图模型表达能力和推理性能，是法律知识表达领域优于现有图模型的有效工具。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [423] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 本文提出了Deep Research Comparator平台，用于评估和比较深度研究代理，通过展示报告及中间步骤，结合细粒度人类反馈，提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 目前评估自动深度研究代理的挑战在于难以有效评估长报告及其中间步骤，缺乏细致反馈机制。

Method: 构建一个集成多个深度研究代理的比较平台，支持报告和中间步骤的并排展示，收集详细的人类反馈，并开发Simple Deepresearch作为基础代理框架。

Result: 平台成功收集了17名标注者对三种深度研究代理的真实用户偏好数据，证明其在代理评估和开发中的实用性。

Conclusion: Deep Research Comparator提供了一个全面的框架，显著改善了深度研究代理的评估方式，促进了相关代理的发展和优化。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [424] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文提出了一个针对增强现实(AR)培训的视觉语言模型(VLM)数据集，并评估了九种先进VLM，发现它们在细粒度装配任务上的表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在增强现实培训应用中未得到充分探索，尤其是在细粒度视觉语言理解方面表现不足。

Method: 构建一个系统化的AR培训视觉语言任务数据集，并对九种先进VLM进行评测。

Result: 即使是先进模型如GPT-4o，在状态检测任务中的最高F1分数仅为40.54%，揭示了模型在细粒度视觉语言对齐上的不足。

Conclusion: 该研究强调需要更好的数据集和基准，并推动技术进步，以提升细粒度视觉语言模型性能，同时促进视障用户的公平学习机会。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [425] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 本文提出了使用答案集编程(ASP)中的默认否定和强否定来优雅地表达规范模态逻辑中的模态算子，并采用ASP的全局约束表示义务和禁止，成功解决了规范模态逻辑中的各种悖论。


<details>
  <summary>Details</summary>
Motivation: 规范模态逻辑的实现存在困难，尤其是如何表达模态算子和解决相应的逻辑悖论。

Method: 利用答案集编程中的默认否定和强否定表达模态算子，采用ASP的全局约束来表示义务和禁止。

Result: 该方法能够优雅地表达规范模态逻辑的模态算子，并有效避免了多种规范逻辑悖论。

Conclusion: 基于ASP的表达方式为规范模态逻辑的实现提供了一种简洁且有效的解决方案。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [426] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 本文提出了一种结合多模态模型微调、结构化推理和智能检索增强生成的皮肤病视觉问答方法，在2025 ImageCLEF MEDIQA-MAGIC竞赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 现实远程医疗中诊断需依赖有限输入、高准确性和可解释性，模拟皮肤科医生系统化推理以提高自动诊断支持。

Method: 微调Qwen、Gemma、LLaMA多模态模型，加入结构化推理层整合模型输出，并结合基于美国皮肤病学会数据库的智能检索增强生成技术。

Result: 团队提交的方案获得竞赛第二名且排名第六，表现出高准确性和竞争力。

Conclusion: 该方法为远程医疗中基于图文的自动诊断支持提供了可靠路径，提升了诊断的准确性与可解释性。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [427] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文提出了WikiHowAgent，一个利用大语言模型的多智能体系统，用以模拟师生互动教学对话，支持程序化学习并评估教学质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏可扩展性，难以利用大规模多样化课程内容，且缺少评估教学质量的有效框架。

Method: 设计包含教师智能体、学习者智能体、交互管理器和评估器的多智能体工作流，基于114,296条师生对话数据集进行教学模拟与质量评估，结合计算指标、量表指标及人工判断进行评价。

Result: 该工作流在多种环境下表现出良好的有效性，揭示了大语言模型在跨领域教学任务中的潜力。

Conclusion: WikiHowAgent为基于大语言模型的教育对话系统提供了可扩展且高质量的教学模拟与评估框架，数据集和实现均已开源。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [428] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 本文批判性审视了AI红队实务，指出当前聚焦模型缺陷忽视系统层面和复杂交互的不足，提出了涵盖宏观系统与微观模型的综合红队框架，并强调多功能团队与系统思维的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI红队主要关注模型漏洞，忽视了复杂的社会技术系统及模型、用户、环境间的交互，导致风险评估不全面。

Method: 提出一个涵盖整个AI开发生命周期的宏观系统红队和微观模型红队相结合的综合框架，结合网络安全经验与系统理论，设计多功能团队的建议方案。

Result: 该框架促进从系统层面识别新兴风险和系统性脆弱性，提升AI红队在治理中的有效性。

Conclusion: 有效的AI红队应超越模型层面，关注社会技术系统的复杂互动，采用多学科团队和系统性思维，以更全面地发现和缓解风险。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [429] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 本文利用大型语言模型GPT-4o-mini生成反事实解释，提升了异常预防和模型鲁棒性的能力。


<details>
  <summary>Details</summary>
Motivation: 反事实解释能够以最小变动改变预测结果，为异常预防和增强模型训练提供人性化见解。

Method: 采用零样本和三样本设置下的GPT-4o-mini生成反事实解释，并在压力预测和心脏疾病检测两个数据集上进行评估。

Result: 相比传统方法，LLM生成的反事实解释在合理性、有效性和稀疏性方面表现优异，并且作为数据增强显著提升下游分类器的准确率。

Conclusion: 提示式生成技术在临床和生理预测任务中具有增强解释能力和模型鲁棒性的潜力。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [430] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: 本文提出了SingLoRA，一种基于单低秩矩阵及其转置的参数高效微调方法，解决了传统LoRA中矩阵尺度不匹配导致的不稳定训练问题，显著提升了性能并降低了参数量。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法中两个低秩矩阵存在尺度差异，导致训练过程不稳定，性能受限。

Method: SingLoRA通过以单一低秩矩阵及其转置分解的方式来更新权重，避免了矩阵间的尺度冲突，从而保证训练稳定且参数量减半。

Result: 在多项任务中，SingLoRA表现优异：例如在常识推理任务中，使用SingLoRA微调LLama 7B模型，准确率达91.3%，超过LoRA和LoRA+，且参数仅为其60%；在图像生成任务中，使用SingLoRA微调Stable Diffusion，图像相似度指标显著提升。

Conclusion: SingLoRA通过重新设计低秩适配结构，有效解决了训练不稳定问题，实现更稳定、参数更少且性能更优的微调方案，适用于多种任务。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [431] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了人工智能测量的形式理论，旨在促进对AI系统和评估方法的比较，结合风险分析技术，并强调测量操作对AI能力的影响。


<details>
  <summary>Details</summary>
Motivation: 当前AI测量缺乏统一的理论框架，导致评估方法多样且难以比较，且AI能力定义依赖于测量方式，亟需形式化测量理论。

Method: 作者提出了一个分层的测量结构，区分直接和间接可观测量，构建一个可校准的AI现象分类方法。

Result: 该框架能够使研究者、从业者和监管者更有效地比较AI系统，结合工程和安全科学的风险分析，并明确测量操作对AI能力定义的影响。

Conclusion: 通过形式化的测量理论，可以实现AI系统的统一分类和标准化评估，从而推动AI评估研究和实践的发展。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [432] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的多模态大语言模型MLlm-DR，用于自动抑郁诊断，能够生成抑郁评分及解释，提升了诊断的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动抑郁诊断方法缺乏对评分确定过程的清晰解释，且现有多模态大语言模型缺乏面向访谈数据的训练，导致诊断性能不佳，限制了临床应用。

Method: 设计了一种整合小型大语言模型和轻量查询模块（LQ-former）的多模态大语言模型MLlm-DR。小型LLM负责生成抑郁评分及评估理由，利用构建的坚实训练集对其进行微调以增强逻辑推理能力；LQ-former则从语音和视觉数据中提取抑郁相关特征，实现多模态信息融合。

Result: 在两个基于访谈的基准数据集CMDC和E-DAIC-WOZ上取得了最先进的诊断效果，验证了方法的有效性和优越性。

Conclusion: 所提多模态大语言模型MLlm-DR在自动抑郁诊断中实现了可解释性和高性能，有望推动临床抑郁诊断的应用。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [433] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型(LLMs)在岩土工程领域的适应与应用，包括关键适应方法和实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 尽管通用型大型语言模型功能强大，但在岩土工程领域的有效应用仍需领域特定的调整，以优化工作流程。

Method: 本文总结了岩土领域适应的关键技术，如提示工程、基于检索的生成、领域自适应预训练和微调，并评述了各类实际应用。

Result: 探讨了岩土工程适应型LLMs在地质解释、地下表征、现场规划、设计计算、数值模拟、安全评估和教学等方面的应用效果及其优势与不足。

Conclusion: 该综述为岩土工程师整合LLMs提供指导，也为学术界进一步研究该跨学科领域奠定基础。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [434] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于注意力扩散模型的方法（ADMC），用于多模态情感和意图识别中的缺失模态特征补全，提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感和意图识别面临传感器故障或数据不完整导致的模态缺失问题，传统方法生成的特征往往过度耦合且不准确。

Method: 构建独立训练的特征提取网络以保持各模态特征独立性，利用基于注意力的扩散网络（ADN）生成符合真实分布的缺失模态特征，同时支持跨模态生成以提升全模态识别性能。

Result: 在IEMOCAP和MIntRec两个基准数据集上，方法在缺失模态和完整模态的情感和意图识别任务中均达到了先进水平。

Conclusion: 所提基于注意力扩散的缺失模态特征补全方法有效解决了模态缺失问题，并提升了多模态情感和意图识别的性能。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [435] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 本研究通过实证研究验证了大型语言模型生成的检索练习题能显著提升学生的知识保持率。


<details>
  <summary>Details</summary>
Motivation: 生成高质量的检索练习题费时费力，尤其在技术快速发展的领域，利用大型语言模型实现自动生成具有潜力，但其效果尚未明确。

Method: 在两门大学数据科学课程中，约60名学生参与，比较一周内使用LLM生成的选择题检索练习与无练习的学习结果。

Result: 使用LLM生成的检索练习的学生知识保持率显著提高，平均正确率从73%提升至89%。

Conclusion: LLM生成的检索练习题能有效支持学生学习，具有推广价值，但其质量需教师手动审核以确保准确性。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [436] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的信息传播仿真环境，通过引入情感引导的记忆和社会信息处理链式思考机制（SIP-CoT），提升了模拟中代理人的心理和行为真实感。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和生成式人工智能的快速发展加速了错误信息的传播，传统模型无法充分模拟在线用户的心理与行为动态。当前大语言模型缺乏情感处理，导致模拟行为与真实人类存在较大差距。

Method: 基于社会信息处理理论，设计了基于情绪引导记忆的SIP-CoT机制，使LLM能够更好地理解社会线索、个性化目标设定和反馈评估，从而改善信息传播模拟的心理真实性。

Result: 实验结果表明，采用SIP-CoT机制的LLM代理人在处理社会信息时表现出更贴近真实人类的行为、态度和情感，提升了模拟的准确性和社会智能水平。

Conclusion: 当前基于LLM的信息传播模拟存在显著不足，整合SIP-CoT和情感记忆机制能显著增强模拟的心理真实性和社会智能，推动更有效的信息传播控制研究。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [437] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 本论文提出利用司法数据构建司法绩效指标体系，基于此进行城市级外商直接投资预测。


<details>
  <summary>Details</summary>
Motivation: 传统基于经济数据的外商直接投资预测存在易被操控的问题，需寻求更可靠的数据源。

Method: 构建基于1200万裁判文书的司法绩效指标体系，形成表格数据集，提出融合行列数据和多专家模型的司法数据表格学习方法（TLJD）进行预测。

Result: 在跨城市和跨时间的预测任务中，TLJD方法在多项指标上优于十个先进基线，R2值达到至少0.92。

Conclusion: 利用司法数据进行外商直接投资预测更为可靠且有效，TLJD方法表现优异，具备实际应用价值。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [438] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 本研究比较了皮肤科专家与两种AI模型生成的治疗方案，发现人类评估者偏好专家方案，而高阶AI评估者则更认可AI方案，表现出评估者性质对方案评价的决定性影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI在诊断以外领域的应用扩大，如何评估AI生成的治疗方案成为关键挑战。

Method: 由10位皮肤科医生、一款通用AI（GPT-4o）、及一款推理AI（o3）为五个复杂病例制定方案，后经匿名化评分；专家组与更高级AI (Gemini 2.5 Pro)分别评价这些方案。

Result: 专家组评分中人类方案优于AI方案，且GPT-4o排名优于推理模型o3；AI评分则相反，推理模型最佳，人类方案得分最低。

Conclusion: 方案质量评价强烈依赖评估者性质，经验驱动与数据驱动逻辑存在巨大差异，未来需发展可解释的人机协同系统以弥合此差异，提升临床护理水平。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [439] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: 该论文提出了ModelAuditor，一种能够在临床实践中检测和修复AI模型性能退化的工具。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医疗成像中的表现虽达到专家水平，但在真实世界中受到设备、环境及人群差异影响，性能大幅下降，且现有的可靠性审计过程复杂且费时。

Method: ModelAuditor作为一个自省代理，通过与用户对话选择任务特定指标，并模拟临床相关的分布变化，生成可解释的报告，识别性能下降原因及提出修复建议。

Result: 在三个实际临床案例中，ModelAuditor成功识别了状态模型的失败模式，提出的策略使性能恢复15-25%，优于基线和现有增广方法。

Conclusion: ModelAuditor通过多代理架构，既高效又经济，在临床部署前可有效审计模型性能，提升医疗AI的可靠性和实用性。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [440] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 提出了一种通过充电结束时的放电脉冲分析电池健康状态（SoH）的方法，适合微电网中实时监控电池健康。


<details>
  <summary>Details</summary>
Motivation: 微电网操作限制传统SoH监测方法的使用，需创新实时监测技术。

Method: 利用等效电路模型参数，解析放电脉冲期间端电压变化，训练模型预测SoH。

Result: 在85%容量退化电池参数训练下，预测90% SoH电池，平均绝对误差约1%，解释性评分约0.9。

Conclusion: 该方法若性能稳定，可集成于电池管理系统，实现连续运行下的电池优化管理。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [441] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了GTA1，一个针对跨平台GUI操作的智能代理，通过测试时扩展方法选择最佳动作方案，并结合强化学习提升视觉元素定位精度，显著提高任务完成率，在多个基准测试中表现领先。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在任务规划中的不确定性和在高分辨率复杂界面中精确定位交互目标的两大挑战。

Method: 引入测试时扩展方法，多候选动作采样结合判别模型选择最佳动作，同时利用强化学习优化动作方案与视觉目标的准确匹配。

Result: GTA1在多个基准测试中表现优异，如Screenspot-Pro、Screenspot-V2和OSWorld-G准确率分别达到50.1%、92.4%和67.7%，任务成功率达45.2%。

Conclusion: 测试时扩展和强化学习方法有效提升了GUI代理的决策质量和视觉定位能力，推动了跨平台GUI自动化任务执行的性能边界。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [442] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 文章提出了一个新的中文早产儿视网膜病变(ROP)风险预测数据集CROP，并开发了Affective-ROPTester框架，用以评估大语言模型在ROP风险预测中的表现及情感偏差。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在ROP风险预测方面的应用和表现尚未被充分研究，尤其是情感偏差对预测结果的影响缺乏系统探讨。

Method: 构建包含993条病例的CROP数据集，设计了Instruction、Chain-of-Thought和In-Context Learning三种提示策略，通过集成情感因素的提示方式，系统评估模型的预测能力与情感偏差。

Result: 模型仅凭内在知识预测效果有限，但运用外部结构化医学知识显著提升了准确性。模型存在情感偏差，倾向于高估中高风险病例；正面情感提示有助于减轻这一偏差。

Conclusion: 情感敏感的提示设计对于提升临床诊断的可靠性至关重要，Affective-ROPTester框架有效评估并缓解了临床语言模型中的情感偏差。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [443] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: 本文探讨了人工智能在游戏中的表现与人类直觉决策的差异，提出了基于认知心理学观察的CogniPlay模型。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在多种游戏中表现优异，但缺乏人类模式化、直觉式决策过程。

Method: 综述认知心理学和相关人工智能行为建模的研究，并基于此提出CogniPlay模型。

Result: 提出了一个基于认知心理学的人工智能模型CogniPlay，用以更好模拟人类行为。

Conclusion: CogniPlay模型有望弥合现有AI与人类直觉决策之间的差距，提高通用游戏人工智能的“人性化”水平。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [444] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 本文提出了神经符号转换系统这一计算模型，旨在结合传统符号算法和大型语言模型，提升自动推理工具的性能与保障。


<details>
  <summary>Details</summary>
Motivation: 当前构建神经符号自动推理系统多为临时性的程序设计模型，缺乏传统符号算法的强保障，也未能充分结合神经网络与符号推理，限制了LLM的推理潜力发挥。

Method: 提出神经符号转换系统，在该模型中，符号状态与“直觉”并行处理，状态转换同时操作符号和直觉，实现神经符号推理的紧密结合。

Result: 该模型能扩展逻辑推理能力，同时保留符号算法的强保障，且有潜力以逻辑编程语言实现。

Conclusion: 神经符号转换系统为构建具备强保障和高效推理能力的神经符号自动推理工具提供了坚实的计算模型基础。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [445] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 本文针对时间序列预测中序列表示、信息提取和目标投射三大核心阶段，评估了多种模块架构，在七个基准数据集上实现了最先进的预测精度和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临序列表示有效性、内存构建及精确预测目标的挑战，不同任务对模型设计提出多样需求。

Method: 将时间序列预测流程分解为输入表示、信息提取与内存构建、目标投射三阶段，分别评估卷积层和自注意力机制等多种架构配置。

Result: 所提模型在七个基准数据集上取得了最先进的预测准确度，并显著降低了训练和推理时间及模型参数数量。

Conclusion: 系统地分解并优化时间序列预测的核心阶段能同时提升预测精度与计算效率，验证了模块化设计优势。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [446] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 本文提出了MusiScene模型，用于根据音乐生成与之匹配的视频场景描述，实现音乐场景想象（MSI）任务。


<details>
  <summary>Details</summary>
Motivation: 人类听音乐时会联想到相应的电影场景，现有音乐字幕模型只关注音乐本身，缺少跨模态场景联想能力。

Method: 构建了一个包含3,371个视频-音频对的大规模数据集，基于Music Understanding LLaMA微调训练MusiScene模型，并对模型进行全面评估。

Result: MusiScene在生成的音乐场景描述上表现出比MU-LLaMA更好的上下文相关性。同时利用生成的字幕提升了基于文本的视频背景音乐生成。

Conclusion: 通过跨模态学习，MusiScene有效实现了音乐场景想象，增强了音乐与视频的结合，拓展了音乐语言模型的应用。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [447] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: BlueLM-2.5-3B是一款紧凑且统一的3B参数级多模态大型语言模型，支持思考和非思考模式，适合边缘设备高效部署，具备强大的多模态与文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一款体积小、性能强大且适合边缘设备部署的多模态大型语言模型，解决现有模型对设备资源要求高、对思考过程控制不足的问题。

Method: 通过多元化数据筛选、关键数据重采样、混合异构强化学习以及高性能训练设施等技术手段训练模型，实现思考与非思考模式并支持思考令牌预算控制。

Result: BlueLM-2.5-3B在多模态和纯文本任务中表现优异，思考模式下性能接近更大规模模型，非思考模式超过同规模竞品，且训练数据需求显著减少，表现出优异的数据效率。

Conclusion: BlueLM-2.5-3B成功实现了高性能、低资源消耗的边缘部署多模态语言模型，推动了高效本地多模态智能的发展，具有重要的研究和应用价值。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [448] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 文章提出了一种统一的基础模型，利用因果Transformer和补丁掩码策略，实现多任务无线网络参数预测，支持多样的预测间隔，具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 无线通信网络日益复杂，准确预测关键参数如CSI、用户位置和网络流量对物理层和MAC层任务至关重要，而传统深度学习方法难以跨场景和任务泛化。

Method: 提出基于因果Transformer的统一基础模型，采用单变量分解统一任务，编码时间间隔粒度，并引入补丁掩码策略支持任意输入长度。

Result: 在大规模数据集训练后，模型在未知场景表现优异，实现了新任务的零样本预测，超过传统全样本基线。

Conclusion: 该基础模型有效提升了无线网络多任务预测的泛化能力和准确性，适用于多样预测需求，推动无线网络参数预测技术发展。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [449] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 本文提出了一种基于属性归因的方法，提升基于可解释人工智能模型的乳腺癌淋巴结放疗后手臂淋巴水肿风险评估的模型解释性。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的人工智能技术缺乏透明度，限制了其在医疗决策中的接受和解释。

Method: 通过使用信息检索中的标准指标，对规则基础的预测模型中的属性进行统计分析，计算每个属性对预测的相关性，提供用户可解释的风险因子影响信息。

Result: 用户研究显示，所提出的方法相比原始的可解释AI模型输出，提供了更高的解释性和实用性。

Conclusion: 基于属性归因的分析方法能够提升医疗场景中可解释人工智能模型的清晰度和用户接受度，有助于淋巴水肿风险预测的理解和应用。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [450] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: HopeBot是一款基于大型语言模型的聊天机器人，通过互动和实时澄清进行抑郁症筛查，研究显示其评分与传统PHQ-9高度一致，用户信任度高且使用体验良好。


<details>
  <summary>Details</summary>
Motivation: 传统静态问卷如PHQ-9有效筛查抑郁症但缺乏互动性和适应性，亟需更具交互性的工具。

Method: 开发HopeBot聊天机器人，结合大型语言模型、检索增强生成与实时澄清技术，进行PHQ-9问卷的口语化管理，并在英国和中国132名成人中进行对比研究。

Result: HopeBot评分与传统自填问卷高度一致（ICC=0.91），71%参与者更信任聊天机器人，舒适度、语音清晰度及敏感话题处理均获得较高评分，87.1%表现出愿意重复使用或推荐。

Conclusion: 基于大型语言模型的语音聊天机器人能够作为抑郁症常规筛查的可行、低负担且可扩展的辅助工具。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [451] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的文本到SQL转换框架CogniSQL-R1-Zero，实现了更高的执行准确率，尤其在复杂查询中表现优异，并发布了两个辅助数据集。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL的转换在生成正确且可执行的复杂SQL查询方面仍存在挑战，传统方法依赖中间监督和复杂奖励机制，训练成本高且效果有限。

Method: 提出利用基于执行正确性和格式标签的轻量级奖励信号进行强化学习，避免中间监督和复杂奖励设计，促进学习的稳定性和模型与任务目标的对齐。

Result: 在Text2SQL基准测试BIRD上，CogniSQL-R1-Zero超越了包括SFT CodeS-7B、DeepSeek-Coder 236B和Mistral 123B在内的现有模型，且仅用较小的7B模型骨干和4块NVIDIA A100 GPU训练。

Conclusion: 强化学习方法证明了在文本到SQL领域的效率和扩展性，同时发布的数据集可推动高效且可解释的文本到SQL研究，促进可执行程序生成的稳定与准确。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [452] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: 本文提出FGNS方法，通过结合局部和全局特征重要性选择具有代表性的邻居实例，提升模型解释的可理解度和用户对模型错误的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释人工智能（XAI）方法难以为非领域专家用户生成清晰和易理解的解释，导致用户难以有效识别模型错误。

Method: 提出了一种后置处理方法——特征引导邻居选择（FGNS），该方法结合局部与全局特征重要性指标，选择更具类别代表性的样本作为邻居而非单纯基于特征空间距离。

Result: 用户研究显示，FGNS显著提升了非专家对模型错误的识别能力，且决策速度和准确率优于传统k-NN解释。定量分析表明FGNS所选邻居更一致且紧密聚类于类别原型。

Conclusion: FGNS促进了更加符合人类认知的模型评估方式，但还需进一步工作以缩小解释质量与用户信任感之间的差距。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [453] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 本文在Lockean论点框架下，研究了置信度确定的信念集合，提出了使信念集在经典逻辑下封闭的新方法，并设计了概率更新的最小修正策略。


<details>
  <summary>Details</summary>
Motivation: 传统的Lockean信念集合在某些场合（如信念变化理论）中不满足经典逻辑推理封闭性，限制了其应用。

Method: 本文提供了两种信念集合经典逻辑封闭的刻画方法，并提出了一种基于概率更新的最小修正策略，使信念集在接受新信息时最少变动并保持逻辑封闭。

Result: 成功构建了可实现经典逻辑封闭的信念集合模型，并通过最小修正概率更新方法实现信念变更，保证了有效且合理的信念调整。

Conclusion: 本文实现了在Lockean框架下信念集合的逻辑封闭以及基于概率的最小修正更新，为信念变化理论提供了新的理论工具和方法。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [454] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: FEVO框架通过多阶段训练显著提升大型语言模型在金融领域的表现，实现了多个金融基准的顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 当前在金融领域应用LLM的研究有限，且金融任务需要丰富的领域知识和复杂的推理能力。

Method: FEVO采用持续预训练扩展金融知识，监督微调灌输结构化推理，强化学习融合知识与推理，结合前沿推理模型和规则过滤构建高质量训练数据。

Result: FEVO系列模型在七个基准测试中表现优异，特别是FEVO-R32B在五个金融基准上超过更大型和专业模型。

Conclusion: 通过扩展领域知识和结构化推理训练，FEVO有效提升了LLM在金融任务中的性能，验证了多阶段强化训练策略的有效性。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [455] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 本文提出了一种结合LSTM、遗传算法和SHAP的AI框架，用于提高医疗设施能耗管理的预测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 医疗设施能耗需求波动大，传统管理方法效率低且成本高，急需高效能量管理解决方案。

Method: 采用LSTM进行能耗时间序列预测，利用遗传算法优化模型参数和负载平衡策略，并用SHAP增强模型解释性。

Result: LSTM在能耗预测中的MAE和RMSE显著优于ARIMA和Prophet模型，遗传算法提升了模型适应性，SHAP提高了透明度。

Conclusion: 该LSTM-GA-SHAP综合方法有效提升了医疗能耗预测准确率和管理效率，促进可持续发展，未来可结合实时部署和强化学习进一步优化。

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [456] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: 本文提出了OpenAgentSafety，一个用于评估AI代理安全性的综合框架，支持真实工具和多样任务，揭示了当前主流大语言模型在安全性上的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理安全评估方法多依赖模拟环境和狭窄任务，缺乏对真实工具交互和多样风险的全面检测，急需一个更完善的评估框架。

Method: 提出OpenAgentSafety框架，涵盖八大风险类别，支持真实工具交互及350+多轮多用户任务，并结合规则分析与大语言模型裁判机制检测安全隐患。

Result: 对五种主流大语言模型进行实证分析，发现它们在51.2%至72.7%的安全敏感任务中存在不安全行为，暴露出显著安全漏洞。

Conclusion: OpenAgentSafety框架有效揭示了AI代理的安全风险，强调部署前需加强安全保障措施，提供了一个可扩展的评估平台以促进更安全的AI应用。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [457] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 本文提出了delta learning假设，利用成对的偏好数据对语言模型进行微调，即使单个数据点质量较弱，也能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当强监督数据稀缺时，单纯依赖高质量训练数据限制了模型性能的提升，故探索利用弱监督数据中的相对质量差异进行有效学习。

Method: 通过构造由较小模型生成的成对偏好数据，利用相对质量差异进行偏好微调，验证delta learning假设，并在大规模8B模型上进行训练。

Result: 在11个标准基准测试（如MATH、MMLU）上，基于弱监督偏好数据的训练结果匹配了使用更强监督（如GPT-4o）的先进开放模型Tulu 3的表现。

Conclusion: delta learning表明即使单个数据点较弱，通过成对弱数据的相对质量差异也能有效指导模型学习，实现简单且经济的模型后训练。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [458] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 本文研究在缺乏完整因果图知识时，通过因果抽象集合来识别因果效应，提出了多个可识别性标准并构建了层级结构。


<details>
  <summary>Details</summary>
Motivation: 实际应用中完整的因果图往往未知，尤其是在复杂和高维场景，需寻找保留部分因果信息的简化表示来识别因果效应。

Method: 将因果抽象形式化为因果图集合，提出并形式化多个因果查询的可识别性标准，构建标准之间的层级关系。

Result: 建立了因果识别标准的层级框架，揭示了不同因果知识水平下的可识别性，辅以文献实例说明并提供推理工具。

Conclusion: 该方法为缺乏完整因果知识时的因果效应识别提供了系统化的理论框架和实用工具，促进对因果识别条件的理解。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [459] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 本文提出了一种名为Aligned Scoring Rule (ASR)的新型评分规则，旨在更好地与人类对文本的偏好保持一致，同时保持评分的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的正确评分规则虽然确保了报告真实信念能最大化期望分数，但并不总是与人类对文本的偏好匹配。

Method: 设计了一个优化方案，通过最小化适当评分规则与参考分数（如人类评分）之间的均方误差来构造ASR。

Result: 实验结果显示，ASR在与人类偏好的一致性方面优于先前的方法，并且保持了评分规则的正确性。

Conclusion: ASR成功地解决了评分规则与人类文本偏好不一致的问题，提升了文本预测评分的实用性。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


### [460] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 本文通过基于二叉决策图的符号搜索方法，成功生成了一个89.6GB的强解查找表，实现了7×6尺寸棋盘的桌游Connect-Four的快速获胜或延迟失败策略。


<details>
  <summary>Details</summary>
Motivation: 尽管Connect-Four已被数学破解并可通过搜索方法找到最佳走法，但生成形式为查找表的强解方案一直被认为不可行。

Method: 采用基于二叉决策图的符号搜索方法，使用高效实现在单CPU核心和128GB内存上计算查找表，同时结合alpha-beta搜索找最快胜或最慢败的走法。

Result: 成功生成了89.6GB的查找表，用时47小时，支持标准7×6棋盘，包含胜-平-负评估及快速胜负策略。

Conclusion: 符号搜索方法结合高效实现使得生成大规模强解查找表成为可能，同时提供了改进的策略选择途径，有助于提升游戏决策质量。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [461] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出Chat2SPaT，一种利用大语言模型（LLMs）将用户对交通信号控制计划的半结构化描述转换为精准信号相位和定时（SPaT）结果的方法，提升了信号控制计划的管理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 交通信号控制尤其是预定时信号控制涉及大量人工制定和更新计划，且多个时段计划导致重复输入，亟需简化且友好的管理流程。

Method: 采用经过策划的提示语，利用LLMs理解用户的描述并输出json格式的相位序列和属性，结合Python脚本处理周期内相位定位及细节，最终生成完整信号控制计划，支持迭代编辑。

Result: 在包含300余条计划描述的测试数据集上，Chat2SPaT对中英文计划生成准确率超过94%。

Conclusion: Chat2SPaT首次建立了评估LLMs理解交通信号控制计划的基准，提供了一个便捷的信号控制计划管理方案，有望成为智能交通系统领域利用LLMs的基础模块。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [462] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 本文证明了在对多个对象进行模糊分类时，满足最优性、独立性和零一致性的聚合函数必然是加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效聚合一系列个体对多个对象的模糊分类，确保聚合函数满足某些合理条件。

Method: 通过数学证明，分析了满足最优性、独立性和零一致性条件的模糊分类聚合函数的结构。

Result: 证明了对于$m\ge 3$个对象和$2\le p\le m$种类型的分类，聚合函数必然是加权算术平均。

Conclusion: 在特定条件下，模糊分类的最优聚合方式是加权算术平均，这为相关分类方法提供了理论支持。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [463] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++ 是对义务逻辑图的语义扩展，增强了法律规则建模能力，支持复杂的法律义务、例外和层级关系。


<details>
  <summary>Details</summary>
Motivation: 现有法律知识图模型在表达法律义务的细粒度和复杂上下文方面存在不足，亟需一个更丰富的语义框架。

Method: 通过引入多种节点和边的类型，包括空间、时间、参与方分组、可例外性及逻辑分组，OLG++ 允许表示复杂的法律义务结构和推理。

Result: 通过食品法规实例展示了OLG++的表达能力，支持基于图查询的法律问答，并优于LegalRuleML和其他图模型。

Conclusion: OLG++提供了更具表现力和结构化的法律知识建模方法，适合复杂法律场景的表述和推理。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [464] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 提出Deep Research Comparator平台，实现深度研究代理的全面评估和反馈收集。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效方法评估自动搜索、分析信息并生成长报告的深度研究代理，特别是无法详细反馈中间步骤。

Method: 搭建包含代理托管、并排比较、细粒度人工反馈和排名计算的综合平台，设计Simple Deepresearch作为端到端代理基线框架，便于集成不同大语言模型。

Result: 从17位标注者处收集了3个深度研究代理的用户偏好数据，验证了平台的实用性。

Conclusion: Deep Research Comparator有效支持深度研究代理的开发评估，促进了代理间的细致比较和改进。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [465] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 该论文介绍了一个专门用于增强现实(AR)训练的视觉语言模型(VLM)数据集，并评估了九个先进模型，结果显示现有模型在细粒度装配任务上表现有限，提出了提升该领域的方法和社会影响。


<details>
  <summary>Details</summary>
Motivation: 目前VLMs在多模态环境中表现优异，但在AR训练中的应用尚未被充分探索，尤其是细粒度装配任务存在显著挑战。

Method: 构建了一个系统化的视觉语言任务数据集，专为AR训练设计，并对九个先进的VLM模型进行了全面评估。

Result: 所有模型在细粒度装配任务的状态检测上表现不佳，最高F1得分仅为40.54%。

Conclusion: 需要改进数据集和基准，深化细粒度视觉语言对齐的研究，促进AR训练中VLMs的实际应用，同时提升视障用户获得AI驱动学习机会的公平性。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [466] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 本文提出利用answer set programming (ASP)中的默认否定和强否定实现义务模态逻辑，使用ASP的全局约束表示义务和禁止，从而优雅解决传统的模态逻辑悖论。


<details>
  <summary>Details</summary>
Motivation: 传统的义务模态逻辑存在多种悖论，实现起来复杂，亟需一种简洁优雅的表达方法。

Method: 通过使用ASP中的默认否定和强否定表达模态算子，利用ASP的全局约束来表示义务和禁止。

Result: 提出的表示方法能够优雅解决义务模态逻辑中的各种悖论问题。

Conclusion: ASP中默认否定和强否定结合全局约束为实现义务模态逻辑提供了一种有效且简洁的方式，解决了传统方法存在的悖论。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [467] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 该论文介绍了在2025年ImageCLEF MEDIQA-MAGIC挑战赛中的一项多模态皮肤病问答与分割任务，主要关注基于图像和症状描述的多项选择题临床诊断。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中需要异步、高准确率和可解释性的皮肤病诊断问题，模拟皮肤科医生系统化推理的诊断方式。

Method: 结合三个核心组件：微调多个开源多模态模型；引入结构化推理层以整合模型输出；利用美国皮肤病学会数据库的检索增强生成技术填补患者信息不足。

Result: 团队以第六名成绩获得第二名，表现出竞争力和高准确率。

Conclusion: 该方法为实现更可靠的自动化诊断支持系统提供了途径，提升了远程医疗诊断决策的准确性和解释性。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [468] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文提出了WikiHowAgent，一个利用大语言模型模拟交互式教学对话的多代理工作流，整合教师和学习者代理及评估系统，实现过程学习和教学质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏扩展性，未充分利用多样化、大规模课程内容，且缺少系统评估教学质量的框架。

Method: 设计了WikiHowAgent多代理系统，包括教师和学习者代理、交互管理器及评估器，利用114,296条教师-学习者对话数据，覆盖17个领域的14,287个教程。采用计算指标、评分量表及人工评估相结合的评估协议。

Result: 该工作流在不同场景下表现有效，提供了大语言模型在各领域教学中的能力见解。数据集和代码已开源。

Conclusion: WikiHowAgent有效促进了过程性学习与教学质量评估，推动了大语言模型在教育领域的应用与研究。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [469] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 本文批判性地审视了AI红队测试的实践，指出其过于集中于模型层面的漏洞，而忽视了复杂社会技术系统中模型与使用者及环境的互动。提出了宏观和微观两个层面的综合红队测试框架，并强调多功能团队的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI红队测试主要关注单一模型漏洞，忽视了更广泛的社会技术系统和复杂交互导致的风险，因此需要一个更全面的红队测试框架。

Method: 提出一个覆盖整个AI开发生命周期的宏观系统红队测试和微观模型红队测试框架，结合网络安全和系统理论经验，推荐多功能团队来识别系统性风险和技术社会因素的相互作用。

Result: 提出的框架弥补了现有红队测试的不足，强调了跨系统视角和多学科团队在识别和缓解AI系统复杂风险中的关键作用。

Conclusion: 有效的AI红队测试应超越单模型漏洞发现，采用宏观系统视角和多功能团队，以应对AI系统中出现的系统性及复杂风险。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [470] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 该论文利用大型语言模型（GPT-4o-mini）生成反事实解释，用于异常预防和数据增强，显著提升预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 反事实解释可提供人性化的预测洞察，帮助异常预防且可用于增强训练数据，从而提升模型的鲁棒性和解释性。

Method: 采用零射击及三射击方式，利用大语言模型生成反事实解释，并在压力预测和心脏病检测两个数据集上与传统方法对比评估。

Result: 提出的方法在合理性（最高99%）、有效性（最高0.99）和稀疏性方面表现优异，且通过增强数据提升下游分类器准确率平均5%。

Conclusion: 基于提示的大语言模型生成反事实解释在临床和生理预测任务中有效提升模型的解释性和鲁棒性。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [471] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: SingLoRA通过单一低秩矩阵及其转置的分解，优化了LoRA方法，提升训练稳定性，减少参数量，同时在多任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法中两矩阵之间存在尺度差异导致训练不稳定，影响性能。

Method: 提出SingLoRA，将权重更新设计为单一低秩矩阵乘以其转置，消除尺度冲突，稳定优化并减少参数量。

Result: 在MNLI任务中，SingLoRA以更少参数达到91.3%准确率，优于LoRA和LoRA+；在DreamBooth图像生成任务中提高了图像保真度。

Conclusion: SingLoRA通过简化低秩适配结构，实现了更稳定、高效的参数微调，提升了多任务性能表现。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [472] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了构建人工智能测量的形式理论，旨在实现AI系统和评估方法之间的比较，结合工程安全领域的定量风险分析技术，并强调AI能力的测量依赖于所选择的操作和尺度。


<details>
  <summary>Details</summary>
Motivation: 希望通过形式化的测量方法，促进AI系统评估的标准化和可比性，连接AI评估与传统风险分析方法，并明确AI能力的测量本质。

Method: 提出了一个分层测量框架，区分直接和间接可观测指标，作为统一、可校准的AI现象分类路径。

Result: 建立了测量架构示意，展示了如何通过不同层次的测量元素实现系统的统一评估。

Conclusion: 形式化的AI测量理论将推动AI评估方法的统一化和科学化，有助于AI性能的准确比较和监管。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [473] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 本文提出了一种多模态大语言模型MLlm-DR，用于基于面试视频进行抑郁症自动诊断，并提供评分解释。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症自动诊断方法缺乏评分生成的清晰解释，限制了临床应用；当前能处理多模态数据的大语言模型缺乏面试数据训练，诊断性能差。

Method: 设计了集成较小大语言模型和轻量查询模块（LQ-former）的多模态大语言模型，使用构建的训练数据集微调以增强领域逻辑推理，并引入LQ-former提取语音和视觉相关特征。

Result: 在CMDC和E-DAIC-WOZ两个面试数据集上，模型取得了最先进的诊断性能，表现出有效性和优越性。

Conclusion: 所提多模态大语言模型有效提升了抑郁症诊断的准确性和可解释性，具有良好的临床应用潜力。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [474] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLMs）在岩土工程领域中的适应与应用，介绍了领域特定的适配方法及其实际应用，分析了模型的优势与不足，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管通用型大语言模型具备广泛能力，但岩土工程领域的有效应用需要针对该领域进行专门适配，以提升工作流程效率。

Method: 介绍了包括提示工程、检索增强生成、领域自适应预训练及微调在内的多种针对岩土领域的模型适配技术。

Result: 总结了岩土适配模型在地质解释、地下特征描述、场地规划、设计计算、数值建模、安全与风险评估及教育辅导等多方面的最新应用。

Conclusion: 提出该领域大语言模型适配的优势与限制，为实践者提供指导，同时为学术界未来的跨学科研究提供基础和方向。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [475] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于注意力扩散模型的多模态特征缺失补全方法，显著提升了在多模态情感和意图识别中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感与意图识别中由于传感器故障或数据不完整导致的模态缺失问题，避免传统方法中过度耦合和生成精度低的问题。

Method: 设计了ADMC框架，其中为每个模态独立训练特征提取网络，利用基于注意力的扩散网络（ADN）生成缺失模态特征，增强生成特征与真实多模态分布的一致性，并提升全模态识别性能。

Result: 方法在IEMOCAP和MIntRec基准数据集上取得了最先进的结果，证明了其在缺失模态和完整模态场景下的有效性。

Conclusion: 提出的基于注意力扩散模型的缺失模态补全方法有效改善了多模态情感与意图识别的性能，具备广泛应用潜力。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [476] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 通过一项针对大学数据科学课程的实验，发现使用大语言模型生成的检索练习题显著提升了学生的知识保留率。


<details>
  <summary>Details</summary>
Motivation: 教师在快速变化的技术领域中生成高质量的检索练习题费时费力，探索利用大语言模型自动生成题目以提高教学效率。

Method: 在两门大学数据科学课程中，比较学生一周内使用大语言模型生成的选择题检索练习与无检索练习周的学习效果。

Result: 使用大语言模型题目的周，学生平均答题准确率为89%，显著高于无题目周的73%。

Conclusion: 大语言模型生成的检索练习题能有效提升学生学习效果，具有推广价值，但需教师审核题目质量。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [477] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的信息传播模拟环境，结合社会信息处理理论和情绪引导记忆，提升了模型对人类心理和行为的模拟能力。


<details>
  <summary>Details</summary>
Motivation: 传统信息传播模型不足以捕捉线上互动的复杂性，且现有高级方法忽略了用户心理和行为动态。LLM虽具有人类推理能力，但在模拟真实人类行为特别是情绪和立场检测方面存在显著缺陷。

Method: 构建LLM基础的模拟环境，通过引入社交信息处理链式思维（SIP-CoT）机制和情绪引导记忆，增强模型对社交线索的解读、目标个性化设定和反馈评价能力。

Result: 实验表明，采用SIP-CoT机制的LLM代理能更有效地处理社会信息，表现出更接近真实人类的行为、态度和情绪。

Conclusion: 该研究揭示了当前LLM在信息传播模拟中的局限性，并证明了融合SIP-CoT及情绪记忆机制能显著提升LLM代理的社交智能和真实感。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [478] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 本文提出利用司法数据预测城市层级的外国直接投资（FDI），通过构建司法绩效指标体系和新颖的表格学习方法TLJD，实现了高精度的FDI预测。


<details>
  <summary>Details</summary>
Motivation: 经济数据易被操纵，导致城市级FDI预测不可靠，因此尝试利用反映司法绩效的海量司法数据以增强预测的准确性和可靠性。

Method: 基于1200多万份公开判决文书构建司法绩效指标表格数据，设计了一种融合行列数据的表格学习模型TLJD，采用混合专家模型根据区域差异调整指标权重。

Result: 在跨城市和跨时间的预测任务中，TLJD模型在多个评价指标上均优于十个先进基线方法，R2达到至少0.92，表现出较强的预测能力。

Conclusion: 利用司法数据进行城市级FDI预测是一种有效路径，所提TLJD模型为经济增长和司法绩效研究提供了新的方法和视角。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [479] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 该研究比较了人类专家与两种AI模型生成皮肤病治疗方案的质量，发现不同评估者对方案的评价存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI从诊断扩展到治疗方案生成，评估AI生成方案的准确性和可靠性成为关键挑战。

Method: 由十名皮肤科医生、一款通用AI（GPT-4o）及一款推理AI（o3）针对五个复杂案例生成方案，两个阶段分别由人类专家和高级AI评审Gemini 2.5 Pro评分。

Result: 人类专家更倾向于评高人类同行的方案，AI方案评分较低；而高级AI评审则反向评价，认为AI方案优于人类方案，推理模型o3排名第一。

Conclusion: 临床方案质量评价受评估者性质影响显著，存在经验性临床判断与数据驱动算法逻辑之间的显著差距，需发展可解释且协同的人-机系统以提升临床决策质量。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [480] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: 本文提出了ModelAuditor，一种能够自我反思并与用户对话的工具，用于模拟临床环境中任务相关的分布变化，识别并解释模型的潜在失效模式，从而提升AI模型在实际医疗影像中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在临床部署过程中面对实际环境变化会出现性能严重下降，且缺乏便捷、可解释的工具来提前识别和修复这些隐藏失效模式。

Method: 设计了多智能体架构的ModelAuditor，它通过与用户交互选取特定任务指标，模拟临床相关的分布变化，生成解释性报告识别失效模式及根因并提出改进策略。

Result: 在三个真实临床场景中验证了ModelAuditor，其能准确识别最先进模型的失效模式并提出针对性建议，能恢复因分布变化损失的15-25%性能，优于基线和现有增强方法。

Conclusion: ModelAuditor提供了一种高效、低成本的临床AI模型审计方案，显著提升了模型在实际环境中的可靠性和可解释性，具备广泛应用前景。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [481] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 提出一种基于放电脉冲分析的实时电池健康状态监测方法，实验验证预测准确度高。


<details>
  <summary>Details</summary>
Motivation: 微电网中受操作约束限制，传统电池健康状态监测方法难以应用，需创新且实时的SoH监测方法。

Method: 基于充电末端放电脉冲的电压响应，使用电池等效电路模型参数估计电池SoH。

Result: 通过对容量退化约85%的两块电池训练模型，成功预测了两块退化到约90% SoH电池，误差在1%以内，模型解释性评分接近0.9。

Conclusion: 该方法性能优越，可集成进电池管理系统，实现连续运行下的电池优化管理。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [482] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为GTA1的图形用户界面（GUI）自动化代理，通过测试时尺度调整策略和强化学习实现了更准确的任务规划和界面元素交互，显著提升了跨平台GUI自动操作的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在任务规划中存在的歧义性和高分辨率界面中准确定位交互目标的难题，提高代理的决策质量和交互准确率。

Method: 引入测试时尺度调整方法，采样多个候选动作并用评判模型选择最优行动，同时利用强化学习增强选定动作与视觉元素的精准对应。

Result: GTA1在多个基准测试中表现优异，如Screenspot-Pro准确率50.1%，OSWorld任务成功率提升至45.2%，实现了当前最优性能。

Conclusion: 通过测试时尺度调整和强化学习结合的方法，有效提升了GUI自动化代理的任务执行效率与准确性，推动了跨平台自动化任务的研究进展。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [483] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 论文介绍了用于早产儿视网膜病变（ROP）风险预测的中文数据集CROP，并提出了评估大语言模型（LLMs）预测能力和情感偏差的自动化框架Affective-ROPTester。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在ROP风险预测领域的能力尚未得到充分探索，且诊断中情感偏差可能影响结果可靠性，因此需要系统评估及改进方法。

Method: 构建包含993条带风险标签的中文数据集CROP，设计三种提示策略（Instruction、CoT、ICL）并引入情感元素，测试模型内在知识和外部医学知识支持下的预测效果及情感偏差。

Result: 结果表明，LLMs单靠内在知识预测效果有限，结合外部数据显著提升性能；模型存在情感偏差，倾向高估中高风险病例；积极情感提示可减轻预测偏差。

Conclusion: 情感敏感的提示 engineering对提升临床诊断模型的可靠性至关重要，Affective-ROPTester提供了评估和缓解情感偏差的有效工具。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [484] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: 本文综述了认知心理学和类人行为建模的研究，探讨其在通用游戏玩法中的应用，并介绍了基于此构建的CogniPlay模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI虽在围棋、国际象棋等游戏中表现优异，但缺乏与人类类似的直觉和模式化决策过程。

Method: 通过回顾认知心理学发现及先前类人行为建模方法，提出基于这些观察的CogniPlay模型。

Result: 提出了一个工作中的类人决策模型，旨在更好地模仿人类的认知和决策过程。

Conclusion: CogniPlay模型为实现真正类人的游戏玩法AI奠定基础，提升AI系统的人类决策模拟能力。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [485] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 本文提出了一种新的神经符号推理计算模型“神经符号转移系统”，旨在结合传统符号算法与大型语言模型，提升自动化推理工具的能力。


<details>
  <summary>Details</summary>
Motivation: 当前结合传统符号算法与大型语言模型构建神经符号自动推理工具的做法缺乏严密的数学保证，神经网络与符号逻辑未能深入融合，限制了推理能力的提升。

Method: 提出神经符号转移系统模型，将符号状态与“直觉”并行结合，状态转移同时作用于符号与直觉，从而实现神经网络与符号推理的同步操作，增强推理能力。

Result: 该模型能够在保证传统符号算法强保证的同时，扩展逻辑推理能力，提升了基于大型语言模型的自动推理系统的性能。作者还展示了该模型如何用逻辑编程语言具体实现。

Conclusion: 神经符号转移系统为构建高效可靠的神经符号自动推理工具提供了理论基础和实现方案，有望推动自动推理技术的更大突破。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [486] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 本文针对时间序列预测任务，提出分解预测流程为输入序列表示、信息提取与记忆构建、最终目标投影三阶段的方法，评估多种架构配置并在七个基准数据集上实现了最先进的预测精度与计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临序列有效表示、信息提取和目标准确预测的挑战，各任务具有不同特点，需求模型针对性克服困难以提升准确性。

Method: 将时间序列预测流程拆分为三大核心阶段：输入序列表示、信息提取与记忆构建、目标投影，探索卷积层和自注意力等模块在不同任务中的效果。

Result: 在七个基准数据集上，模型不仅达成了最先进的预测准确率，而且显著提升了计算效率，包括缩短训练与推理时间及减少模型参数。

Conclusion: 通过阶段性拆解与模块探索的系统方法，有效提升时间序列预测模型的准确率和效率，具备较好的泛化性和应用潜力。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [487] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 本文提出MusciScene模型，利用跨模态视频和音乐信息，实现音乐场景想象（MSI）任务，从而生成与音乐情境相关的文字描述，并提升视频背景音乐生成效果。


<details>
  <summary>Details</summary>
Motivation: 人类听音乐时能够联想到相应的场景，现有音乐描述模型仅关注音乐元素，缺乏场景联想能力。因此，本文旨在让音乐语言模型实现类似人类的音乐场景想象。

Method: 构建了包含3,371对视频-音频标题的大型数据集，微调音乐理解模型LLaMA用于MSI任务，提出了MusiScene模型，综合评估其在生成语境相关音乐场景描述上的表现。

Result: MusiScene在生成与音乐情境相关的描述方面优于现有的MU-LLaMA模型，生成的音乐场景描述提升了视频背景音乐从文本生成的效果。

Conclusion: 本研究成功实现了音乐场景想象任务，提升了音乐语言模型的跨模态理解能力，为音乐与视觉内容的融合应用提供了新方法。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [488] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: 提出了BlueLM-2.5-3B，一种高效适合边缘设备部署的紧凑多模态大语言模型，具备强大的推理和通用能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种3B规模多模态大语言模型，支持思考和非思考模式，同时实现对思考计算预算的显式控制，满足边缘设备高效推理需求。

Method: 通过多样化数据收集、关键数据重采样、混合异构强化学习和高性能训练基础设施训练模型。

Result: 模型在多模态和纯文本任务上表现优异，思考模式下与更大模型相近，非思考模式优于同规模模型，数据利用效率高。

Conclusion: BlueLM-2.5-3B为边缘设备部署的高性能多模态大语言模型提供了新思路，有助推动相关技术进展。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [489] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 本文提出了一种无线网络多任务预测的统一基础模型，利用单变量分解、因果Transformer和补丁遮蔽策略，提升了模型对不同场景和任务的泛化能力，实现了零样本新任务预测性能超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着移动通信网络的复杂性增加，准确预测关键系统参数对PHY和MAC层任务至关重要，而传统深度学习方法难以跨场景和任务泛化。

Method: 设计了一个支持多任务和多时间间隔预测的统一基础模型，通过单变量分解统一异构任务，编码时间粒度，采用因果Transformer主干结构，训练时应用补丁遮蔽策略以支持任意输入长度。

Result: 在大规模数据集上训练后，该模型展现了对未见场景的强泛化能力，并且在新任务上实现了超越传统全样本基线的零样本性能。

Conclusion: 提出的统一基础模型有效解决了无线网络多任务预测中的泛化和多样性问题，对于未来无线网络预测具有重要的应用价值。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [490] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 本文提出了一种基于归因的解释性人工智能方法，以提高乳腺癌淋巴结放疗后手臂淋巴水肿风险评估的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的人工智能在医疗决策中缺乏透明度，限制了其解释性和接受度。

Method: 利用信息检索中的标准度量，对基于规则的预测模型中的属性进行统计分析，计算每个属性对预测的相关性，提供风险因素影响的可解释信息。

Result: 用户研究表明，所提方法比原始解释性AI模型输出具有更高的可解释性和实用性。

Conclusion: 该归因方法有效提升了手臂淋巴水肿风险预测中解释性的同时，提高了用户对结果的理解和接受度。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [491] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: HopeBot是一款利用大语言模型开发的聊天机器人，能够动态应用和解释PHQ-9抑郁自评量表。研究显示，HopeBot的评分与传统自评量表高度一致，且用户信任度和满意度较高，体现出其作为抑郁筛查辅助工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的PHQ-9抑郁筛查工具虽有效但缺乏互动性和适应性，限制了其使用体验和效果。

Method: 开发基于大语言模型的HopeBot聊天机器人，结合检索增强生成和实时澄清技术。在英中两国132名成人中对比了HopeBot与传统自评版本的评分结果，同时收集用户反馈。

Result: HopeBot与传统PHQ-9评分高度一致（ICC=0.91），45%的评分完全相同。用户对HopeBot信任度高（71%），并对其结构清晰、解释指导和支持性语气给予好评。各项满意度评分均较高，87.1%愿意重复使用或推荐。

Conclusion: 基于语音的大语言模型聊天机器人可作为一种可扩展、低负担的抑郁筛查辅助工具，具备实际应用潜力。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [492] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的轻量级奖励策略框架CogniSQL-R1-Zero，实现了高效且准确的自然语言到SQL的翻译。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生成流畅的SQL语句方面效果有所提升，但对于复杂查询生成正确可执行的SQL仍有较大挑战。

Method: 通过设计基于执行正确性和格式标签合规的轻量奖励信号，结合强化学习，避免中间监督和复杂奖励塑形，促进模型训练稳定和任务目标对齐。

Result: 所提方法在Text2SQL基准BIRD上取得了最先进的执行准确率，超越了多种规模更大且采用监督或指令微调的方法。

Conclusion: 该强化学习方法在资源有限（4块NVIDIA A100 GPU）条件下实现了高效的文本到SQL翻译，为后续高效且可解释的Text-to-SQL研究提供了新思路，且发布了两个高质量数据集以促进研究。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [493] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: 提出了一种新的后置解释方法FGNS，通过结合局部和全局特征重要性选择类别代表性样本，从而提高非专业用户的模型误差识别能力和决策速度。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法难以为无领域专业知识的用户生成清晰且易理解的解释结果。

Method: 引入Feature-Guided Neighbor Selection (FGNS)，利用局部和全局特征重要性选择邻居样本，增强解释的可解释性。

Result: 用户研究表明FGNS显著提升了非专家识别模型错误的能力，决策更快更准确，相较传统k-NN方法邻居样本更贴近类别特征且聚类更紧密。

Conclusion: FGNS迈出了实现更符合人类理解的模型评估的重要一步，但仍需解决解释质量与用户信任感之间的差距。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [494] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 本文在Lockean理论框架内探讨通过概率描述的信念集，解决了这些信念集在经典逻辑推理下不封闭的问题，并提出了一种最小修正的概率更新方法。


<details>
  <summary>Details</summary>
Motivation: Lockean信念集在经典逻辑推理下通常不封闭，增加了信念变化理论的复杂性，需寻找方法保证逻辑封闭性并实现合理的信念更新。

Method: 本文提供了两种对经典逻辑推理封闭信念集的刻画，并提出了一种基于概率的最小修正更新方法，使信念集在接纳新信息时做出最小必要调整并实现逻辑封闭。

Result: 本文证明了可以通过最小修正的方法使信念集在概率描述下实现经典逻辑的逻辑封闭，有效地保持信念一致性和合理更新。

Conclusion: 提出的方法解决了Lockean信念集逻辑封闭性不足的问题，成功实现了概率信念集的最小修正更新，在理论和应用上均具有重要意义。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [495] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: 该论文提出了FEVO框架，通过继续预训练、监督微调和强化学习三阶段提升大语言模型在金融领域的表现，FEVO-R32B模型在多个金融基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融领域因缺乏领域特定知识和推理能力，表现有限，亟需有效提升其金融专业能力。

Method: FEVO采用多阶段策略：继续预训练以扩展金融知识，监督微调以灌输结构化推理模式，强化学习以结合知识与推理；利用高质量数据集训练系列模型。

Result: 训练得到的FEVO-R32B模型在七个基准测试中，特别是五个金融专业测试中实现了最先进表现，优于更大规模模型及专业模型，且优于仅用强化学习训练的版本。

Conclusion: 通过系统的金融知识扩展和结构化逻辑推理训练，FEVO有效提升了大语言模型的金融领域能力，展现了其在专业领域应用的潜力。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [496] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LSTM、遗传算法和SHAP的AI框架，用于提升医疗设施的能源管理效率，显著优于传统预测模型。


<details>
  <summary>Details</summary>
Motivation: 医疗设施能源需求波动大，传统能耗管理方法效率低、成本高，亟需高效智能化解决方案。

Method: 采用LSTM进行能源需求时间序列预测，用遗传算法优化模型参数和负载均衡策略，通过SHAP方法解释模型预测，增强透明性和信任度。

Result: LSTM模型在预测精度上显著优于ARIMA和Prophet，MAE和RMSE均大幅降低，遗传算法提升了适应实时波动的能力，SHAP解释增强了模型透明度。

Conclusion: 该LSTM-GA-SHAP集成方法有效提升了医疗设施能源预测的准确性和效率，为智能能源管理奠定了坚实基础，未来可结合强化学习进一步优化。 

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [497] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: 本文提出了一个名为OpenAgentSafety的框架，用于全面评估AI代理在现实环境下的安全行为。该框架支持真实工具交互及多用户任务，结合规则和LLM评估方法，揭示现有模型存在显著安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽然能够解决复杂任务并已应用于实际，但其潜在不安全行为未被充分评估。现有基准测试多依赖模拟环境和狭窄任务，难以准确反映现实中安全风险。

Method: 引入OpenAgentSafety框架，支持真实工具（如浏览器、文件系统、消息平台等）的多轮多用户任务测试，涵盖八大风险类别。该框架结合规则分析与大型语言模型评价方法检测显性和隐性不安全行为，且设计具扩展性。

Result: 对五款主流大型语言模型进行实证测试，结果显示安全漏洞显著，如Claude-Sonnet-3.7在51.2%的任务中表现出不安全行为，o3-mini更高达72.7%，表明存在严重安全隐患。

Conclusion: OpenAgentSafety提供了一个全面、可拓展的工具，用以严格评估AI代理的安全性，研究结果强调在部署到现实环境前必须加强安全防护措施。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [498] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 通过利用个体较弱但成对的偏好数据，提出了delta学习假设，证明了模型能够超越单个数据点的性能限制，实现更高效的后续训练。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型改进依赖高质量数据，强监督常有限，导致性能提升受限。探索弱监督数据如何通过偏好成对形式驱动模型提升的潜力。

Method: 提出delta学习假设，利用两个较弱模型生成的偏好成对数据，进行后续训练；在8B模型上实证，显示相对质量差异可引导学习，即使单独微调弱数据效果不佳。

Result: 在11项基准测试上，基于delta学习的模型表现匹配依赖强监督Tulu 3模型，证明简单廉价的delta学习后训练可达最先进性能。同时理论证明了弱教师模型的性能差异对改进学生模型有益。

Conclusion: delta学习表明，即便是弱数据成对，也能为模型提供有效信号，促使模型性能显著提升，提供一种简单经济的后训练方法。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [499] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 该论文研究在缺乏完整因果图的情况下，利用因果抽象集合实现因果推断识别，提出并构建了因果识别标准的层次结构。


<details>
  <summary>Details</summary>
Motivation: 传统的因果推断依赖完整的因果图，但实际中经常无法获得完整因果图，尤其是在复杂或高维情景下。为此，利用部分因果信息的因果抽象成为研究热点。

Method: 将因果抽象形式化为一组因果图的集合，定义了多种识别标准，并构建了这些标准的层次结构，以揭示它们之间的关系。通过文献例子说明该框架的应用。

Result: 提出了多种识别标准及其层次关系，明确了在不同因果知识层次下的识别能力，提供了无完整因果知识时因果识别的推理工具。

Conclusion: 该工作系统化了因果抽象背景下的识别问题，帮助理解和应用因果推断方法，特别是在缺乏全因果知识时提高识别能力。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [500] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 本文提出了一种新的文本评分规则ASR，通过最小化与人类评分的均方误差，实现了更好的人类偏好对齐，并保持评分规则的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的概率预测评分规则虽具正确性，但未必与人类对文本的偏好一致，导致文本信息的评分难以准确反映人类评价。

Method: 设计对齐评分规则（ASR），通过最小化已有正确评分规则与人类参考评分之间的均方误差来优化文本评分。

Result: 实验表明，ASR在与人类偏好的对齐上优于之前的方法，同时保持了评分规则的正确性。

Conclusion: ASR有效解决了文本评分正确性与人类偏好对齐之间的矛盾，为文本信息的准确评估提供了新的方法。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [501] [A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation](https://arxiv.org/abs/2507.05331)
*TRI LBM Team,Jose Barreiros,Andrew Beaulieu,Aditya Bhat,Rick Cory,Eric Cousineau,Hongkai Dai,Ching-Hsin Fang,Kunimatsu Hashimoto,Muhammad Zubair Irshad,Masha Itkina,Naveen Kuppuswamy,Kuan-Hui Lee,Katherine Liu,Dale McConachie,Ian McMahon,Haruki Nishimura,Calder Phillips-Grafflin,Charles Richter,Paarth Shah,Krishnan Srinivasan,Blake Wulfe,Chen Xu,Mengchao Zhang,Alex Alspach,Maya Angeles,Kushal Arora,Vitor Campagnolo Guizilini,Alejandro Castro,Dian Chen,Ting-Sheng Chu,Sam Creasey,Sean Curtis,Richard Denitto,Emma Dixon,Eric Dusel,Matthew Ferreira,Aimee Goncalves,Grant Gould,Damrong Guoy,Swati Gupta,Xuchen Han,Kyle Hatch,Brendan Hathaway,Allison Henry,Hillel Hochsztein,Phoebe Horgan,Shun Iwase,Donovon Jackson,Siddharth Karamcheti,Sedrick Keh,Joseph Masterjohn,Jean Mercat,Patrick Miller,Paul Mitiguy,Tony Nguyen,Jeremy Nimmer,Yuki Noguchi,Reko Ong,Aykut Onol,Owen Pfannenstiehl,Richard Poyner,Leticia Priebe Mendes Rocha,Gordon Richardson,Christopher Rodriguez,Derick Seale,Michael Sherman,Mariah Smith-Jones,David Tago,Pavel Tokmakov,Matthew Tran,Basile Van Hoorick,Igor Vasiljevic,Sergey Zakharov,Mark Zolotas,Rares Ambrus,Kerri Fetzer-Borelli,Benjamin Burchfiel,Hadas Kress-Gazit,Siyuan Feng,Stacie Ford,Russ Tedrake*

Main category: cs.RO

TL;DR: 本文评估了多任务机器人操作策略（大型行为模型LBMs），通过扩展扩散策略范式，在仿真和真实机器人数据上进行了严格测试。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人模仿学习和大规模基础模型发展迅速，现实中的性能评估仍然困难，限制了对当前能力的深入理解和进一步发展。

Method: 提出并验证了一个评价流程，采用盲测和随机试验，在仿真及真实环境中对多任务和单任务模型进行对比分析。

Result: 多任务预训练提高了策略的成功率和鲁棒性，能更快速学习复杂新任务，性能随着预训练数据量和多样性增长而提升。

Conclusion: 多任务预训练的机器人操作基础模型在效率和表现上优于单任务模型，拓展了机器人操作的能力和应用潜力。

Abstract: Robot manipulation has seen tremendous progress in recent years, with
imitation learning policies enabling successful performance of dexterous and
hard-to-model tasks. Concurrently, scaling data and model size has led to the
development of capable language and vision foundation models, motivating
large-scale efforts to create general-purpose robot foundation models. While
these models have garnered significant enthusiasm and investment, meaningful
evaluation of real-world performance remains a challenge, limiting both the
pace of development and inhibiting a nuanced understanding of current
capabilities. In this paper, we rigorously evaluate multitask robot
manipulation policies, referred to as Large Behavior Models (LBMs), by
extending the Diffusion Policy paradigm across a corpus of simulated and
real-world robot data. We propose and validate an evaluation pipeline to
rigorously analyze the capabilities of these models with statistical
confidence. We compare against single-task baselines through blind, randomized
trials in a controlled setting, using both simulation and real-world
experiments. We find that multi-task pretraining makes the policies more
successful and robust, and enables teaching complex new tasks more quickly,
using a fraction of the data when compared to single-task baselines. Moreover,
performance predictably increases as pretraining scale and diversity grows.
Project page: https://toyotaresearchinstitute.github.io/lbm1/

</details>


### [502] [Feature Geometry for Stereo Sidescan and Forward-looking Sonar](https://arxiv.org/abs/2507.05410)
*Kalin Norman,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 本文提出了一种基于几何的方法，实现了用于海洋机器人中前视声纳和侧扫声纳的立体声声纳数据融合。


<details>
  <summary>Details</summary>
Motivation: 为了实现海洋机器人中不同类型声纳数据的融合，提高特征匹配和三维信息恢复能力。

Method: 借鉴立体相机的极几何，利用两声纳之间的相对姿态，将一个声纳观测到的特征投影到另一个声纳图像中，同时分析特征相对位置和声纳相对姿态对投影的影响。

Result: 通过仿真结果识别出了适合于现场机器人应用的立体声纳配置，有助于特征对应和三维信息恢复。

Conclusion: 该方法为海洋机器人中双模态声纳的特征对应和三维信息提取提供了一种有效的几何投影方案。

Abstract: In this paper, we address stereo acoustic data fusion for marine robotics and
propose a geometry-based method for projecting observed features from one sonar
to another for a cross-modal stereo sonar setup that consists of both a
forward-looking and a sidescan sonar. Our acoustic geometry for sidescan and
forward-looking sonar is inspired by the epipolar geometry for stereo cameras,
and we leverage relative pose information to project where an observed feature
in one sonar image will be found in the image of another sonar. Additionally,
we analyze how both the feature location relative to the sonar and the relative
pose between the two sonars impact the projection. From simulated results, we
identify desirable stereo configurations for applications in field robotics
like feature correspondence and recovery of the 3D information of the feature.

</details>


### [503] [CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning](https://arxiv.org/abs/2507.05458)
*Yi-Shiuan Tung,Bradley Hayes,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文提出了一种名为CRED的轨迹生成方法，通过联合优化环境设计和轨迹选择，提高了主动偏好学习中人类奖励函数的估计效果，尤其适用于长时间任务。


<details>
  <summary>Details</summary>
Motivation: 机器人在实际应用中需根据人类偏好调整行为，而现有主动偏好学习方法难以全面探索轨迹空间，且难以获取有效的查询，影响奖励函数的准确学习。

Method: CRED通过环境设计想象新场景，并利用反事实推理，从当前奖励信念中采样，生成多样且信息丰富的轨迹供人类评选，从而改善奖励估计。

Result: 在GridWorld和基于OpenStreetMap的数据的真实导航实验中，CRED表现出更好的奖励学习效果和更强的跨环境泛化能力。

Conclusion: CRED有效提升了主动偏好学习的奖励函数学习与泛化能力，为机器人在多样环境中适应人类偏好提供了有力方法支持。

Abstract: For effective real-world deployment, robots should adapt to human
preferences, such as balancing distance, time, and safety in delivery routing.
Active preference learning (APL) learns human reward functions by presenting
trajectories for ranking. However, existing methods often struggle to explore
the full trajectory space and fail to identify informative queries,
particularly in long-horizon tasks. We propose CRED, a trajectory generation
method for APL that improves reward estimation by jointly optimizing
environment design and trajectory selection. CRED "imagines" new scenarios
through environment design and uses counterfactual reasoning -- by sampling
rewards from its current belief and asking "What if this reward were the true
preference?" -- to generate a diverse and informative set of trajectories for
ranking. Experiments in GridWorld and real-world navigation using OpenStreetMap
data show that CRED improves reward learning and generalizes effectively across
different environments.

</details>


### [504] [Gaussian Process-Based Active Exploration Strategies in Vision and Touch](https://arxiv.org/abs/2507.05522)
*Ho Jin Choi,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种融合视觉与触觉数据的统一高斯过程距离场模型，以实现机器人对物体形状和表面属性的主动感知，提升复杂环境中的物体操作能力。


<details>
  <summary>Details</summary>
Motivation: 机器人在非结构化环境中由于先验知识有限，难以理解物体的形状、材料及语义，限制了其操作能力。人类通过多传感器交互探索学习这些属性，启发了该研究融合视觉与触觉的思路。

Method: 采用高斯过程距离场（GPDF）编码点云的有符号距离、解析梯度和海森矩阵及表面不确定性，结合差分渲染迭代整合视觉与触觉数据，量化多传感器不确定性并规划探索动作，提升物体几何估计精度。

Result: 通过在真实机器人（Franka Research 3与定制DIGIT触觉传感器、Intel Realsense D435）上实验，展示了该方法在主动探索和精确恢复物体三维结构及表面属性上的有效性，同时提出了高斯过程诱导点方法以提升可扩展性。

Conclusion: 该方法实现了多模态融合的概率建模，支持机器人主动感知复杂对象几何及表面特性，具备扩展到更多属性的潜力，推动了机器人在复杂非结构化环境中的自主操作能力。

Abstract: Robots struggle to understand object properties like shape, material, and
semantics due to limited prior knowledge, hindering manipulation in
unstructured environments. In contrast, humans learn these properties through
interactive multi-sensor exploration. This work proposes fusing visual and
tactile observations into a unified Gaussian Process Distance Field (GPDF)
representation for active perception of object properties. While primarily
focusing on geometry, this approach also demonstrates potential for modeling
surface properties beyond geometry. The GPDF encodes signed distance using
point cloud, analytic gradient and Hessian, and surface uncertainty estimates,
which are attributes that common neural network shape representation lack. By
utilizing a point cloud to construct a distance function, GPDF does not need
extensive pretraining on large datasets and can incorporate observations by
aggregation. Starting with an initial visual shape estimate, the framework
iteratively refines the geometry by integrating dense vision measurements using
differentiable rendering and tactile measurements at uncertain surface regions.
By quantifying multi-sensor uncertainties, it plans exploratory motions to
maximize information gain for recovering precise 3D structures. For the
real-world robot experiment, we utilize the Franka Research 3 robot
manipulator, which is fixed on a table and has a customized DIGIT tactile
sensor and an Intel Realsense D435 RGBD camera mounted on the end-effector. In
these experiments, the robot explores the shape and properties of objects
assumed to be static and placed on the table. To improve scalability, we
investigate approximation methods like inducing point method for Gaussian
Processes. This probabilistic multi-modal fusion enables active exploration and
mapping of complex object geometries, extending potentially beyond geometry.

</details>


### [505] [PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs](https://arxiv.org/abs/2507.05555)
*Obin Kwon,Sankalp Yamsani,Noboru Myers,Sean Taylor,Jooyoung Hong,Kyungseo Park,Alex Alspach,Joohyung Kim*

Main category: cs.RO

TL;DR: PAPRLE是一个模块化机器人肢体环境，支持灵活配置和多种输入设备控制，实现双向力反馈，促进遥控操作和AI研究。


<details>
  <summary>Details</summary>
Motivation: 为应对不同任务需求，提升遥控机器人肢体的灵活性和适应性，开发一个可插拔、模块化的机器人肢体控制生态系统。

Method: 设计并实现PAPRLE系统，支持多种控制输入（傀儡装置、游戏控制器、VR接口），引入可插拔傀儡装置，实现关节空间和任务空间控制及实时力反馈，支持多种空间肢体排列。

Result: PAPRLE在多种实际场景中验证了其灵活性和多设备、多机器人组合的适配性，实现了高效双向遥控，促进了数据采集和学习控制研究。

Conclusion: PAPRLE作为开源硬件和软件平台，将推动机器人肢体遥控系统的广泛应用和社区扩展，促进体现式AI及机器人控制领域进展。

Abstract: We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular
ecosystem that enables flexible placement and control of robotic limbs. With
PAPRLE, a user can change the arrangement of the robotic limbs, and control
them using a variety of input devices, including puppeteers, gaming
controllers, and VR-based interfaces. This versatility supports a wide range of
teleoperation scenarios and promotes adaptability to different task
requirements. To further enhance configurability, we introduce a pluggable
puppeteer device that can be easily mounted and adapted to match the target
robot configurations. PAPRLE supports bilateral teleoperation through these
puppeteer devices, agnostic to the type or configuration of the follower robot.
By supporting both joint-space and task-space control, the system provides
real-time force feedback, improving user fidelity and physical interaction
awareness. The modular design of PAPRLE facilitates novel spatial arrangements
of the limbs and enables scalable data collection, thereby advancing research
in embodied AI and learning-based control. We validate PAPRLE in various
real-world settings, demonstrating its versatility across diverse combinations
of leader devices and follower robots. The system will be released as open
source, including both hardware and software components, to support broader
adoption and community-driven extension. Additional resources and
demonstrations are available at the project website:
https://uiuckimlab.github.io/paprle-pages

</details>


### [506] [Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube](https://arxiv.org/abs/2507.05607)
*Chongshan Fan,Shenghai Yuan*

Main category: cs.RO

TL;DR: Auto-RubikAI是一种结合知识库、视觉语言模型和大语言模型的机器人自主规划系统，可以在无需大量演示数据的情况下，解算魔方复原任务，实现多步解释性控制。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统依赖预定义脚本，或现代方法依赖大规模训练和演示，难以实现少数据、高解释性的结构化操作任务。

Method: 系统结合symbolic知识库解决符号推理，视觉语言模型解析三维场景，大语言模型生成结构化指令，形成三模块架构，支持空间不确定性环境下的高效作业。

Result: 在仿真及实物7自由度机械臂上测试，任务成功率达79%，相较其他方法减少解算步骤且保持可解释性和安全性。

Conclusion: Auto-RubikAI为智能制造及机器人自主执行任务提供一种数据低需求、高效且模块化的规划框架，具备良好实用前景。

Abstract: This paper presents Auto-RubikAI, a modular autonomous planning framework
that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM),
and a large language model (LLM) to solve structured manipulation tasks
exemplified by Rubik's Cube restoration. Unlike traditional robot systems based
on predefined scripts, or modern approaches relying on pretrained networks and
large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step
task execution with minimal data requirements and no prior demonstrations. The
proposed system employs a KB module to solve group-theoretic restoration steps,
overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to
construct a semantic 3D scene representation, while the LLM generates
structured robotic control code via prompt chaining. This tri-module
architecture enables robust performance under spatial uncertainty. We deploy
Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic
arm, demonstrating effective Sim-to-Real adaptation without retraining.
Experiments show a 79% end-to-end task success rate across randomized
configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our
KB-enhanced method reduces average solution steps while maintaining
interpretability and safety. Auto-RubikAI provides a cost-efficient, modular
foundation for embodied task planning in smart manufacturing, robotics
education, and autonomous execution scenarios. Code, prompts, and hardware
modules will be released upon publication.

</details>


### [507] [DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation](https://arxiv.org/abs/2507.05627)
*Young Hun Kim,Seungyeon Kim,Yonghyeon Lee,Frank Chongwoo Park*

Main category: cs.RO

TL;DR: DreamGrasp利用大规模预训练图像生成模型的想象能力，实现部分视角下复杂多物体场景的鲁棒3D重建。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，由于遮挡和稀疏视角，传统基于对称性或监督学习的3D识别方法难以普适，且缺乏可靠深度信息，亟需新的解决方案。

Method: DreamGrasp结合粗糙3D重建、对比学习实现实例分割，并利用文本引导对单个实例进行细化，从而推断场景未观察部分。

Result: 实验表明，DreamGrasp能够准确恢复物体几何形状，在复杂多物体环境中实现高成功率的连续整理和目标检索。

Conclusion: DreamGrasp克服了现有方法的局限性，实现了对部分视角场景中多物体的高效3D重建和应用，具有广泛的实际应用价值。

Abstract: Partial-view 3D recognition -- reconstructing 3D geometry and identifying
object instances from a few sparse RGB images -- is an exceptionally
challenging yet practically essential task, particularly in cluttered, occluded
real-world settings where full-view or reliable depth data are often
unavailable. Existing methods, whether based on strong symmetry priors or
supervised learning on curated datasets, fail to generalize to such scenarios.
In this work, we introduce DreamGrasp, a framework that leverages the
imagination capability of large-scale pre-trained image generative models to
infer the unobserved parts of a scene. By combining coarse 3D reconstruction,
instance segmentation via contrastive learning, and text-guided instance-wise
refinement, DreamGrasp circumvents limitations of prior methods and enables
robust 3D reconstruction in complex, multi-object environments. Our experiments
show that DreamGrasp not only recovers accurate object geometry but also
supports downstream tasks like sequential decluttering and target retrieval
with high success rates.

</details>


### [508] [A Physics-Based Continuum Model for Versatile, Scalable, and Fast Terramechanics Simulation](https://arxiv.org/abs/2507.05643)
*Huzaifa Unjhawala,Luning Bakke,Harry Zhang,Michael Taylor,Ganesh Arivoli,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: 本文提出了Chrono的连续表示模型（Chrono::CRM），一种基于物理的高效通用地形力学仿真方法，能够模拟复杂任务如挖掘和车辆轮胎与地形的相互作用，具有可扩展性和GPU加速优势，实现大规模高保真仿真。


<details>
  <summary>Details</summary>
Motivation: 传统地形力学方法如半经验模型在应对复杂地形交互和任务时存在局限，需要一个物理基础更强、适用范围更广泛且高效的仿真模型。

Method: 基于Chrono的平滑粒子流体动力学（SPH）框架，结合动力学引擎实现可模拟刚性及柔性部件交互，采用GPU加速和“活跃域”技术提升计算效率，实现大规模地形仿真。

Result: 该模型通过三项物理实验（包括NASA MGRU3漫游车测试）验证，并与高精度离散元方法（DEM）仿真对比，展示出高准确性和高效性，能够处理长达10公里、含1亿粒子的地形。

Conclusion: Chrono::CRM是一个开源的、可扩展且高效的地形力学仿真方案，支持复杂机械与地形交互，适用于大规模离线或近实时仿真，推动地形力学研究与应用的发展。

Abstract: This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.

</details>


### [509] [3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting](https://arxiv.org/abs/2507.05661)
*Haitao Lu,Haijier Chen,Haoze Liu,Shoujian Zhang,Bo Xu,Ziao Liu*

Main category: cs.RO

TL;DR: 提出了一种基于3D高斯点云渲染的单目RGB图像定位框架3DGS-LSR，实现了厘米级精度的城市环境自主机器人定位。


<details>
  <summary>Details</summary>
Motivation: 传统GNSS定位在复杂城市环境中易受信号遮挡和多路径效应影响，导致定位不可靠；同时传统地图构建方法存储和计算开销大，不适合资源受限的机器人平台。

Method: 结合多传感器数据构建高精度3DGS地图，机器人端只需单目RGB图像。利用SuperPoint和SuperGlue进行特征提取和匹配，采用迭代优化策略通过逐步渲染细化定位结果，支持实时导航。

Result: 在KITTI数据集的多种城市道路环境中，3DGS-LSR实现了0.026m、0.029m和0.081m的平均定位精度，显著优于其他代表性方法，且只需单目RGB输入。

Conclusion: 3DGS-LSR提供了一种高效、精确且适用于实际复杂城市环境的自主机器人定位方案，能够在GNSS失效情况下保持可靠定位能力。

Abstract: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

</details>


### [510] [Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains](https://arxiv.org/abs/2507.05663)
*Neelay Joglekar,Fei Liu,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 本文提出了一种稳定的远端运动中心（RCM）操纵器控制方法，解决了传统电缆驱动手术机器人中不可视关节误差的问题，提升了机器人自主手术的可行性。


<details>
  <summary>Details</summary>
Motivation: RCM手术机器人在微创手术中精确操作至关重要，但电缆驱动导致的关节测量误差影响控制准确性，尤其是插入点之前不可见的误差无法通过视觉追踪纠正。

Method: 设计了一种可证明稳定的闭环跟踪控制器，专门针对不可视的臂链部分；并将其集成到双层控制方案中，以控制整个运动链。

Result: 在模拟和真实环境中严格验证了该方法的理论稳定性和有效性。

Conclusion: 该研究为实现从遥操作到自主手术迈出了关键一步，提供了针对RCM手术机器人误差控制的新思路。

Abstract: Remote Center of Motion (RCM) robotic manipulators have revolutionized
Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation
within the patient's body cavity without disturbing the insertion point on the
patient. Accurate RCM tool control is vital for incorporating autonomous
subtasks like suturing, blood suction, and tumor resection into robotic
surgical procedures, reducing surgeon fatigue and improving patient outcomes.
However, these cable-driven systems are subject to significant joint reading
errors, corrupting the kinematics computation necessary to perform control.
Although visual tracking with endoscopic cameras can correct errors on in-view
joints, errors in the kinematic chain prior to the insertion point are
irreparable because they remain out of view. No prior work has characterized
the stability of control under these conditions. We fill this gap by designing
a provably stable tracking-in-the-loop controller for the out-of-view portion
of the RCM manipulator kinematic chain. We additionally incorporate this
controller into a bilevel control scheme for the full kinematic chain. We
rigorously benchmark our method in simulated and real world settings to verify
our theoretical findings. Our work provides key insights into the next steps
required for the transition from teleoperated to autonomous surgery.

</details>


### [511] [Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control](https://arxiv.org/abs/2507.05674)
*Xinyao Qin,Xiaoteng Ma,Yang Qi,Qihan Liu,Chuanyi Xue,Ning Gui,Qinyu Dong,Jun Yang,Bin Liang*

Main category: cs.RO

TL;DR: 提出了基于扩散模型的四足机器人多任务预训练与在线强化学习微调框架DMLoco，实现语言条件控制和稳定任务切换。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型的多任务泛化和语言条件优势，解决四足机器人运动中稳定性差和任务切换困难的问题。

Method: 基于Denoising Diffusion Implicit Models先进行多任务数据预训练，再通过在线PPO算法在仿真中微调，实现稳定性和任务转换。使用TensorRT优化部署，保证50Hz实时运行。

Result: DMLoco支持语言引导的多技能执行，适应性强，任务切换稳定，适用于资源受限的机器人平台，实现高效快速的在线控制。

Conclusion: 结合扩散模型与强化学习的多任务预训练与微调策略，有效提升四足机器人语言条件控制和任务切换能力，具备良好实用价值。

Abstract: Recent research has highlighted the powerful capabilities of imitation
learning in robotics. Leveraging generative models, particularly diffusion
models, these approaches offer notable advantages such as strong multi-task
generalization, effective language conditioning, and high sample efficiency.
While their application has been successful in manipulation tasks, their use in
legged locomotion remains relatively underexplored, mainly due to compounding
errors that affect stability and difficulties in task transition under limited
data. Online reinforcement learning (RL) has demonstrated promising results in
legged robot control in the past years, providing valuable insights to address
these challenges. In this work, we propose DMLoco, a diffusion-based framework
for quadruped robots that integrates multi-task pretraining with online PPO
finetuning to enable language-conditioned control and robust task transitions.
Our approach first pretrains the policy on a diverse multi-task dataset using
diffusion models, enabling language-guided execution of various skills. Then,
it finetunes the policy in simulation to ensure robustness and stable task
transition during real-world deployment. By utilizing Denoising Diffusion
Implicit Models (DDIM) for efficient sampling and TensorRT for optimized
deployment, our policy runs onboard at 50Hz, offering a scalable and efficient
solution for adaptive, language-guided locomotion on resource-constrained
robotic platforms.

</details>


### [512] [Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning](https://arxiv.org/abs/2507.05695)
*Xiatao Sun,Yuxuan Wang,Shuo Yang,Yinxing Chen,Daniel Rakita*

Main category: cs.RO

TL;DR: 本文提出了hPGA-DP，一种结合射影几何代数（PGA）和扩散策略的机器人运动生成方法，通过融合几何归纳偏置提高训练效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习中的扩散策略每次训练需重新学习空间变换（如平移、旋转），效率较低。引入几何归纳偏置能减少冗余，提高训练效率。

Method: 将射影几何代数（PGA）框架引入扩散策略，设计P-GATr架构利用其E(3)-等变性，采用混合架构以P-GATr作为编码解码器，结合U-Net或Transformer模块完成去噪。

Result: 实验证明hPGA-DP在模拟与真实环境中较传统方法展现出更优的任务表现和训练效率，并通过混合架构实现更快收敛。

Conclusion: 通过引入PGA几何偏置和混合架构，hPGA-DP有效提升了机器人运动生成任务的性能与训练速度，具备较强实用价值。

Abstract: Diffusion policies have become increasingly popular in robot learning due to
their reliable convergence in motion generation tasks. At a high level, these
policies learn to transform noisy action trajectories into effective ones,
conditioned on observations. However, each time such a model is trained in a
robotics context, the network must relearn fundamental spatial representations
and operations, such as translations and rotations, from scratch in order to
ground itself and operate effectively in a 3D environment. Incorporating
geometric inductive biases directly into the network can alleviate this
redundancy and substantially improve training efficiency. In this paper, we
introduce hPGA-DP, a diffusion policy approach that integrates a mathematical
framework called Projective Geometric Algebra (PGA) to embed strong geometric
inductive biases. PGA is particularly well-suited for this purpose as it
provides a unified algebraic framework that naturally encodes geometric
primitives, such as points, directions, and rotations, enabling neural networks
to reason about spatial structure through interpretable and composable
operations. Specifically, we propose a novel diffusion policy architecture that
incorporates the Projective Geometric Algebra Transformer (P-GATr), leveraging
its E(3)-equivariant properties established in prior work. Our approach adopts
a hybrid architecture strategy, using P-GATr as both a state encoder and action
decoder, while employing U-Net or Transformer-based modules for the denoising
process. Several experiments and ablation studies in both simulated and
real-world environments demonstrate that hPGA-DP not only improves task
performance and training efficiency through the geometric bias of P-GATr, but
also achieves substantially faster convergence through its hybrid model
compared to architectures that rely solely on P-GATr.

</details>


### [513] [DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving](https://arxiv.org/abs/2507.05710)
*Hyeongchan Ham,Heejin Ahn*

Main category: cs.RO

TL;DR: 本文提出了一种基于分布鲁棒优化(DRO)和证据深度学习(EDL)的自动驾驶运动规划安全框架，通过动态调整不确定性约束，在感知置信度不同的情况下实现安全与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖神经网络感知，但基于不确定感知结果的控制决策存在显著安全风险，因此需要一种方法有效处理感知中的不确定性。

Method: 提出结合EDL的DRO框架，引入基于证据分布的新型模糊集，动态调整保守性，整合进模型预测控制(MPC)，形成计算可行的DRO-EDL-MPC算法。

Result: 在CARLA仿真环境中验证，方法在高感知置信度下保持效率，在低置信度下保证约束保守，提升了安全性。

Conclusion: 该方法有效兼顾感知不确定性与控制安全性，实现了自动驾驶运动规划的鲁棒性和效率的动态平衡。

Abstract: Safety is a critical concern in motion planning for autonomous vehicles.
Modern autonomous vehicles rely on neural network-based perception, but making
control decisions based on these inference results poses significant safety
risks due to inherent uncertainties. To address this challenge, we present a
distributionally robust optimization (DRO) framework that accounts for both
aleatoric and epistemic perception uncertainties using evidential deep learning
(EDL). Our approach introduces a novel ambiguity set formulation based on
evidential distributions that dynamically adjusts the conservativeness
according to perception confidence levels. We integrate this uncertainty-aware
constraint into model predictive control (MPC), proposing the DRO-EDL-MPC
algorithm with computational tractability for autonomous driving applications.
Validation in the CARLA simulator demonstrates that our approach maintains
efficiency under high perception confidence while enforcing conservative
constraints under low confidence.

</details>


### [514] [Simultaneous Triggering and Synchronization of Sensors and Onboard Computers](https://arxiv.org/abs/2507.05717)
*Morten Nissov,Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文介绍了一种基于常见组件和已确立同步方法的实时低成本同步系统，实现高低速传感器触发及同步。


<details>
  <summary>Details</summary>
Motivation: 机器人高精度估计算法需要准确的数据时间戳，但时间戳的不准确性影响线上估计性能，目前缺乏经济高效的实时时间同步方案。

Method: 提出一种利用常见组件和同步方法的多功能系统，实现多种传感器的同步和触发。

Result: 系统成功展示了对高低速传感器的同步和触发能力。

Conclusion: 该系统为实时且低成本的时间戳同步提供了有效解决方案，适用于机器人等需要高精度时间同步的场景。

Abstract: High fidelity estimation algorithms for robotics require accurate data.
However, timestamping of sensor data is a key issue that rarely receives the
attention it deserves. Inaccurate timestamping can be compensated for in
post-processing but is imperative for online estimation. Simultaneously, even
online mitigation of timing issues can be achieved through a relaxation of the
tuning parameters from their otherwise more performative optimal values, but at
a detriment to performance. To address the need for real-time, low-cost
timestamping, a versatile system which utilizes readily-available components
and established methods for synchronization is introduced. The synchronization
and triggering (of both high- and low-rate sensors) capabilities of the system
are demonstrated.

</details>


### [515] [A Learning-based Planning and Control Framework for Inertia Drift Vehicles](https://arxiv.org/abs/2507.05748)
*Bei Zhou,Zhouheng Li,Lei Xie,Hongye Su,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化的学习规划与控制框架，实现了惯性漂移的平滑过渡和精准路径跟踪。


<details>
  <summary>Details</summary>
Motivation: 惯性漂移作为连续漂移阶段之间的过渡动作，能帮助自动驾驶赛车应对连续急弯，但因车辆动力学复杂和环境变化使得控制困难。

Method: 利用贝叶斯优化进行规划逻辑设计，确保惯性漂移与持续漂移阶段的平滑过渡和最小速度损失；并通过性能驱动的控制策略学习缓解模型误差。

Result: 仿真结果表明，提出的方法能在8字形轨迹上实现平稳、稳定的惯性漂移。

Conclusion: 所提框架有效解决了惯性漂移快速切换过程中的控制难题，提升了自动赛车的漂移能力和路径跟踪精度。

Abstract: Inertia drift is a transitional maneuver between two sustained drift stages
in opposite directions, which provides valuable insights for navigating
consecutive sharp corners for autonomous racing.However, this can be a
challenging scenario for the drift controller to handle rapid transitions
between opposing sideslip angles while maintaining accurate path tracking.
Moreover, accurate drift control depends on a high-fidelity vehicle model to
derive drift equilibrium points and predict vehicle states, but this is often
compromised by the strongly coupled longitudinal-lateral drift dynamics and
unpredictable environmental variations. To address these challenges, this paper
proposes a learning-based planning and control framework utilizing Bayesian
optimization (BO), which develops a planning logic to ensure a smooth
transition and minimal velocity loss between inertia and sustained drift
phases. BO is further employed to learn a performance-driven control policy
that mitigates modeling errors for enhanced system performance. Simulation
results on an 8-shape reference path demonstrate that the proposed framework
can achieve smooth and stable inertia drift through sharp corners.

</details>


### [516] [LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving](https://arxiv.org/abs/2507.05754)
*Yuhang Zhang,Jiaqi Liu,Chengkai Xu,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出了一种结合模仿学习端到端框架与大语言模型增强的城市自动驾驶架构LeAD，提升复杂场景的理解与决策能力。


<details>
  <summary>Details</summary>
Motivation: 当前城市自动驾驶系统难以有效理解复杂交通语义信息和判断其他参与者意图，导致决策与人类驾驶员推理不一致。

Method: 提出双速率架构LeAD，高频端到端系统保证实时感知规划控制，低频大语言模型通过多模态感知融合和链式推理提升场景理解与决策优化。

Result: 在CARLA模拟器上，LeAD优于现有方法，异常场景表现优异，Leaderboard V1 得分71，路线完成率达93%。

Conclusion: LeAD通过结合端到端学习与大语言模型增强，有效提升城市自动驾驶在复杂和边缘场景下的性能，实现更接近人类驾驶的决策能力。

Abstract: A principal barrier to large-scale deployment of urban autonomous driving
systems lies in the prevalence of complex scenarios and edge cases. Existing
systems fail to effectively interpret semantic information within traffic
contexts and discern intentions of other participants, consequently generating
decisions misaligned with skilled drivers' reasoning patterns. We present LeAD,
a dual-rate autonomous driving architecture integrating imitation
learning-based end-to-end (E2E) frameworks with large language model (LLM)
augmentation. The high-frequency E2E subsystem maintains real-time
perception-planning-control cycles, while the low-frequency LLM module enhances
scenario comprehension through multi-modal perception fusion with HD maps and
derives optimal decisions via chain-of-thought (CoT) reasoning when baseline
planners encounter capability limitations. Our experimental evaluation in the
CARLA Simulator demonstrates LeAD's superior handling of unconventional
scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route
completion of 93%.

</details>


### [517] [Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments](https://arxiv.org/abs/2507.05861)
*Woonsang Kang,Joohyung Lee,Seungjun Kim,Jungchan Cho,Yoonseon Oh*

Main category: cs.RO

TL;DR: 本文提出了一种模块化的联邦学习框架，针对抓取姿态检测中的通信开销问题，提高了通信效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 抓取姿态检测虽然是机器人自主能力的基础，但依赖大规模数据集带来了数据隐私和中心化难题。传统联邦学习通信开销大，限制了资源受限机器人的应用。

Method: 通过分析模型各功能模块的学习动态，识别收敛较慢的模块，采用两阶段训练：先全模型训练，再仅针对收敛慢模块进行高效通信和聚合，减少通信负担。

Result: 在GraspNet-1B数据集和真实机器人实验中，该方法在通信预算相同时，准确率高于FedAvg及其他基线，且在复杂环境下抓取成功率更高。

Conclusion: 该模块化联邦学习框架有效提升了抓取姿态检测的通信效率和模型表现，实现了分散训练中通信代价与性能的良好平衡。

Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy,
but its reliance on large, diverse datasets creates significant data privacy
and centralization challenges. Federated Learning (FL) offers a
privacy-preserving solution, but its application to GPD is hindered by the
substantial communication overhead of large models, a key issue for
resource-constrained robots. To address this, we propose a novel module-wise FL
framework that begins by analyzing the learning dynamics of the GPD model's
functional components. This analysis identifies slower-converging modules, to
which our framework then allocates additional communication effort. This is
realized through a two-phase process: a standard full-model training phase is
followed by a communication-efficient phase where only the identified subset of
slower-converging modules is trained and their partial updates are aggregated.
Extensive experiments on the GraspNet-1B dataset demonstrate that our method
outperforms standard FedAvg and other baselines, achieving higher accuracy for
a given communication budget. Furthermore, real-world experiments on a physical
robot validate our approach, showing a superior grasp success rate compared to
baseline methods in cluttered scenes. Our work presents a
communication-efficient framework for training robust, generalized GPD models
in a decentralized manner, effectively improving the trade-off between
communication cost and model performance.

</details>


### [518] [Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data](https://arxiv.org/abs/2507.05884)
*Chang Liu,Zhexiong Xue,Tamas Sziranyi*

Main category: cs.RO

TL;DR: 本文比较评估了多种路径规划算法在基于高分辨率卫星影像和机载LiDAR数据生成的加权像素级路网上的表现，结果显示Dijkstra算法在2D和3D路径规划中表现最稳定高效。


<details>
  <summary>Details</summary>
Motivation: 无人驾驶车辆在非结构化环境（如森林和山区）导航面临地形复杂和道路状况多变的挑战，需评估不同路径规划算法在高精度地理信息数据上的性能。

Method: 采用A*、Dijkstra、RRT*及新改进蚁群算法（NIACO）在2D卫星影像数据上测试路径规划性能；在3D机载LiDAR数据上测试3D A*、3D Dijkstra、RRT-Connect和NIACO的表现，比较路径代价、计算时间和内存消耗。

Result: Dijkstra算法在2D和3D两种情况下均表现出最高的稳定性和效率，特别是在密集的像素级地理路网上表现优越。

Conclusion: 基于Dijkstra的路径规划算法在静态地形导航中可靠性高，为未来复杂环境动态路径规划的研究奠定基础。

Abstract: Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.

</details>


### [519] [FineGrasp: Towards Robust Grasping for Delicate Objects](https://arxiv.org/abs/2507.05978)
*Yun Du,Mengao Zhao,Tianwei Lin,Yiwei Jin,Chaodong Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: 提出了一种针对小物体和精细部件抓取的新方法FineGrasp，通过网络改进、标签归一化和混合仿真训练有效提升抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有抓取方法难以对小物体或精细部件生成可行的抓取姿态，导致抓取失败。

Method: 提出FineGrasp方法，包括网络结构改进以处理精细区域，标签归一化策略解决标签不平衡问题，并引入新的仿真抓取数据集，采用混合仿真到真实训练。

Result: 实验结果显示FineGrasp在抓取小型物体方面有显著提升，验证了在语义抓取中的有效性。

Conclusion: FineGrasp有效提升了机器人对小物体和精细部件的抓取能力，推动了语义驱动机器人抓取的发展。

Abstract: Recent advancements in robotic grasping have led to its integration as a core
module in many manipulation systems. For instance, language-driven semantic
segmentation enables the grasping of any designated object or object part.
However, existing methods often struggle to generate feasible grasp poses for
small objects or delicate components, potentially causing the entire pipeline
to fail. To address this issue, we propose a novel grasping method, FineGrasp,
which introduces improvements in three key aspects. First, we introduce
multiple network modifications to enhance the ability of to handle delicate
regions. Second, we address the issue of label imbalance and propose a refined
graspness label normalization strategy. Third, we introduce a new simulated
grasp dataset and show that mixed sim-to-real training further improves grasp
performance. Experimental results show significant improvements, especially in
grasping small objects, and confirm the effectiveness of our system in semantic
grasping.

</details>


### [520] [AURA-CVC: Autonomous Ultrasound-guided Robotic Assistance for Central Venous Catheterization](https://arxiv.org/abs/2507.05979)
*Deepak Raina,Lidia Al-Zogbi,Brian Teixeira,Vivek Singh,Ankur Kapoor,Thorsten Fleiter,Muyinatu A. Lediju Bell,Vinciya Pandian,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一套基于机器人和超声引导的中心静脉穿刺(CVC)全流程系统，实现从扫描初始化到针头插入的自主操作，并在高精度仿真模型上验证了其高成功率和精准度。


<details>
  <summary>Details</summary>
Motivation: 中心静脉穿刺操作复杂，需持续的超声引导，受解剖变异和操作者技术影响大，且针头误插可能导致严重并发症。现有机器人系统尚未实现完全自主操作。

Method: 使用深度学习模型从患者颈部深度图像中定位解剖标志，自动确定扫描区域及路径，结合机器人运动规划进行血管扫描、分割和重建，定位最佳穿刺点，最终结合超声引导和操作者反馈完成针头插入。

Result: 系统在10个模拟临床场景中首次尝试均成功穿刺，血管重建误差平均2.15毫米，自主针头插入误差接近1毫米。

Conclusion: 这是首个在高保真仿真模型上展示的集规划、扫描和插入于一体的机器人中心静脉穿刺系统，实验结果表明其具备临床应用潜力。

Abstract: Purpose: Central venous catheterization (CVC) is a critical medical procedure
for vascular access, hemodynamic monitoring, and life-saving interventions. Its
success remains challenging due to the need for continuous ultrasound-guided
visualization of a target vessel and approaching needle, which is further
complicated by anatomical variability and operator dependency. Errors in needle
placement can lead to life-threatening complications. While robotic systems
offer a potential solution, achieving full autonomy remains challenging. In
this work, we propose an end-to-end robotic-ultrasound-guided CVC pipeline,
from scan initialization to needle insertion. Methods: We introduce a
deep-learning model to identify clinically relevant anatomical landmarks from a
depth image of the patient's neck, obtained using RGB-D camera, to autonomously
define the scanning region and paths. Then, a robot motion planning framework
is proposed to scan, segment, reconstruct, and localize vessels (veins and
arteries), followed by the identification of the optimal insertion zone.
Finally, a needle guidance module plans the insertion under ultrasound guidance
with operator's feedback. This pipeline was validated on a high-fidelity
commercial phantom across 10 simulated clinical scenarios. Results: The
proposed pipeline achieved 10 out of 10 successful needle placements on the
first attempt. Vessels were reconstructed with a mean error of 2.15
\textit{mm}, and autonomous needle insertion was performed with an error less
than or close to 1 \textit{mm}. Conclusion: To our knowledge, this is the first
robotic CVC system demonstrated on a high-fidelity phantom with integrated
planning, scanning, and insertion. Experimental results show its potential for
clinical translation.

</details>


### [521] [Robust Speech-Workload Estimation for Intelligent Human-Robot Systems](https://arxiv.org/abs/2507.05985)
*Julian Fortune,Julie A. Adams,Jamison Heard*

Main category: cs.RO

TL;DR: 本文提出了一种实时估计语音工作负荷的算法，以改善高需求任务环境中的操作性能。


<details>
  <summary>Details</summary>
Motivation: 高需求任务环境中，操作员的工作负荷状态波动影响任务表现，需实时监测不同工作负荷成分以适应系统交互。

Method: 设计并实现了一种用于实时估计语音工作负荷的算法，分析其准确性并验证其在不同个体和人机合作范式中的通用性。

Result: 算法准确有效，能够实时估计语音工作负荷，适用于多种个体和人机团队环境。

Conclusion: 实时语音工作负荷估计是实现适应性人机系统的关键，有助于提高任务执行效率和准确性。

Abstract: Demanding task environments (e.g., supervising a remotely piloted aircraft)
require performing tasks quickly and accurately; however, periods of low and
high operator workload can decrease task performance. Intelligent modulation of
the system's demands and interaction modality in response to changes in
operator workload state may increase performance by avoiding undesirable
workload states. This system requires real-time estimation of each workload
component (i.e., cognitive, physical, visual, speech, and auditory) to adapt
the correct modality. Existing workload systems estimate multiple workload
components post-hoc, but few estimate speech workload, or function in
real-time. An algorithm to estimate speech workload and mitigate undesirable
workload states in real-time is presented. An analysis of the algorithm's
accuracy is presented, along with the results demonstrating the algorithm's
generalizability across individuals and human-machine teaming paradigms.
Real-time speech workload estimation is a crucial element towards developing
adaptive human-machine systems.

</details>


### [522] [SCCRUB: Surface Cleaning Compliant Robot Utilizing Bristles](https://arxiv.org/abs/2507.06053)
*Jakub F. Kowalewski,Keeyon Hajjafar,Alyssa Ugent,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 本文展示了软机器人臂通过扭矩和压力清洁表面粘附残留物的方法，实现了高效安全的擦洗任务。


<details>
  <summary>Details</summary>
Motivation: 传统硬式机器人虽然能施加足够的擦洗力，但因安全性限制需在隔离环境工作；软机器人安全性高，但难以产生持续扭矩和横向力以完成擦洗任务。

Method: 训练神经网络学习软臂的逆运动学和弹性，实现开环力和位置控制，使软臂能够施加连续的扭矩和压力进行擦洗。

Result: 软臂成功去除盘子上的烧焦食物残渣和马桶座上的粘性果酱，平均除污率达99.7%。

Conclusion: 软机器人能够安全有效地施加持续扭矩，用于复杂表面污染的擦洗，展示了软机器人在实际清洁应用中的潜力。

Abstract: Scrubbing surfaces is a physically demanding and time-intensive task.
Removing adhered contamination requires substantial friction generated through
pressure and torque or high lateral forces. Rigid robotic manipulators, while
capable of exerting these forces, are usually confined to structured
environments isolated from humans due to safety risks. In contrast, soft robot
arms can safely work around humans and adapt to environmental uncertainty, but
typically struggle to transmit the continuous torques or lateral forces
necessary for scrubbing. Here, we demonstrate a soft robotic arm scrubbing
adhered residues using torque and pressure, a task traditionally challenging
for soft robots. We train a neural network to learn the arm's inverse
kinematics and elasticity, which enables open-loop force and position control.
Using this learned model, the robot successfully scrubbed burnt food residue
from a plate and sticky fruit preserve from a toilet seat, removing an average
of 99.7% of contamination. This work demonstrates how soft robots, capable of
exerting continuous torque, can effectively and safely scrub challenging
contamination from surfaces.

</details>


### [523] [Learning-Augmented Model-Based Multi-Robot Planning for Time-Critical Search and Inspection Under Uncertainty](https://arxiv.org/abs/2507.06129)
*Abhish Khanal,Joseph Prince Mathew,Cameron Nowzari,Gregory J. Stein*

Main category: cs.RO

TL;DR: 本文提出了一种基于图神经网络和模型规划的多机器人协同搜索框架，用于灾害响应中优先检查可能需要紧急处理的位置，提高了搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 灾害响应和监控中快速识别需要紧急关注的区域非常关键，但部署响应团队到每个地点效率低且常不可行，因此需要多机器人高效协同，优先检查潜在重点区域，减少旅行时间。

Method: 利用图神经网络从噪声传感器数据中估计兴趣点(POIs)需要关注的可能性，并基于此预测指导多机器人模型规划器，生成成本效益高的检查路线。

Result: 仿真实验显示相较于传统和学习基线，所提规划方法在1、3、5机器人配置下分别提升性能16.3%、26.7%和26.2%；此外还在四旋翼无人机平台上进行了真实验证。

Conclusion: 该研究展示了结合图神经网络与多机器人模型规划能显著提升灾害响应中多机器人搜索与检测的效率和效果，具备实际应用潜力。

Abstract: In disaster response or surveillance operations, quickly identifying areas
needing urgent attention is critical, but deploying response teams to every
location is inefficient or often impossible. Effective performance in this
domain requires coordinating a multi-robot inspection team to prioritize
inspecting locations more likely to need immediate response, while also
minimizing travel time. This is particularly challenging because robots must
directly observe the locations to determine which ones require additional
attention. This work introduces a multi-robot planning framework for
coordinated time-critical multi-robot search under uncertainty. Our approach
uses a graph neural network to estimate the likelihood of PoIs needing
attention from noisy sensor data and then uses those predictions to guide a
multi-robot model-based planner to determine the cost-effective plan. Simulated
experiments demonstrate that our planner improves performance at least by
16.3\%, 26.7\%, and 26.2\% for 1, 3, and 5 robots, respectively, compared to
non-learned and learned baselines. We also validate our approach on real-world
platforms using quad-copters.

</details>


### [524] [Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling](https://arxiv.org/abs/2507.06149)
*Charles Champagne Cossette,Taylor Scott Clawson,Andrew Feit*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯分布轨迹的动态物体碰撞概率估计算法，具有快速且误差低的优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略了碰撞概率的时间相关性，导致碰撞概率被高估，因此需要提出一种能够有效考虑时间相关性的算法。

Method: 提出了一种自适应sigma点采样方案，利用动态物体的位置序列和高斯分布，快速估计碰撞概率，显著降低计算误差和时间复杂度。

Result: 算法在Intel Xeon Gold 6226R上实现中位误差为3.5%，中位运行时长为0.21毫秒，并在400个6秒自动驾驶日志片段的真实场景中验证了准确性与实时性。

Conclusion: 该算法简单高效，能够准确考虑碰撞概率的时间依赖性，适用于实际自动驾驶等领域中动态物体碰撞概率的估计。

Abstract: A novel algorithm is presented for the estimation of collision probabilities
between dynamic objects with uncertain trajectories, where the trajectories are
given as a sequence of poses with Gaussian distributions. We propose an
adaptive sigma-point sampling scheme, which ultimately produces a fast, simple
algorithm capable of estimating the collision probability with a median error
of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold
6226R Processor. Importantly, the algorithm explicitly accounts for the
collision probability's temporal dependence, which is often neglected in prior
work and otherwise leads to an overestimation of the collision probability.
Finally, the method is tested on a diverse set of relevant real-world
scenarios, consisting of 400 6-second snippets of autonomous vehicle logs,
where the accuracy and latency is rigorously evaluated.

</details>


### [525] [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
*William Li,Lei Hamilton,Kaise Al-natour,Sanjeev Mohindra*

Main category: cs.RO

TL;DR: 本文评估了大型语言模型在Meta PARTNER基准上的机器人任务解决能力，发现推理型模型表现优于非推理型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在具身机器人任务中的表现，以推动具身机器人领域的发展。

Method: 利用Meta PARTNER基准中的随机厨房场景进行实验，比较了多种前沿模型在不同观测配置下的表现。

Result: 推理模型OpenAI o3-mini在所有测试配置中均优于非推理模型GPT-4o和Llama 3。

Conclusion: 推理型大型语言模型在具身机器人任务中表现更佳，显示了推动该领域研究的潜力。

Abstract: This paper focuses on evaluating the effectiveness of Large Language Models
at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR
provides simplified environments and robotic interactions within randomized
indoor kitchen scenes. Each randomized kitchen scene is given a task where two
robotic agents cooperatively work together to solve the task. We evaluated
multiple frontier models on Meta PARTNER environments. Our results indicate
that reasoning models like OpenAI o3-mini outperform non-reasoning models like
OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied
environments. o3-mini displayed outperform across centralized, decentralized,
full observability, and partial observability configurations. This provides a
promising avenue of research for embodied robotic development.

</details>


### [526] [Learning Agile Tensile Perching for Aerial Robots from Demonstrations](https://arxiv.org/abs/2507.06172)
*Kangle Yuan,Atar Babgei,Luca Romanello,Hai-Nguyen Nguyen,Ronald Clark,Mirko Kovac,Sophie F. Armanini,Basaran Bahadir Kocer*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的轨迹控制框架，实现无人机通过带有绳索的张力附着机制准确附着在多种结构上。


<details>
  <summary>Details</summary>
Motivation: 无人机通过附着在结构上延长续航时间，但传统张力附着机制建模复杂，需精确控制飞行动力学和绳索动力。

Method: 采用带示范的软行为者评论家算法（SACfD），结合最佳与次优示范，提高训练效率，实现位置和速度的精确控制，从而准确锁定绳索特定位段并完成缠绕固定。

Result: 通过大量仿真和实地实验验证，框架能够实现灵活且可靠的轨迹生成，确保张力附着的成功。

Conclusion: 所提轨迹框架有效解决了张力附着的复杂动力学问题，提升了无人机附着的操控精准性和稳定性。

Abstract: Perching on structures such as trees, beams, and ledges is essential for
extending the endurance of aerial robots by enabling energy conservation in
standby or observation modes. A tethered tensile perching mechanism offers a
simple, adaptable solution that can be retrofitted to existing robots and
accommodates a variety of structure sizes and shapes. However, tethered tensile
perching introduces significant modelling challenges which require precise
management of aerial robot dynamics, including the cases of tether slack &
tension, and momentum transfer. Achieving smooth wrapping and secure anchoring
by targeting a specific tether segment adds further complexity. In this work,
we present a novel trajectory framework for tethered tensile perching,
utilizing reinforcement learning (RL) through the Soft Actor-Critic from
Demonstrations (SACfD) algorithm. By incorporating both optimal and suboptimal
demonstrations, our approach enhances training efficiency and responsiveness,
achieving precise control over position and velocity. This framework enables
the aerial robot to accurately target specific tether segments, facilitating
reliable wrapping and secure anchoring. We validate our framework through
extensive simulation and real-world experiments, and demonstrate effectiveness
in achieving agile and reliable trajectory generation for tensile perching.

</details>


### [527] [Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model](https://arxiv.org/abs/2507.06174)
*Koki Yamane,Yunhan Li,Masashi Konosu,Koki Inami,Junji Oaki,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本论文提出利用4通道双边控制实现具备力反馈的低成本机械臂快速远程操作，并通过集成非线性补偿、速度及外力估计和可变增益技术提升性能。同时，结合力反馈信息的模仿学习策略表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有远程操作多依赖单边控制，缺乏力反馈，难以胜任快速和接触丰富的任务。本文旨在克服这一局限，实现低成本机械臂的高性能远程操作。

Method: 基于准确的机械臂动力学辨识，采用4通道双边控制，集成非线性项补偿、速度与外力估计及与惯量变化对应的可变增益，并将力反馈信息纳入模仿学习策略的输入和输出。

Result: 证明了即使在无力传感器的低成本机械臂上，利用4通道双边控制也能实现带力反馈的快速远程操作；模仿学习中加入力反馈显著提升策略表现。

Conclusion: 本方法实验证明了结合精确动力学和力反馈的4通道双边控制具备实际应用价值，可以在经济实惠的硬件上实现高保真远程操控和示教数据采集。

Abstract: In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.

</details>


### [528] [Is Diversity All You Need for Scalable Robotic Manipulation?](https://arxiv.org/abs/2507.06219)
*Modi Shi,Li Chen,Jin Chen,Yuxiang Lu,Chiming Liu,Guanghui Ren,Ping Luo,Di Huang,Maoqing Yao,Hongyang Li*

Main category: cs.RO

TL;DR: 本论文研究了数据多样性在机器人操作学习中影响数据扩展效果的作用，挑战了“多样性越多越好”的传统观点。


<details>
  <summary>Details</summary>
Motivation: 尽管在NLP和CV领域数据扩展带来了成功，但机器人操作领域中数据扩展原理尚未被充分理解，特别是任务、机器人类型和示范者多样性三方面的影响。

Method: 通过在不同机器人平台上进行大量实验，分析任务多样性、机器人多样性及专家多样性对模型迁移和学习性能的影响，并提出了基于速度模态去偏方法以缓解学习中的速度歧义问题。

Result: 发现任务多样性比单任务示范数更关键，单一高质量机器人示范数据能更高效地实现跨机器人平台迁移，多样化专家示范存在干扰，针对速度多模态的去偏方法提升性能15%，相当于使用2.5倍预训练数据。

Conclusion: 本研究揭示了机器人操作中有效数据规模化的关键因素，为数据采集和模型训练提供了新的视角和实践指导。

Abstract: Data scaling has driven remarkable success in foundation models for Natural
Language Processing (NLP) and Computer Vision (CV), yet the principles of
effective data scaling in robotic manipulation remain insufficiently
understood. In this work, we investigate the nuanced role of data diversity in
robot learning by examining three critical dimensions-task (what to do),
embodiment (which robot to use), and expert (who demonstrates)-challenging the
conventional intuition of "more diverse is better". Throughout extensive
experiments on various robot platforms, we reveal that (1) task diversity
proves more critical than per-task demonstration quantity, benefiting transfer
from diverse pre-training tasks to novel downstream scenarios; (2)
multi-embodiment pre-training data is optional for cross-embodiment
transfer-models trained on high-quality single-embodiment data can efficiently
transfer to different platforms, showing more desirable scaling property during
fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,
arising from individual operational preferences and stochastic variations in
human demonstrations, can be confounding to policy learning, with velocity
multimodality emerging as a key contributing factor. Based on this insight, we
propose a distribution debiasing method to mitigate velocity ambiguity, the
yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to
using 2.5 times pre-training data. Collectively, these findings provide new
perspectives and offer practical guidance on how to scale robotic manipulation
datasets effectively.

</details>


### [529] [EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://arxiv.org/abs/2507.06224)
*Yixiang Chen,Peiyan Li,Yan Huang,Jiabing Yang,Kehan Chen,Liang Wang*

Main category: cs.RO

TL;DR: EC-Flow通过利用机器人本体运动学从无标签视频中学习操作，实现对变形物体、遮挡和非位移任务的强大泛化能力，在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动的机器人操作系统依赖低层动作标注数据，且受限于刚性物体场景，难以处理复杂操作如变形物体和遮挡问题。

Method: 提出基于本体运动学的EC-Flow框架，预测本体中心流，结合目标对齐模块通过运动一致性和目标图像预测实现语言指令与物体交互的连接，并利用标准URDF文件将预测转换为机器人执行动作。

Result: 在模拟和真实任务中，EC-Flow分别在遮挡物体操作、变形物体操作和非位移任务上比现有方法提升62%、45%和80%。

Conclusion: EC-Flow有效利用机器人本体运动学提升了无标注视频学习的泛化能力，拓展了语言驱动机器人操作的应用场景，具备实际可用性和显著性能优势。

Abstract: Current language-guided robotic manipulation systems often require low-level
action-labeled datasets for imitation learning. While object-centric flow
prediction methods mitigate this issue, they remain limited to scenarios
involving rigid objects with clear displacement and minimal occlusion. In this
work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly
learns manipulation from action-unlabeled videos by predicting
embodiment-centric flow. Our key insight is that incorporating the embodiment's
inherent kinematics significantly enhances generalization to versatile
manipulation scenarios, including deformable object handling, occlusions, and
non-object-displacement tasks. To connect the EC-Flow with language
instructions and object interactions, we further introduce a goal-alignment
module by jointly optimizing movement consistency and goal-image prediction.
Moreover, translating EC-Flow to executable robot actions only requires a
standard robot URDF (Unified Robot Description Format) file to specify
kinematic constraints across joints, which makes it easy to use in practice. We
validate EC-Flow on both simulation (Meta-World) and real-world tasks,
demonstrating its state-of-the-art performance in occluded object handling (62%
improvement), deformable object manipulation (45% improvement), and
non-object-displacement tasks (80% improvement) than prior state-of-the-art
object-centric flow methods. For more information, see our project website at
https://ec-flow1.github.io .

</details>


### [530] [A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation](https://arxiv.org/abs/2507.05331)
*TRI LBM Team,Jose Barreiros,Andrew Beaulieu,Aditya Bhat,Rick Cory,Eric Cousineau,Hongkai Dai,Ching-Hsin Fang,Kunimatsu Hashimoto,Muhammad Zubair Irshad,Masha Itkina,Naveen Kuppuswamy,Kuan-Hui Lee,Katherine Liu,Dale McConachie,Ian McMahon,Haruki Nishimura,Calder Phillips-Grafflin,Charles Richter,Paarth Shah,Krishnan Srinivasan,Blake Wulfe,Chen Xu,Mengchao Zhang,Alex Alspach,Maya Angeles,Kushal Arora,Vitor Campagnolo Guizilini,Alejandro Castro,Dian Chen,Ting-Sheng Chu,Sam Creasey,Sean Curtis,Richard Denitto,Emma Dixon,Eric Dusel,Matthew Ferreira,Aimee Goncalves,Grant Gould,Damrong Guoy,Swati Gupta,Xuchen Han,Kyle Hatch,Brendan Hathaway,Allison Henry,Hillel Hochsztein,Phoebe Horgan,Shun Iwase,Donovon Jackson,Siddharth Karamcheti,Sedrick Keh,Joseph Masterjohn,Jean Mercat,Patrick Miller,Paul Mitiguy,Tony Nguyen,Jeremy Nimmer,Yuki Noguchi,Reko Ong,Aykut Onol,Owen Pfannenstiehl,Richard Poyner,Leticia Priebe Mendes Rocha,Gordon Richardson,Christopher Rodriguez,Derick Seale,Michael Sherman,Mariah Smith-Jones,David Tago,Pavel Tokmakov,Matthew Tran,Basile Van Hoorick,Igor Vasiljevic,Sergey Zakharov,Mark Zolotas,Rares Ambrus,Kerri Fetzer-Borelli,Benjamin Burchfiel,Hadas Kress-Gazit,Siyuan Feng,Stacie Ford,Russ Tedrake*

Main category: cs.RO

TL;DR: 本文评估了多任务机器人操作策略（大型行为模型LBMs），发现多任务预训练提升了策略的成功率和鲁棒性，且可用更少数据更快学习复杂新任务。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作虽然进步显著，但真实世界性能评估困难，限制了发展和对能力的理解。

Method: 扩展扩散策略范式，构建严格评测流程，使用模拟和真实机器人数据进行盲测对比多任务与单任务模型。

Result: 多任务预训练策略比单任务更成功、鲁棒，且预训练规模和多样性增加时性能提升明显。

Conclusion: 多任务预训练和大规模多样数据是提升机器人操作策略性能的关键，有助于快速学习复杂任务并推动基础模型发展。

Abstract: Robot manipulation has seen tremendous progress in recent years, with
imitation learning policies enabling successful performance of dexterous and
hard-to-model tasks. Concurrently, scaling data and model size has led to the
development of capable language and vision foundation models, motivating
large-scale efforts to create general-purpose robot foundation models. While
these models have garnered significant enthusiasm and investment, meaningful
evaluation of real-world performance remains a challenge, limiting both the
pace of development and inhibiting a nuanced understanding of current
capabilities. In this paper, we rigorously evaluate multitask robot
manipulation policies, referred to as Large Behavior Models (LBMs), by
extending the Diffusion Policy paradigm across a corpus of simulated and
real-world robot data. We propose and validate an evaluation pipeline to
rigorously analyze the capabilities of these models with statistical
confidence. We compare against single-task baselines through blind, randomized
trials in a controlled setting, using both simulation and real-world
experiments. We find that multi-task pretraining makes the policies more
successful and robust, and enables teaching complex new tasks more quickly,
using a fraction of the data when compared to single-task baselines. Moreover,
performance predictably increases as pretraining scale and diversity grows.
Project page: https://toyotaresearchinstitute.github.io/lbm1/

</details>


### [531] [Feature Geometry for Stereo Sidescan and Forward-looking Sonar](https://arxiv.org/abs/2507.05410)
*Kalin Norman,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 本文提出了一种基于几何的方法，用于将水声立体声中一个声纳观测到的特征投影到另一个声纳，实现跨模态声纳数据融合。


<details>
  <summary>Details</summary>
Motivation: 实现水下机器人中不同类型声纳之间的特征对应和三维信息恢复，提高海洋声纳数据融合的准确性与应用效果。

Method: 借鉴立体视觉中极线几何，通过已知的声纳相对位姿，将一个声纳图像中观测到的特征投影到另一个声纳图像中，分析特征位置和两个声纳相对位姿对投影的影响。

Result: 通过仿真验证了方法的有效性，并识别出了适合水下机器人应用的立体声纳配置，提升了特征对应和三维信息恢复能力。

Conclusion: 该提出的几何投影方法为跨模态声纳立体声系统提供了理论基础和实用策略，促进了水下机器人领域的声纳数据融合和环境感知。

Abstract: In this paper, we address stereo acoustic data fusion for marine robotics and
propose a geometry-based method for projecting observed features from one sonar
to another for a cross-modal stereo sonar setup that consists of both a
forward-looking and a sidescan sonar. Our acoustic geometry for sidescan and
forward-looking sonar is inspired by the epipolar geometry for stereo cameras,
and we leverage relative pose information to project where an observed feature
in one sonar image will be found in the image of another sonar. Additionally,
we analyze how both the feature location relative to the sonar and the relative
pose between the two sonars impact the projection. From simulated results, we
identify desirable stereo configurations for applications in field robotics
like feature correspondence and recovery of the 3D information of the feature.

</details>


### [532] [CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning](https://arxiv.org/abs/2507.05458)
*Yi-Shiuan Tung,Bradley Hayes,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文提出了CRED，一种主动偏好学习中的轨迹生成方法，通过环境设计和轨迹选择联合优化，提高奖励函数估计的准确性，适用于长时程任务。


<details>
  <summary>Details</summary>
Motivation: 现有主动偏好学习方法在探索轨迹空间和识别信息查询方面表现不佳，特别是在长时程任务中，难以全面优化人类偏好。

Method: CRED通过环境设计“想象”新场景，并利用反事实推理采样当前奖励信念下的可能偏好，生成多样且信息丰富的轨迹集合，从而提升奖励学习效果。

Result: 在GridWorld和基于OpenStreetMap数据的真实导航实验中，CRED显著改进奖励学习效果，并能有效适应不同环境。

Conclusion: CRED方法通过联合环境设计和轨迹选择，提升了主动偏好学习中的奖励估计和轨迹生成能力，促进机器人更好地适应复杂的人类偏好需求。

Abstract: For effective real-world deployment, robots should adapt to human
preferences, such as balancing distance, time, and safety in delivery routing.
Active preference learning (APL) learns human reward functions by presenting
trajectories for ranking. However, existing methods often struggle to explore
the full trajectory space and fail to identify informative queries,
particularly in long-horizon tasks. We propose CRED, a trajectory generation
method for APL that improves reward estimation by jointly optimizing
environment design and trajectory selection. CRED "imagines" new scenarios
through environment design and uses counterfactual reasoning -- by sampling
rewards from its current belief and asking "What if this reward were the true
preference?" -- to generate a diverse and informative set of trajectories for
ranking. Experiments in GridWorld and real-world navigation using OpenStreetMap
data show that CRED improves reward learning and generalizes effectively across
different environments.

</details>


### [533] [Gaussian Process-Based Active Exploration Strategies in Vision and Touch](https://arxiv.org/abs/2507.05522)
*Ho Jin Choi,Nadia Figueroa*

Main category: cs.RO

TL;DR: 该论文提出通过融合视觉和触觉数据，使用统一的高斯过程距离场（GPDF）表征实现机器人对物体属性的主动感知。


<details>
  <summary>Details</summary>
Motivation: 机器人缺乏对物体形状、材质及语义等属性的理解，限制了其在非结构化环境中的操作能力。

Method: 利用点云构建高斯过程距离场，结合可微渲染的视觉测量和触觉测量，对物体几何形状进行迭代精细优化，规划探索动作以最大化信息增益。

Result: 在Franka Research 3机器人平台上，成功实现了对静态放置物体的形状及属性的主动探索与精准恢复。

Conclusion: 该方法无需大量预训练，能够有效融合多感官数据，提升机器人对复杂物体几何及表面属性的建模能力，具备良好的扩展潜力。

Abstract: Robots struggle to understand object properties like shape, material, and
semantics due to limited prior knowledge, hindering manipulation in
unstructured environments. In contrast, humans learn these properties through
interactive multi-sensor exploration. This work proposes fusing visual and
tactile observations into a unified Gaussian Process Distance Field (GPDF)
representation for active perception of object properties. While primarily
focusing on geometry, this approach also demonstrates potential for modeling
surface properties beyond geometry. The GPDF encodes signed distance using
point cloud, analytic gradient and Hessian, and surface uncertainty estimates,
which are attributes that common neural network shape representation lack. By
utilizing a point cloud to construct a distance function, GPDF does not need
extensive pretraining on large datasets and can incorporate observations by
aggregation. Starting with an initial visual shape estimate, the framework
iteratively refines the geometry by integrating dense vision measurements using
differentiable rendering and tactile measurements at uncertain surface regions.
By quantifying multi-sensor uncertainties, it plans exploratory motions to
maximize information gain for recovering precise 3D structures. For the
real-world robot experiment, we utilize the Franka Research 3 robot
manipulator, which is fixed on a table and has a customized DIGIT tactile
sensor and an Intel Realsense D435 RGBD camera mounted on the end-effector. In
these experiments, the robot explores the shape and properties of objects
assumed to be static and placed on the table. To improve scalability, we
investigate approximation methods like inducing point method for Gaussian
Processes. This probabilistic multi-modal fusion enables active exploration and
mapping of complex object geometries, extending potentially beyond geometry.

</details>


### [534] [PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs](https://arxiv.org/abs/2507.05555)
*Obin Kwon,Sankalp Yamsani,Noboru Myers,Sean Taylor,Jooyoung Hong,Kyungseo Park,Alex Alspach,Joohyung Kim*

Main category: cs.RO

TL;DR: PAPRLE是一个模块化的机器人肢体环境，支持灵活配置和多种输入设备控制，实现双向遥操作和实时力反馈，促进空间布局创新和扩展数据采集，推动具身AI和学习控制研究。


<details>
  <summary>Details</summary>
Motivation: 提升机器人肢体的灵活配置能力及多样化控制输入，满足不同遥操作任务需求。

Method: 设计了模块化系统和可插拔操纵装置，支持多种控制设备及机器人配置，实现关节空间与任务空间的双向遥操作和实时力反馈。

Result: 系统在多种实际应用场景中表现出高度通用性和适应性，支持不同主从设备组合，增强用户交互体验。

Conclusion: PAPRLE通过开放硬件和软件资源，促进社区共同发展，推动机器人肢体控制领域的创新与应用拓展。

Abstract: We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular
ecosystem that enables flexible placement and control of robotic limbs. With
PAPRLE, a user can change the arrangement of the robotic limbs, and control
them using a variety of input devices, including puppeteers, gaming
controllers, and VR-based interfaces. This versatility supports a wide range of
teleoperation scenarios and promotes adaptability to different task
requirements. To further enhance configurability, we introduce a pluggable
puppeteer device that can be easily mounted and adapted to match the target
robot configurations. PAPRLE supports bilateral teleoperation through these
puppeteer devices, agnostic to the type or configuration of the follower robot.
By supporting both joint-space and task-space control, the system provides
real-time force feedback, improving user fidelity and physical interaction
awareness. The modular design of PAPRLE facilitates novel spatial arrangements
of the limbs and enables scalable data collection, thereby advancing research
in embodied AI and learning-based control. We validate PAPRLE in various
real-world settings, demonstrating its versatility across diverse combinations
of leader devices and follower robots. The system will be released as open
source, including both hardware and software components, to support broader
adoption and community-driven extension. Additional resources and
demonstrations are available at the project website:
https://uiuckimlab.github.io/paprle-pages

</details>


### [535] [Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube](https://arxiv.org/abs/2507.05607)
*Chongshan Fan,Shenghai Yuan*

Main category: cs.RO

TL;DR: Auto-RubikAI是一种集成知识库、视觉语言模型和大型语言模型的自主规划框架，实现了魔方的还原操作。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统依赖预定义脚本或大量演示数据，缺乏解释性及灵活性，且现有大型语言模型在符号推理上存在不足。

Method: 结合知识库模块解决群论还原步骤，视觉语言模型解析RGB-D构建3D语义场景，大型语言模型通过提示链生成结构化机器人控制代码，形成三模块架构应对空间不确定性。

Result: 模拟和真实环境部署，7自由度机器人手臂表现出79%的任务成功率，较CFOP、DeepCubeA和Two-Phase基线减少了解决步骤，并保证了解释性和安全性。

Conclusion: Auto-RubikAI提供了一种模块化、低成本、高解释性的解决方案，适用于智能制造、机器人教育和自主执行等领域。

Abstract: This paper presents Auto-RubikAI, a modular autonomous planning framework
that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM),
and a large language model (LLM) to solve structured manipulation tasks
exemplified by Rubik's Cube restoration. Unlike traditional robot systems based
on predefined scripts, or modern approaches relying on pretrained networks and
large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step
task execution with minimal data requirements and no prior demonstrations. The
proposed system employs a KB module to solve group-theoretic restoration steps,
overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to
construct a semantic 3D scene representation, while the LLM generates
structured robotic control code via prompt chaining. This tri-module
architecture enables robust performance under spatial uncertainty. We deploy
Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic
arm, demonstrating effective Sim-to-Real adaptation without retraining.
Experiments show a 79% end-to-end task success rate across randomized
configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our
KB-enhanced method reduces average solution steps while maintaining
interpretability and safety. Auto-RubikAI provides a cost-efficient, modular
foundation for embodied task planning in smart manufacturing, robotics
education, and autonomous execution scenarios. Code, prompts, and hardware
modules will be released upon publication.

</details>


### [536] [DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation](https://arxiv.org/abs/2507.05627)
*Young Hun Kim,Seungyeon Kim,Yonghyeon Lee,Frank Chongwoo Park*

Main category: cs.RO

TL;DR: DreamGrasp利用大型预训练图像生成模型的想象能力，结合粗略3D重建、对比学习分割和文本引导细化，实现了复杂多物体环境下的部分视角3D识别和重建。


<details>
  <summary>Details</summary>
Motivation: 传统基于强对称性先验或监督学习的方法难以应对现实中视角受限、多物体遮挡的挑战，缺乏泛化能力，导致难以进行准确的3D识别与重建。

Method: 提出DreamGrasp框架，利用大规模预训练图像生成模型推断场景未观察部分，结合粗略的3D重建、对比学习实例分割和文本引导的实例细化步骤，提升部分视角3D识别的鲁棒性。

Result: DreamGrasp在复杂多物体环境下能准确恢复物体几何形状，并支持后续任务如顺序去杂和目标检索，成功率高。

Conclusion: 通过利用图像生成模型的想象能力和多模态融合，DreamGrasp有效突破了传统部分视角3D识别的限制，具有广泛的应用前景。

Abstract: Partial-view 3D recognition -- reconstructing 3D geometry and identifying
object instances from a few sparse RGB images -- is an exceptionally
challenging yet practically essential task, particularly in cluttered, occluded
real-world settings where full-view or reliable depth data are often
unavailable. Existing methods, whether based on strong symmetry priors or
supervised learning on curated datasets, fail to generalize to such scenarios.
In this work, we introduce DreamGrasp, a framework that leverages the
imagination capability of large-scale pre-trained image generative models to
infer the unobserved parts of a scene. By combining coarse 3D reconstruction,
instance segmentation via contrastive learning, and text-guided instance-wise
refinement, DreamGrasp circumvents limitations of prior methods and enables
robust 3D reconstruction in complex, multi-object environments. Our experiments
show that DreamGrasp not only recovers accurate object geometry but also
supports downstream tasks like sequential decluttering and target retrieval
with high success rates.

</details>


### [537] [A Physics-Based Continuum Model for Versatile, Scalable, and Fast Terramechanics Simulation](https://arxiv.org/abs/2507.05643)
*Huzaifa Unjhawala,Luning Bakke,Harry Zhang,Michael Taylor,Ganesh Arivoli,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: Chrono::CRM是一种基于物理的通用、高效可扩展的土壤机械模拟模型，支持复杂地形和设备交互，且具备GPU加速和大规模模拟能力。


<details>
  <summary>Details</summary>
Motivation: 传统土壤机械模型多为半经验模型，无法应对复杂任务如挖掘和复杂轮胎与土壤的相互作用，需求一种更准确高效的物理建模方法。

Method: 基于Chrono的SPH框架，采用物理驱动的连续表示模型，结合GPU加速和“主动区域”技术实现大规模高效模拟，支持刚性和柔性工具的交互。

Result: 模型通过NASA MGRU3探测车实验和高保真DEM仿真验证，计算效率和半经验方法相当，能处理长达10公里、含上亿SPH粒子的地形。

Conclusion: Chrono::CRM为高保真、大规模越野土壤机械仿真提供了开放源码的高效工具，促进了相关领域的研究和应用发展。

Abstract: This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.

</details>


### [538] [3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting](https://arxiv.org/abs/2507.05661)
*Haitao Lu,Haijier Chen,Haoze Liu,Shoujian Zhang,Bo Xu,Ziao Liu*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D高斯点云的大规模重定位框架3DGS-LSR，使用单目RGB图像实现厘米级定位，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市环境中，GNSS信号受阻和多路径效应导致定位不可靠，传统地图存储和计算资源需求高，不适合资源受限的机器人平台。

Method: 结合多传感器数据构建高精度3D高斯点云地图，机器人端仅需单目摄像头。利用SuperPoint和SuperGlue提取匹配特征，并通过迭代优化和逐步渲染实现实时定位。

Result: 在KITTI数据集上，3DGS-LSR在不同道路场景中平均定位精度分别为0.026m、0.029m和0.081m，显著优于其他方法且仅需单目图像输入。

Conclusion: 该方法在复杂城市环境中为自主机器人提供了可靠的定位能力，有效解决了GNSS信号失效问题，支持实时导航。

Abstract: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

</details>


### [539] [Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains](https://arxiv.org/abs/2507.05663)
*Neelay Joglekar,Fei Liu,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 该论文提出了一种稳定的遥中心运动(RCM)机器人操纵器控制方法，解决了关节传感误差带来的运动学计算问题，促进了自动化微创手术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有RCM机器人在受限视野下的关节误差无法修正，影响精确控制，限制了自动辅助手术的实现。

Method: 设计了一个针对视野外链路的稳定跟踪反馈控制器，并将其整合入双层控制方案，理论验证其稳定性。

Result: 在模拟和真实环境中系统验证了方法的稳定性和有效性，确保了误差可控，提升了机械臂控制精度。

Conclusion: 该研究为实现从遥控到自主微创手术提供了理论和实践基础，推动了手术机器人自动化进程。

Abstract: Remote Center of Motion (RCM) robotic manipulators have revolutionized
Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation
within the patient's body cavity without disturbing the insertion point on the
patient. Accurate RCM tool control is vital for incorporating autonomous
subtasks like suturing, blood suction, and tumor resection into robotic
surgical procedures, reducing surgeon fatigue and improving patient outcomes.
However, these cable-driven systems are subject to significant joint reading
errors, corrupting the kinematics computation necessary to perform control.
Although visual tracking with endoscopic cameras can correct errors on in-view
joints, errors in the kinematic chain prior to the insertion point are
irreparable because they remain out of view. No prior work has characterized
the stability of control under these conditions. We fill this gap by designing
a provably stable tracking-in-the-loop controller for the out-of-view portion
of the RCM manipulator kinematic chain. We additionally incorporate this
controller into a bilevel control scheme for the full kinematic chain. We
rigorously benchmark our method in simulated and real world settings to verify
our theoretical findings. Our work provides key insights into the next steps
required for the transition from teleoperated to autonomous surgery.

</details>


### [540] [Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control](https://arxiv.org/abs/2507.05674)
*Xinyao Qin,Xiaoteng Ma,Yang Qi,Qihan Liu,Chuanyi Xue,Ning Gui,Qinyu Dong,Jun Yang,Bin Liang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于扩散模型的四足机器人控制框架DMLoco，结合了多任务预训练与在线PPO微调，实现语言驱动控制和稳定任务切换.


<details>
  <summary>Details</summary>
Motivation: 扩散模型在多任务泛化和语言条件控制方面表现出色，但其在四足机器人行走中的应用较少，主要因稳定性问题和任务切换困难。利用在线强化学习的优势，旨在弥补这些不足。

Method: 首先使用扩散模型在多任务数据集上进行预训练，实现语言指导的多技能执行；然后在仿真中通过PPO算法微调，提升鲁棒性和任务切换稳定性；采用DDIM采样和TensorRT优化实现实时高效部署。

Result: DMLoco在资源受限的平台上实现了50Hz的高效运行，能够适应语言指导的多任务控制并稳定过渡任务，实现了四足机器人的自适应运动。

Conclusion: DMLoco有效结合了扩散模型与在线强化学习，解决了四足机器人的多任务语言控制和稳定任务转换问题，具备高效、可扩展的实际应用潜力。

Abstract: Recent research has highlighted the powerful capabilities of imitation
learning in robotics. Leveraging generative models, particularly diffusion
models, these approaches offer notable advantages such as strong multi-task
generalization, effective language conditioning, and high sample efficiency.
While their application has been successful in manipulation tasks, their use in
legged locomotion remains relatively underexplored, mainly due to compounding
errors that affect stability and difficulties in task transition under limited
data. Online reinforcement learning (RL) has demonstrated promising results in
legged robot control in the past years, providing valuable insights to address
these challenges. In this work, we propose DMLoco, a diffusion-based framework
for quadruped robots that integrates multi-task pretraining with online PPO
finetuning to enable language-conditioned control and robust task transitions.
Our approach first pretrains the policy on a diverse multi-task dataset using
diffusion models, enabling language-guided execution of various skills. Then,
it finetunes the policy in simulation to ensure robustness and stable task
transition during real-world deployment. By utilizing Denoising Diffusion
Implicit Models (DDIM) for efficient sampling and TensorRT for optimized
deployment, our policy runs onboard at 50Hz, offering a scalable and efficient
solution for adaptive, language-guided locomotion on resource-constrained
robotic platforms.

</details>


### [541] [Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning](https://arxiv.org/abs/2507.05695)
*Xiatao Sun,Yuxuan Wang,Shuo Yang,Yinxing Chen,Daniel Rakita*

Main category: cs.RO

TL;DR: 这篇论文提出了hPGA-DP，一种结合了投影几何代数（PGA）以增强几何归纳偏置的扩散策略，用于机器人学习中的运动生成。


<details>
  <summary>Details</summary>
Motivation: 传统扩散策略在机器人任务中需要每次从零学习空间表示（如平移和旋转），效率较低。引入几何归纳偏置可以减少这种冗余，提高训练效率。

Method: 该方法引入了基于PGA的项目几何代数变换器（P-GATr），利用其E(3)对称性，构建混合架构，将P-GATr用作状态编码器和动作解码器，同时使用U-Net或Transformer模块进行去噪。

Result: 在模拟和真实环境中，hPGA-DP实验结果显示通过P-GATr的几何偏置提升了任务性能和训练效率，并且混合模型较单一P-GATr架构实现了更快收敛。

Conclusion: 结合PGA的几何归纳偏置与混合架构的扩散策略显著提升了机器人运动生成的性能和训练速度，证明了该方法的有效性和潜力。

Abstract: Diffusion policies have become increasingly popular in robot learning due to
their reliable convergence in motion generation tasks. At a high level, these
policies learn to transform noisy action trajectories into effective ones,
conditioned on observations. However, each time such a model is trained in a
robotics context, the network must relearn fundamental spatial representations
and operations, such as translations and rotations, from scratch in order to
ground itself and operate effectively in a 3D environment. Incorporating
geometric inductive biases directly into the network can alleviate this
redundancy and substantially improve training efficiency. In this paper, we
introduce hPGA-DP, a diffusion policy approach that integrates a mathematical
framework called Projective Geometric Algebra (PGA) to embed strong geometric
inductive biases. PGA is particularly well-suited for this purpose as it
provides a unified algebraic framework that naturally encodes geometric
primitives, such as points, directions, and rotations, enabling neural networks
to reason about spatial structure through interpretable and composable
operations. Specifically, we propose a novel diffusion policy architecture that
incorporates the Projective Geometric Algebra Transformer (P-GATr), leveraging
its E(3)-equivariant properties established in prior work. Our approach adopts
a hybrid architecture strategy, using P-GATr as both a state encoder and action
decoder, while employing U-Net or Transformer-based modules for the denoising
process. Several experiments and ablation studies in both simulated and
real-world environments demonstrate that hPGA-DP not only improves task
performance and training efficiency through the geometric bias of P-GATr, but
also achieves substantially faster convergence through its hybrid model
compared to architectures that rely solely on P-GATr.

</details>


### [542] [DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving](https://arxiv.org/abs/2507.05710)
*Hyeongchan Ham,Heejin Ahn*

Main category: cs.RO

TL;DR: 本文提出了一种结合证据深度学习的分布鲁棒优化框架，用于处理自动驾驶中感知不确定性，提升运动规划的安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 考虑到自动驾驶系统中基于神经网络的感知存在固有不确定性，传统控制决策面临显著安全风险。

Method: 利用证据深度学习评估感知不确定性，设计基于证据分布的新型模糊集合，并将其集成至模型预测控制（MPC），提出DRO-EDL-MPC算法。

Result: 在CARLA模拟器中验证，算法在高感知置信时保持效率，低置信时实现保守约束，有效平衡安全与性能。

Conclusion: 该方法通过动态调整控制保守性，有效应对感知不确定性，提升自动驾驶运动规划的安全性和鲁棒性。

Abstract: Safety is a critical concern in motion planning for autonomous vehicles.
Modern autonomous vehicles rely on neural network-based perception, but making
control decisions based on these inference results poses significant safety
risks due to inherent uncertainties. To address this challenge, we present a
distributionally robust optimization (DRO) framework that accounts for both
aleatoric and epistemic perception uncertainties using evidential deep learning
(EDL). Our approach introduces a novel ambiguity set formulation based on
evidential distributions that dynamically adjusts the conservativeness
according to perception confidence levels. We integrate this uncertainty-aware
constraint into model predictive control (MPC), proposing the DRO-EDL-MPC
algorithm with computational tractability for autonomous driving applications.
Validation in the CARLA simulator demonstrates that our approach maintains
efficiency under high perception confidence while enforcing conservative
constraints under low confidence.

</details>


### [543] [Simultaneous Triggering and Synchronization of Sensors and Onboard Computers](https://arxiv.org/abs/2507.05717)
*Morten Nissov,Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、实时的时间戳同步系统，利用现成组件和成熟同步方法，提高机器人传感器数据的时间戳准确性。


<details>
  <summary>Details</summary>
Motivation: 机器人高精度估计算法依赖准确数据，但传感器数据时间戳问题常被忽视，影响在线估计性能。

Method: 设计了一种多功能系统，结合现成硬件和成熟同步技术，实现高低频传感器的同步和触发。

Result: 系统成功演示了传感器数据同步和触发能力，有效解决了时间戳不准确问题。

Conclusion: 提出的系统实现了实时、低成本且准确的时间戳同步，提升了机器人传感器数据质量及估计算法性能。

Abstract: High fidelity estimation algorithms for robotics require accurate data.
However, timestamping of sensor data is a key issue that rarely receives the
attention it deserves. Inaccurate timestamping can be compensated for in
post-processing but is imperative for online estimation. Simultaneously, even
online mitigation of timing issues can be achieved through a relaxation of the
tuning parameters from their otherwise more performative optimal values, but at
a detriment to performance. To address the need for real-time, low-cost
timestamping, a versatile system which utilizes readily-available components
and established methods for synchronization is introduced. The synchronization
and triggering (of both high- and low-rate sensors) capabilities of the system
are demonstrated.

</details>


### [544] [A Learning-based Planning and Control Framework for Inertia Drift Vehicles](https://arxiv.org/abs/2507.05748)
*Bei Zhou,Zhouheng Li,Lei Xie,Hongye Su,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化的规划与控制框架，以实现高效的惯性漂移过渡，提升自动赛车在连续急弯中的表现。


<details>
  <summary>Details</summary>
Motivation: 惯性漂移作为两阶段持续漂移之间的过渡动作，对自动赛车连续急弯导航有重要意义，但现有漂移控制难以在快速转向与轨迹跟踪间保持平衡，且精确建模受到动力耦合与环境影响限制。

Method: 利用贝叶斯优化开发规划逻辑，实现惯性漂移与持续漂移间的平滑过渡及速度损失最小化；同时通过贝叶斯优化学习基于性能的控制策略，弥补模型误差。

Result: 仿真结果表明，该框架在八字形轨迹下能够实现平稳稳定的惯性漂移穿越急弯。

Conclusion: 所提框架有效提升了自动赛车惯性漂移控制的稳定性与路径追踪精度，具备实际应用潜力。

Abstract: Inertia drift is a transitional maneuver between two sustained drift stages
in opposite directions, which provides valuable insights for navigating
consecutive sharp corners for autonomous racing.However, this can be a
challenging scenario for the drift controller to handle rapid transitions
between opposing sideslip angles while maintaining accurate path tracking.
Moreover, accurate drift control depends on a high-fidelity vehicle model to
derive drift equilibrium points and predict vehicle states, but this is often
compromised by the strongly coupled longitudinal-lateral drift dynamics and
unpredictable environmental variations. To address these challenges, this paper
proposes a learning-based planning and control framework utilizing Bayesian
optimization (BO), which develops a planning logic to ensure a smooth
transition and minimal velocity loss between inertia and sustained drift
phases. BO is further employed to learn a performance-driven control policy
that mitigates modeling errors for enhanced system performance. Simulation
results on an 8-shape reference path demonstrate that the proposed framework
can achieve smooth and stable inertia drift through sharp corners.

</details>


### [545] [LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving](https://arxiv.org/abs/2507.05754)
*Yuhang Zhang,Jiaqi Liu,Chengkai Xu,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出了LeAD，一种结合端到端模仿学习和大型语言模型增强的城市自动驾驶架构，提升了复杂交通场景下的理解和决策能力，在模拟器中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统难以有效理解复杂交通环境的语义信息和其他参与者的意图，导致决策与人类驾驶员不一致，限制了大规模应用。

Method: 设计了双速率架构，快速的端到端子系统负责实时感知与控制，低频率的大型语言模型融合多模态感知和高清地图，利用链式思维推理优化决策，弥补基础规划器的不足。

Result: 在CARLA模拟器中，LeAD在应对非常规场景表现出色，Leaderboard V1基准得分71分，路线完成率达到93%。

Conclusion: 结合端到端模仿学习与大型语言模型的双速率架构有效提升了自动驾驶系统在复杂和边缘场景下的理解与决策水平，促进了自动驾驶技术的应用推广。

Abstract: A principal barrier to large-scale deployment of urban autonomous driving
systems lies in the prevalence of complex scenarios and edge cases. Existing
systems fail to effectively interpret semantic information within traffic
contexts and discern intentions of other participants, consequently generating
decisions misaligned with skilled drivers' reasoning patterns. We present LeAD,
a dual-rate autonomous driving architecture integrating imitation
learning-based end-to-end (E2E) frameworks with large language model (LLM)
augmentation. The high-frequency E2E subsystem maintains real-time
perception-planning-control cycles, while the low-frequency LLM module enhances
scenario comprehension through multi-modal perception fusion with HD maps and
derives optimal decisions via chain-of-thought (CoT) reasoning when baseline
planners encounter capability limitations. Our experimental evaluation in the
CARLA Simulator demonstrates LeAD's superior handling of unconventional
scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route
completion of 93%.

</details>


### [546] [Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments](https://arxiv.org/abs/2507.05861)
*Woonsang Kang,Joohyung Lee,Seungjun Kim,Jungchan Cho,Yoonseon Oh*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人抓取姿态检测的模块化联邦学习框架，通过分析模型中收敛较慢的模块，针对性地分配通信资源，从而减少通信开销，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取姿态检测依赖大规模多样化数据，带来了数据隐私和集中化问题。联邦学习可保护隐私，但大模型通信开销大，限制资源受限机器人的应用。

Method: 提出模块化联邦学习框架，通过两阶段训练，先全模型训练，再仅训练收敛较慢模块并聚合部分更新，以提高通信效率。

Result: 在GraspNet-1B数据集上，该方法优于FedAvg等基线，通信预算内 achieves更高准确率。真实机器人实验证明在复杂场景下抓取成功率更高。

Conclusion: 该框架有效平衡通信成本与模型性能，实现了分布式环境下高效、稳健的抓取姿态检测模型训练。

Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy,
but its reliance on large, diverse datasets creates significant data privacy
and centralization challenges. Federated Learning (FL) offers a
privacy-preserving solution, but its application to GPD is hindered by the
substantial communication overhead of large models, a key issue for
resource-constrained robots. To address this, we propose a novel module-wise FL
framework that begins by analyzing the learning dynamics of the GPD model's
functional components. This analysis identifies slower-converging modules, to
which our framework then allocates additional communication effort. This is
realized through a two-phase process: a standard full-model training phase is
followed by a communication-efficient phase where only the identified subset of
slower-converging modules is trained and their partial updates are aggregated.
Extensive experiments on the GraspNet-1B dataset demonstrate that our method
outperforms standard FedAvg and other baselines, achieving higher accuracy for
a given communication budget. Furthermore, real-world experiments on a physical
robot validate our approach, showing a superior grasp success rate compared to
baseline methods in cluttered scenes. Our work presents a
communication-efficient framework for training robust, generalized GPD models
in a decentralized manner, effectively improving the trade-off between
communication cost and model performance.

</details>


### [547] [Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data](https://arxiv.org/abs/2507.05884)
*Chang Liu,Zhexiong Xue,Tamas Sziranyi*

Main category: cs.RO

TL;DR: 本文比较评估了多种路径规划算法在基于高分辨率卫星影像和机载LiDAR数据的像素级道路网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 无人驾驶车辆在森林和山区等非结构化环境中导航面临复杂地形和道路条件的挑战，需评估不同路径规划算法的适用性。

Method: 在2D和3D道路地图上分别测试A*、Dijkstra、RRT*、改进蚁群算法（NIACO）等算法，利用卫星和机载LiDAR数据构建加权路网，统一起止点条件进行性能对比。

Result: Dijkstra算法在2D和3D场景中均表现出最佳的稳定性和效率，尤其适用于密集的像素级地理空间道路地图。

Conclusion: Dijkstra算法在静态地形导航中表现可靠，为未来复杂环境下动态路径规划研究奠定基础。

Abstract: Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.

</details>


### [548] [FineGrasp: Towards Robust Grasping for Delicate Objects](https://arxiv.org/abs/2507.05978)
*Yun Du,Mengao Zhao,Tianwei Lin,Yiwei Jin,Chaodong Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: 本文提出了FineGrasp方法，通过网络改进、标签归一化和仿真数据训练显著提升了机器人对小型和精细物体的抓取能力。


<details>
  <summary>Details</summary>
Motivation: 现有抓取方法在处理小物体或精细部件时抓取姿态生成困难，导致管线失败。

Method: FineGrasp方法包括网络结构改进、抓取标签归一化策略和引入新的模拟抓取数据集，采用仿真到真实的混合训练。

Result: 实验显示FineGrasp在抓取小物体方面显著提升性能，验证了该系统在语义抓取中的有效性。

Conclusion: 通过多方面改进，FineGrasp有效解决了细小物体抓取难题，增强了抓取系统的稳定性和准确性。

Abstract: Recent advancements in robotic grasping have led to its integration as a core
module in many manipulation systems. For instance, language-driven semantic
segmentation enables the grasping of any designated object or object part.
However, existing methods often struggle to generate feasible grasp poses for
small objects or delicate components, potentially causing the entire pipeline
to fail. To address this issue, we propose a novel grasping method, FineGrasp,
which introduces improvements in three key aspects. First, we introduce
multiple network modifications to enhance the ability of to handle delicate
regions. Second, we address the issue of label imbalance and propose a refined
graspness label normalization strategy. Third, we introduce a new simulated
grasp dataset and show that mixed sim-to-real training further improves grasp
performance. Experimental results show significant improvements, especially in
grasping small objects, and confirm the effectiveness of our system in semantic
grasping.

</details>


### [549] [AURA-CVC: Autonomous Ultrasound-guided Robotic Assistance for Central Venous Catheterization](https://arxiv.org/abs/2507.05979)
*Deepak Raina,Lidia Al-Zogbi,Brian Teixeira,Vivek Singh,Ankur Kapoor,Thorsten Fleiter,Muyinatu A. Lediju Bell,Vinciya Pandian,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一种端到端的机器人辅助超声引导中心静脉导管置入系统，实现了从扫描初始化到针头插入的全自动化，首次在高仿真模型上证实了其有效性。


<details>
  <summary>Details</summary>
Motivation: 中心静脉导管置入在临床操作中因解剖变异和操作者经验差异导致成功率不高及并发症风险，现有机器人系统尚难实现全自动操作。

Method: 采用深度学习模型从RGB-D图像中识别解剖标志，自动定义扫描区域和路径；机器人运动规划完成血管扫描、分割、重建及定位，并识别最佳插入区；针头引导模块在超声监控和操作者反馈下完成针头插入。

Result: 系统在10个模拟临床场景中均实现首次针头插入成功，血管重建误差平均2.15毫米，针头插入误差小于或接近1毫米。

Conclusion: 本研究首次展示了集成规划、扫描和插入功能的机器人中心静脉导管置入系统，实验结果显示其具备临床应用潜力。

Abstract: Purpose: Central venous catheterization (CVC) is a critical medical procedure
for vascular access, hemodynamic monitoring, and life-saving interventions. Its
success remains challenging due to the need for continuous ultrasound-guided
visualization of a target vessel and approaching needle, which is further
complicated by anatomical variability and operator dependency. Errors in needle
placement can lead to life-threatening complications. While robotic systems
offer a potential solution, achieving full autonomy remains challenging. In
this work, we propose an end-to-end robotic-ultrasound-guided CVC pipeline,
from scan initialization to needle insertion. Methods: We introduce a
deep-learning model to identify clinically relevant anatomical landmarks from a
depth image of the patient's neck, obtained using RGB-D camera, to autonomously
define the scanning region and paths. Then, a robot motion planning framework
is proposed to scan, segment, reconstruct, and localize vessels (veins and
arteries), followed by the identification of the optimal insertion zone.
Finally, a needle guidance module plans the insertion under ultrasound guidance
with operator's feedback. This pipeline was validated on a high-fidelity
commercial phantom across 10 simulated clinical scenarios. Results: The
proposed pipeline achieved 10 out of 10 successful needle placements on the
first attempt. Vessels were reconstructed with a mean error of 2.15
\textit{mm}, and autonomous needle insertion was performed with an error less
than or close to 1 \textit{mm}. Conclusion: To our knowledge, this is the first
robotic CVC system demonstrated on a high-fidelity phantom with integrated
planning, scanning, and insertion. Experimental results show its potential for
clinical translation.

</details>


### [550] [Robust Speech-Workload Estimation for Intelligent Human-Robot Systems](https://arxiv.org/abs/2507.05985)
*Julian Fortune,Julie A. Adams,Jamison Heard*

Main category: cs.RO

TL;DR: 本文提出了一种实时估计言语工作负载的算法，并验证了其在不同个体和人机合作模式中的泛化能力，以提升操作性能。


<details>
  <summary>Details</summary>
Motivation: 在复杂任务环境中，操作者的工作负载（认知、身体、视觉、言语、听觉）变化影响任务表现，实时估计各工作负载成分以智能调整系统交互模式是提升性能的关键。

Method: 开发了一种实时估计言语工作负载的算法，并在多个个体及人机合作场景中测试其准确性和泛化能力。

Result: 算法能准确实时估计言语工作负载，且在不同个体和合作模式中表现出良好泛化能力，证明其用于避免不良负载状态的有效性。

Conclusion: 实时言语工作负载估计是实现自适应人机系统的重要环节，能帮助避免不良负载状态，提升复杂任务的完成效率和准确性。

Abstract: Demanding task environments (e.g., supervising a remotely piloted aircraft)
require performing tasks quickly and accurately; however, periods of low and
high operator workload can decrease task performance. Intelligent modulation of
the system's demands and interaction modality in response to changes in
operator workload state may increase performance by avoiding undesirable
workload states. This system requires real-time estimation of each workload
component (i.e., cognitive, physical, visual, speech, and auditory) to adapt
the correct modality. Existing workload systems estimate multiple workload
components post-hoc, but few estimate speech workload, or function in
real-time. An algorithm to estimate speech workload and mitigate undesirable
workload states in real-time is presented. An analysis of the algorithm's
accuracy is presented, along with the results demonstrating the algorithm's
generalizability across individuals and human-machine teaming paradigms.
Real-time speech workload estimation is a crucial element towards developing
adaptive human-machine systems.

</details>


### [551] [SCCRUB: Surface Cleaning Compliant Robot Utilizing Bristles](https://arxiv.org/abs/2507.06053)
*Jakub F. Kowalewski,Keeyon Hajjafar,Alyssa Ugent,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 本文展示了一种软体机器人手臂通过扭矩和压力有效去除附着污渍的研究，成功完成传统软体机器人难以处理的擦洗任务。


<details>
  <summary>Details</summary>
Motivation: 擦洗表面是高强度且耗时的任务，刚性机器人虽能施加足够力，但因安全限制通常受限于封闭环境；软体机器人安全性高适应性强，但难以传递持续扭矩和侧向力完成擦洗。

Method: 通过训练神经网络学习软体机器人手臂的逆运动学与弹性，实现开环的力和位置控制。

Result: 机器人成功去除盘子上烧焦残渣和马桶座上的果酱污染，平均清洁率达99.7%。

Conclusion: 具备持续扭矩输出能力的软体机器人能够安全高效地完成复杂的擦洗任务，拓展了软体机器人的应用范围。

Abstract: Scrubbing surfaces is a physically demanding and time-intensive task.
Removing adhered contamination requires substantial friction generated through
pressure and torque or high lateral forces. Rigid robotic manipulators, while
capable of exerting these forces, are usually confined to structured
environments isolated from humans due to safety risks. In contrast, soft robot
arms can safely work around humans and adapt to environmental uncertainty, but
typically struggle to transmit the continuous torques or lateral forces
necessary for scrubbing. Here, we demonstrate a soft robotic arm scrubbing
adhered residues using torque and pressure, a task traditionally challenging
for soft robots. We train a neural network to learn the arm's inverse
kinematics and elasticity, which enables open-loop force and position control.
Using this learned model, the robot successfully scrubbed burnt food residue
from a plate and sticky fruit preserve from a toilet seat, removing an average
of 99.7% of contamination. This work demonstrates how soft robots, capable of
exerting continuous torque, can effectively and safely scrub challenging
contamination from surfaces.

</details>


### [552] [Learning-Augmented Model-Based Multi-Robot Planning for Time-Critical Search and Inspection Under Uncertainty](https://arxiv.org/abs/2507.06129)
*Abhish Khanal,Joseph Prince Mathew,Cameron Nowzari,Gregory J. Stein*

Main category: cs.RO

TL;DR: 本文提出了一个基于图神经网络和多机器人规划的时间关键多机器人搜索框架，用于在灾难响应中优先检查可能紧急的地点，有效提升了多机器人协作和搜索效率。


<details>
  <summary>Details</summary>
Motivation: 灾难响应中需要快速识别紧急区域，但派遣机器人全覆盖效率低且不可行，需有效协调多机器人以优先检查重点区域并减少旅行时间。

Method: 利用图神经网络从噪声传感器数据估计紧急地点概率，然后指导基于模型的多机器人规划器制定成本效益高的搜索计划。

Result: 模拟实验显示，该规划器相比非学习和其他学习基线，在1、3和5机器人设置下分别提升了16.3%、26.7%和26.2%的性能，且在四旋翼无人机平台上实验证实了该方法的有效性。

Conclusion: 本文方法有效提升了多机器人在不确定环境下的时间关键搜索与检查能力，具有较强的实际应用潜力。

Abstract: In disaster response or surveillance operations, quickly identifying areas
needing urgent attention is critical, but deploying response teams to every
location is inefficient or often impossible. Effective performance in this
domain requires coordinating a multi-robot inspection team to prioritize
inspecting locations more likely to need immediate response, while also
minimizing travel time. This is particularly challenging because robots must
directly observe the locations to determine which ones require additional
attention. This work introduces a multi-robot planning framework for
coordinated time-critical multi-robot search under uncertainty. Our approach
uses a graph neural network to estimate the likelihood of PoIs needing
attention from noisy sensor data and then uses those predictions to guide a
multi-robot model-based planner to determine the cost-effective plan. Simulated
experiments demonstrate that our planner improves performance at least by
16.3\%, 26.7\%, and 26.2\% for 1, 3, and 5 robots, respectively, compared to
non-learned and learned baselines. We also validate our approach on real-world
platforms using quad-copters.

</details>


### [553] [Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling](https://arxiv.org/abs/2507.06149)
*Charles Champagne Cossette,Taylor Scott Clawson,Andrew Feit*

Main category: cs.RO

TL;DR: 提出了一种基于适应性sigma点采样的动态物体碰撞概率估计算法，计算快速，误差较低。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了碰撞概率的时间相关性，导致碰撞概率的高估。

Method: 采用适应性sigma点采样方案，利用带有高斯分布的轨迹位姿序列，快速估计碰撞概率。

Result: 在Intel Xeon Gold 6226R处理器上，算法中位绝对误差为3.5%，中位运行时间为0.21毫秒。

Conclusion: 方法准确且高效，能显式考虑时间相关性，在实际自动驾驶场景中表现良好。

Abstract: A novel algorithm is presented for the estimation of collision probabilities
between dynamic objects with uncertain trajectories, where the trajectories are
given as a sequence of poses with Gaussian distributions. We propose an
adaptive sigma-point sampling scheme, which ultimately produces a fast, simple
algorithm capable of estimating the collision probability with a median error
of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold
6226R Processor. Importantly, the algorithm explicitly accounts for the
collision probability's temporal dependence, which is often neglected in prior
work and otherwise leads to an overestimation of the collision probability.
Finally, the method is tested on a diverse set of relevant real-world
scenarios, consisting of 400 6-second snippets of autonomous vehicle logs,
where the accuracy and latency is rigorously evaluated.

</details>


### [554] [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
*William Li,Lei Hamilton,Kaise Al-natour,Sanjeev Mohindra*

Main category: cs.RO

TL;DR: 该论文评估了大型语言模型在解决具身机器人任务中的表现，使用Meta PARTNER基准测试。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在具身机器人任务中的有效性，推动机器人合作任务的智能化发展。

Method: 利用Meta PARTNER基准，构建随机厨房场景，由两个机器人协作解决任务，并测试不同前沿模型的表现。

Result: 实验结果显示，具备推理能力的模型（如OpenAI o3-mini）在各种环境配置下均优于非推理模型（如OpenAI GPT-4o和Llama 3）。

Conclusion: 推理模型在具身机器人任务中表现更优，Meta PARTNER提供了有潜力的研究方向，促进具身机器人智能化进展。

Abstract: This paper focuses on evaluating the effectiveness of Large Language Models
at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR
provides simplified environments and robotic interactions within randomized
indoor kitchen scenes. Each randomized kitchen scene is given a task where two
robotic agents cooperatively work together to solve the task. We evaluated
multiple frontier models on Meta PARTNER environments. Our results indicate
that reasoning models like OpenAI o3-mini outperform non-reasoning models like
OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied
environments. o3-mini displayed outperform across centralized, decentralized,
full observability, and partial observability configurations. This provides a
promising avenue of research for embodied robotic development.

</details>


### [555] [Learning Agile Tensile Perching for Aerial Robots from Demonstrations](https://arxiv.org/abs/2507.06172)
*Kangle Yuan,Atar Babgei,Luca Romanello,Hai-Nguyen Nguyen,Ronald Clark,Mirko Kovac,Sophie F. Armanini,Basaran Bahadir Kocer*

Main category: cs.RO

TL;DR: 本文提出了一种利用强化学习（SACfD算法）的系绳张力悬停轨迹框架，解决了系绳张力悬停中的动力学建模和精准控制问题，实现了飞行机器人在多样结构上的稳定挂靠。


<details>
  <summary>Details</summary>
Motivation: 飞行机器人通过悬停节能，但系绳张力悬停面临动力学建模挑战，如系绳松紧与动量传递问题，且需实现对特定系绳段的精准控制确保安全挂靠。

Method: 采用基于演示的Soft Actor-Critic强化学习算法（SACfD），结合最佳与次优示范，提升训练效率和控制精度，实现飞行机器人对系绳特定段的精确目标定位、环绕和锚定。

Result: 通过大量仿真和真实实验验证框架的有效性，展示了该方法在系绳张力悬停轨迹生成上的敏捷性和可靠性。

Conclusion: 所提强化学习轨迹框架有效解决了系绳张力悬停的复杂动力学与控制问题，实现了飞行机器人在不同结构上的高效稳定挂靠。

Abstract: Perching on structures such as trees, beams, and ledges is essential for
extending the endurance of aerial robots by enabling energy conservation in
standby or observation modes. A tethered tensile perching mechanism offers a
simple, adaptable solution that can be retrofitted to existing robots and
accommodates a variety of structure sizes and shapes. However, tethered tensile
perching introduces significant modelling challenges which require precise
management of aerial robot dynamics, including the cases of tether slack &
tension, and momentum transfer. Achieving smooth wrapping and secure anchoring
by targeting a specific tether segment adds further complexity. In this work,
we present a novel trajectory framework for tethered tensile perching,
utilizing reinforcement learning (RL) through the Soft Actor-Critic from
Demonstrations (SACfD) algorithm. By incorporating both optimal and suboptimal
demonstrations, our approach enhances training efficiency and responsiveness,
achieving precise control over position and velocity. This framework enables
the aerial robot to accurately target specific tether segments, facilitating
reliable wrapping and secure anchoring. We validate our framework through
extensive simulation and real-world experiments, and demonstrate effectiveness
in achieving agile and reliable trajectory generation for tensile perching.

</details>


### [556] [Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model](https://arxiv.org/abs/2507.06174)
*Koki Yamane,Yunhan Li,Masashi Konosu,Koki Inami,Junji Oaki,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本文提出了利用低成本无力传感机械臂和四通道双边控制实现快速带力反馈的远程操作方法，提高了模仿学习的数据采集质量。


<details>
  <summary>Details</summary>
Motivation: 现有低成本机械臂远程操作大多只传递目标位置，缺乏力反馈，难以应对快速或接触丰富的操作需求。

Method: 基于精确辨识的机械臂动力学，结合非线性补偿、速度和外力估计以及对应惯性变化的可变增益，采用四通道双边控制实现力反馈远程操作。

Result: 实验表明使用四通道双边控制采集的数据结合力信息，能够显著提升模仿学习策略的性能。

Conclusion: 该系统在低成本硬件上实现了高保真远程操作和高质量示范数据采集，具备实际应用价值。

Abstract: In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.

</details>


### [557] [Is Diversity All You Need for Scalable Robotic Manipulation?](https://arxiv.org/abs/2507.06219)
*Modi Shi,Li Chen,Jin Chen,Yuxiang Lu,Chiming Liu,Guanghui Ren,Ping Luo,Di Huang,Maoqing Yao,Hongyang Li*

Main category: cs.RO

TL;DR: 这篇论文探讨了机器人操作中数据多样性对学习效果的影响，针对任务、机器人实体和演示者三个维度进行了深入分析。


<details>
  <summary>Details</summary>
Motivation: 虽然自然语言处理和计算机视觉领域在数据规模上取得了显著成功，但机器人操作中关于如何有效扩展数据规模的原则尚不明确，因此作者希望理解数据多样性在机器人学习中的作用。

Method: 通过在多个机器人平台上的大量实验，作者评估了任务多样性、机器人实体多样性和专家多样性对模型迁移和学习性能的影响，特别提出了一种分布去偏方法来解决速度模态多样性带来的学习困扰。

Result: 实验发现任务多样性对迁移效果更为关键，多实体预训练可选，高质量单实体预训练更适合微调，专家多样性中的速度多模态是关键问题，采用分布去偏方法后新模型GO-1-Pro性能提升15%，相当于使用2.5倍的预训练数据。

Conclusion: 本文揭示了机器人操作中数据多样性的关键影响因素和相互关系，提出有效的去偏方法，指导如何合理扩展机器人操作数据集以提升模型性能。

Abstract: Data scaling has driven remarkable success in foundation models for Natural
Language Processing (NLP) and Computer Vision (CV), yet the principles of
effective data scaling in robotic manipulation remain insufficiently
understood. In this work, we investigate the nuanced role of data diversity in
robot learning by examining three critical dimensions-task (what to do),
embodiment (which robot to use), and expert (who demonstrates)-challenging the
conventional intuition of "more diverse is better". Throughout extensive
experiments on various robot platforms, we reveal that (1) task diversity
proves more critical than per-task demonstration quantity, benefiting transfer
from diverse pre-training tasks to novel downstream scenarios; (2)
multi-embodiment pre-training data is optional for cross-embodiment
transfer-models trained on high-quality single-embodiment data can efficiently
transfer to different platforms, showing more desirable scaling property during
fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,
arising from individual operational preferences and stochastic variations in
human demonstrations, can be confounding to policy learning, with velocity
multimodality emerging as a key contributing factor. Based on this insight, we
propose a distribution debiasing method to mitigate velocity ambiguity, the
yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to
using 2.5 times pre-training data. Collectively, these findings provide new
perspectives and offer practical guidance on how to scale robotic manipulation
datasets effectively.

</details>


### [558] [EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://arxiv.org/abs/2507.06224)
*Yixiang Chen,Peiyan Li,Yan Huang,Jiabing Yang,Kehan Chen,Liang Wang*

Main category: cs.RO

TL;DR: 本文提出了EC-Flow，一种基于人体运动学的流预测框架，实现无动作标签的视频学习，显著提升了复杂操控任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的语言指导机器人操控系统依赖低层次动作标签数据，且对象中心的流预测方法仅限于刚性物体，难以处理变形物体、遮挡及非物体位移任务。

Method: 引入基于人体运动学的Embodiment-Centric Flow，通过嵌入运动学信息学习操控流；设计目标对齐模块，联合优化动作一致性与目标图像预测；仅需机器人URDF文件转换为机器人动作。

Result: 在模拟与真实任务中，EC-Flow在遮挡物体处理提升62%，变形物体操作提升45%，非物体位移任务提升80%，优于现有对象中心流方法。

Conclusion: 通过融合人体运动学信息，EC-Flow显著拓展了语言指导机器人操控的应用范围，提高了复杂操控场景的泛化能力和表现。

Abstract: Current language-guided robotic manipulation systems often require low-level
action-labeled datasets for imitation learning. While object-centric flow
prediction methods mitigate this issue, they remain limited to scenarios
involving rigid objects with clear displacement and minimal occlusion. In this
work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly
learns manipulation from action-unlabeled videos by predicting
embodiment-centric flow. Our key insight is that incorporating the embodiment's
inherent kinematics significantly enhances generalization to versatile
manipulation scenarios, including deformable object handling, occlusions, and
non-object-displacement tasks. To connect the EC-Flow with language
instructions and object interactions, we further introduce a goal-alignment
module by jointly optimizing movement consistency and goal-image prediction.
Moreover, translating EC-Flow to executable robot actions only requires a
standard robot URDF (Unified Robot Description Format) file to specify
kinematic constraints across joints, which makes it easy to use in practice. We
validate EC-Flow on both simulation (Meta-World) and real-world tasks,
demonstrating its state-of-the-art performance in occluded object handling (62%
improvement), deformable object manipulation (45% improvement), and
non-object-displacement tasks (80% improvement) than prior state-of-the-art
object-centric flow methods. For more information, see our project website at
https://ec-flow1.github.io .

</details>
