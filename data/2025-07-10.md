<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.AI](#cs.AI) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP感知损失和课程对比正则化的水下图像增强方法，提升了图像的感知质量和内容恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强方法忽略了人类视觉感知，导致增强图像感知质量下降或内容恢复不充分。

Method: 引入CLIP视觉语义特征提取，学习适当的prompt对作为感知模型，将其作为损失模块融入增强网络，并结合课程对比正则化，以更好约束增强图像在感知空间中的表现。

Result: 所提方法在视觉质量和泛化能力上超过了现有先进方法。

Conclusion: 通过结合CLIP感知模型和课程对比正则化，有效提升了水下图像增强的感知质量和恢复效果，避免了过度或不足增强。

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [2] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 本文提出了SPARC框架，实现不同AI模型之间统一的高层语义概念对齐，大幅提升跨模型解释性。


<details>
  <summary>Details</summary>
Motivation: 现有解释性方法如稀疏自动编码器各自为模型生成独立的语义空间，导致概念空间不兼容，限制了跨模型的理解和比较。

Method: SPARC通过引入全局TopK稀疏机制和交叉重建损失，强制不同模型在统一稀疏潜在空间中激活相同的语义维度，从而实现多模态多架构间的概念对齐。

Result: 在Open Images数据集上，SPARC的概念对齐Jaccard相似度达到0.80，较先前方法提升三倍，实现了跨模型语义维度的共享和一致性。

Conclusion: SPARC实现了不同模型间的统一语义表达，支持模型间直接比较和多模态检索等应用，增强了AI系统的可解释性和通用性。

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [3] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: 本文提出BayesSDF，一种用于神经隐式有符号距离函数(SDF)模型的不确定性量化的新方法，通过拉普拉斯近似实现高效且几何感知的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 神经隐式3D表示中不确定性量化存在计算效率低、扩展性差和几何不一致等问题，现有方法缺乏直接几何积分，导致不确定性图校准不良。

Method: 提出BayesSDF框架，利用拉普拉斯近似和基于Hessian的度量，实现对局部曲面不稳定性的量化，提供计算高效且具有表面感知的不确定性估计。

Result: 实验表明，BayesSDF的不确定性预测与重建差的几何结构高度相关，且在合成和真实数据集上的校准和几何一致性方面优于现有方法。

Conclusion: BayesSDF为不确定性感知的3D场景重建、模拟和机器人决策提供了坚实基础，有效提升了基于SDF的隐式3D表示方法的不确定性量化水平。

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [4] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出了LIRA框架，通过结合语义增强特征提取和局部视觉耦合方法，提升了大规模多模态模型在图像分割和理解中的准确性，显著减少了错误分割和虚假理解现象。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多模态模型在图像分割和理解方面存在分割不准确及理解产生幻觉的问题，主要由于视觉理解能力不足及缺乏细粒度感知。

Method: 提出LIRA框架，包括语义增强特征提取器（SEFE）用于融合语义和像素级特征以提升对象属性推断，及交错局部视觉耦合（ILVC）方法通过基于分割掩码提取局部特征并自回归生成局部描述，提供细粒度监督减少虚假理解。同时引入AttrEval数据集量化分割精度与语义推断能力的关系。

Result: 实验表明，LIRA在图像分割和理解任务上均达到最先进水平，显著提升了分割精度和理解准确度。

Conclusion: 通过结合语义增强和细粒度视觉耦合机制，LIRA有效缓解了多模态模型中的分割不准和理解幻觉问题，提升了模型的应用潜力。

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [5] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: 本文综述了离线手写文本识别领域中数据增强和生成技术，涵盖传统方法及基于深度学习的新进展，如GAN、扩散模型和变换器方法，旨在提升识别系统的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线手写文本识别系统在历史文档数字化、自动表单处理等应用中关键，但受限于标注训练数据的稀缺，尤其是低资源语言及复杂文字系统，亟需有效的数据增强与生成技术。

Method: 通过严格的PRISMA方法学筛选文献，从1302篇研究中精选848篇，系统评估传统增强手段及深度学习技术（GAN、扩散模型、变换器等），分析数据集、评价指标和最新方法，探讨生成真实多样手写样本的挑战。

Result: 全面总结现有手写数据增强与生成方法的效果与局限，揭示当前技术在保持笔迹真实性和解决数据匮乏方面的不足，识别关键研究空白。

Conclusion: 本综述明确了离线手写文本生成领域的研究进展与挑战，提出未来研究方向，为提升多语言多风格手写识别系统的性能提供理论参考和实践指导。

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [6] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: 本文提出了中央集中复制粘贴数据增强（CCPDA）方法，以提升野火多分类分割模型中特别是火灾类别的分割效果。


<details>
  <summary>Details</summary>
Motivation: 野火图像数据稀缺且标注成本高，传统数据增强方法效果有限，亟需一种有效提升火灾类别分割性能的数据增强策略。

Method: CCPDA方法包括识别源图像中的火灾簇、对火焰核心区域进行集中处理，以及将处理后的火焰簇粘贴到目标图像中，增加数据多样性且保持火灾特征。

Result: 通过加权多目标优化和数值分析，CCPDA在提升火灾类别分割性能方面优于其它增强方法，在有限小样本标注数据上表现出明显优势。

Conclusion: CCPDA方法有效缓解了小规模手工标注训练数据的不足，显著提升了野火深度学习分割模型中特别是火灾类别的分割效果，具备实际应用价值。

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [7] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: 本文提出了AR2方法，通过对齐干净图像与损坏图像的类激活图(CAM)来提升预训练卷积神经网络在输入干扰下的注意力一致性，从而增强模型对噪声、模糊、天气和数字失真等常见腐败的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在面对常见图像腐败时性能显著下降，限制了其在实际应用中的可靠性。

Method: AR2利用迭代修复策略，通过CAM引导的注意力对齐与标准微调交替进行，无需改变网络结构。

Result: 在CIFAR-10-C、CIFAR-100-C和ImageNet-C等标准腐败测试集上，AR2表现优于现有最先进方法，在保持干净数据准确率的同时显著提升腐败鲁棒性。

Conclusion: AR2为提升模型在多样腐败环境下的可靠性提供了一种稳健且可扩展的有效方法。

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [8] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: 本文提出了首个专门针对水下多鱼追踪的综合数据集MFT25，并设计了基于无迹卡尔曼滤波的SU-T追踪框架，实现鱼类追踪的新突破。


<details>
  <summary>Details</summary>
Motivation: 尽管多目标追踪技术在陆地应用取得进展，水下追踪尤其是鱼类追踪场景仍未充分研究，而这对海洋生态与水产养殖极为重要。

Method: 构建包含15段多样化视频序列及超过40万标注框的MFT25数据集；设计基于无迹卡尔曼滤波的尺度感知追踪器SU-T，结合鱼类特有形态的FishIoU匹配策略，优化鱼类非线性运动追踪。

Result: SU-T在MFT25数据集上达到34.1 HOTA和44.6 IDF1，领先现有方法并揭示鱼类追踪与陆地目标追踪间的本质差异。

Conclusion: MFT25和SU-T为水下多鱼追踪研究奠定坚实基础，对海洋生物学、水产养殖监测及生态保护具有重要应用价值。数据集和代码已公开。

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [9] [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116)
*Juyi Lin,Amir Taherin,Arash Akbari,Arman Akbari,Lei Lu,Guangyu Chen,Taskin Padir,Xiaomeng Yang,Weiwei Chen,Yiqian Li,Xue Lin,David Kaeli,Pu Zhao,Yanzhi Wang*

Main category: cs.CV

TL;DR: 提出了一种高效且通用的VLA模型优化框架VOTE，通过无分词微调和集成投票提升动作预测速度和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉语言动作模型在面对新物体和环境时泛化能力有限，且现有方法为提升泛化引入额外视觉组件导致计算负担重、效率低。

Method: VOTE采用无分词的微调方式实现并行准确动作预测，减少计算开销，结合集成投票策略提升模型性能和泛化能力。

Result: 实验显示该方法在保持性能的同时，推理速度提升35倍，吞吐率达145 Hz。

Conclusion: VOTE有效提升了VLA模型的推理效率和泛化性能，为实用化提供了新的方案，相关代码将开源。

Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35$\times$ faster inference and 145 Hz
throughput. All the details and codes will be open-sourced.

</details>


### [10] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: 该论文提出了SImpHAR框架，通过仿真生成逼真生物阻抗信号和两阶段训练策略，显著提升了基于生物阻抗的无人机活动识别的准确率和宏观F1值。


<details>
  <summary>Details</summary>
Motivation: 当前基于穿戴式传感器的人体活动识别在医疗、健身和人机交互中应用广泛，而生物阻抗传感虽具细粒度动作捕捉优势但因标注数据稀缺而难以充分利用。

Method: 提出一个基于3D人体网格的仿真管线，结合最短路径估计、软体物理和文本到运动生成，实现数字孪生数据增强；设计两阶段解耦训练策略，支持更广泛活动识别且无需标注对齐的合成数据。

Result: 在自采集ImpAct数据集及两个公开基准上，SImpHAR相较于最先进方法，准确率和宏观F1分别提升最高22.3%和21.8%。

Conclusion: 仿真驱动的数据增强与模块化训练策略为基于生物阻抗的活动识别提供了有效提升路径，展示了其应用潜力。

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [11] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: 本文提出了一种分层多阶段变换器架构PCL-Former，用于时序动作定位任务，通过不同模块分别完成候选段检测、动作分类和精确定位，实现了多数据集上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 受变换器和多阶段架构在视频识别和目标检测领域成功的启发，探索其在时序动作定位中充分利用时空信息的潜力。

Method: 设计了三个专门的变换器模块对应不同子任务：Proposal-Former负责候选动作段识别，Classification-Former执行动作分类，Localization-Former精确预测动作的起止时间。

Result: 在THUMOS-14、ActivityNet-1.3和HACS三个数据集上，PCL-Former分别提升了2.8%、1.2%和4.8%的性能，显著优于现有最先进方法。

Conclusion: 针对时序动作定位任务，基于多阶段变换器架构的PCL-Former有效利用时空特征，实现了细粒度动作定位，且提升了多个基准测试的性能。

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [12] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: 提出了THOR，一种结合热感应的自适应空间时间RGB帧采样方法，显著减少了视频数据处理量且保持高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴摄像头用于手部行为监测时，RGB图像的连续处理消耗大、隐私风险高，需寻找有效节能且保护隐私的处理方案。

Method: 利用低分辨率热成像捕捉手部活动变化，实时调整RGB帧采样率；通过热成像定位手-物体交互区域，仅裁剪必要部分进行识别。

Result: 通过14人野外实测及大规模Ego4D数据验证，仅用3%原始RGB数据实现与全视频相当的95% F1分数，准确捕捉所有活动段。

Conclusion: 该方法为长期实时监测手部活动及健康风险行为的可穿戴摄像头应用提供了更加实用、高效且隐私友好的技术路径。

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [13] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: 本文提出了一种事件自编码器架构，用于高效压缩和重构事件摄像头数据，提升实时目标检测性能，同时大幅减少计算量。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的视觉系统在动态环境中存在运动模糊、高延迟和数据冗余问题，事件摄像头虽优势明显但检测对象困难，本文旨在解决这些难题。

Method: 设计了基于卷积编码的事件自编码器，结合自适应阈值选择和轻量级分类器，保证空间和时间特征的保留及识别准确性，同时降低计算复杂度。

Result: 在Smart Event Face Dataset( SEFD )上准确率与YOLO-v4相当，但参数减少了35.5倍，嵌入式设备上帧率达到8至44.8 FPS，FPS提升高达87.84倍。

Conclusion: 该方法显著提升了事件视觉的实时性能和效率，非常适合低功耗、高速的边缘计算场景。

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [14] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出了一种名为Video-RTS的视频推理方法，通过结合高效的强化学习和视频适配的推理时扩展策略，提高数据效率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习和大语言模型的视频推理方法依赖大量标注数据和长链思考注释，数据收集和微调成本高，难以扩展。

Method: 基于对强化学习样本数据规模的观察，跳过了资源密集的监督微调步骤，采用纯强化学习训练并使用基于输出的奖励，无需额外标注；同时引入稀疏到稠密的视频推理时扩展策略，通过输出一致性逐步添加帧以提高推理效率。

Result: 在多个视频推理基准测试中，Video-RTS在仅使用3.6%训练样本的情况下，准确率平均提升2.4%；例如在Video-Holmes上提升4.2%，在MMVU上提升2.6%。

Conclusion: 纯强化学习训练结合自适应推理时扩展策略有效提升视频推理性能，减少了数据和计算资源需求，具备良好的扩展性。

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [15] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: 本文提出了一种面向位姿估计的新型预训练策略Mask6D，通过结合2D-3D对应关系图和可见掩膜图，提高了在复杂遮挡场景下单目RGB图像的6D物体位姿估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D特征提取的位姿估计网络在复杂遮挡条件下难以提取区分性强且具有位姿感知的特征，导致性能下降。

Method: 引入Mask6D预训练策略，将位姿感知的2D-3D对应关系图和可见掩膜图与RGB图像结合，进行基于重建的模型预训练；设计关注物体的预训练损失函数以减少背景干扰；然后使用传统的位姿训练方法微调网络实现精确位姿预测。

Result: 广泛实验表明，该方法在鲁棒性和准确性上超过了以往端到端的位姿估计方法。

Conclusion: Mask6D策略有效提升了在复杂遮挡环境下基于单目RGB图像的6D物体位姿估计性能，对背景干扰具有较强抑制作用。

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [16] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双向协作框架BC-HOI，用于开放词汇人机交互检测，通过注意力偏置引导和大语言模型监督提升细粒度实例交互特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模视觉语言模型的方法生成的视觉特征过于整体粗糙，与检测任务中需要的细粒度特征不符，因此需要更精细的特征生成方法。

Method: 设计了双向协作框架BC-HOI，包括注意力偏置引导组件（ABG）和大语言模型监督引导组件（LSG），通过ABG引导VLM生成细粒度实例交互特征，并利用LSG提供细粒度的标记级监督，提升模型性能。

Result: 在两个主流基准数据集HICO-DET和V-COCO上，方法在开放词汇和封闭设置下均表现出优越性能。

Conclusion: BC-HOI框架有效解决了视觉语言模型特征粗糙的问题，通过双向协作显著提升了开放词汇人机交互检测的性能，具有良好的应用前景。

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [17] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 本文综述了基于视觉的交通场景分析，通过构建综合分类体系，分析了关键交通实体、视觉驱动任务和数据集，促进道路安全研究。


<details>
  <summary>Details</summary>
Motivation: 随着视觉传感器和计算机视觉算法的进步，需要系统分类交通场景中的关键元素，并整合现有任务和数据集以促进道路安全应用。

Method: 本文提出了一个包含十个大类和二十个子类的统一分类体系，涵盖异常和正常但关键的交通实体，系统分析了35个视觉任务和73个数据集。

Result: 文章揭示了各个基准的优缺点，推动标准统一和资源优化，提供了跨域的综合分析视角和可视化总结表格。

Conclusion: 该综述指出当前研究的不足，强调多角度潜在影响和未来解决方案，为研究者提供全面视角、资源选择指导和关键研究空白的识别。

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [18] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: 本文提出了FIFA框架，用于评价视频多模态大语言模型生成内容的真实性，克服了现有方法局限，并通过Post-Correction机制修正幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 当前视频多模态大语言模型在文本生成中存在幻觉问题，且现有评估方法局限于单一任务且无法评估开放式回答的真实性。

Method: 提出FIFA框架，通过提取描述性事实、构建时空语义依赖图、利用VideoQA模型进行验证，并引入Post-Correction工具进行内容修正。

Result: 大量实验表明FIFA比现有评价方法更符合人类判断，Post-Correction有效提升了文本和视频生成的事实一致性。

Conclusion: FIFA为视频多模态语言模型的真实性评估提供了一种统一且有效的方案，结合Post-Correction机制，有助于减少生成内容的幻觉问题。

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [19] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种针对文本到图像扩散模型（T2I DMs）概念遗忘的新方法KSCU，通过关键步骤的细粒度微调，实现有效遗忘并保留生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的概念遗忘方法难以在遗忘效果与保持模型生成能力间取得平衡，存在安全风险。

Method: KSCU方法利用扩散模型生成过程中的逐步采样特点，重点微调对最终生成影响最大的关键步骤，减少参数更新量。

Result: 大规模基准测试表明KSCU有效防止生成不良图像，同时更好地保留模型的生成能力。

Conclusion: KSCU通过关键步骤微调，实现了概念遗忘与生成能力的良好权衡，为模型安全应用提供了新思路。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [20] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: 本文提出了一种将英语语音转换为流畅、逼真的3D手语动画的完整系统，整合了语音识别、文本翻译和三维动画生成。


<details>
  <summary>Details</summary>
Motivation: 针对现有研究多聚焦手语转文本，忽视了从口语到手语动画的复杂多步骤转换，旨在帮助聋哑人更便捷交流。

Method: 首先使用Whisper将语音转文本，再用MarianMT模型翻译成简化的美式手语词汇（ASL gloss），结合Word2Vec和FastText词嵌入提升翻译准确度，最后通过基于3D关键点的动画系统生成连续自然的手语动画。建立了Sign3D-WLASL和BookGlossCorpus-CG数据集支撑翻译与动画训练。

Result: 系统在翻译阶段取得BLEU分数高达0.8923，动画生成流畅自然，能够实现从英语语音到逼真手语动画的完整转换。

Conclusion: 该管道方法综合利用音频、文本和运动数据，填补了自动将英语口语转换为3D手语动画的研究空白，为聋哑人提供高质量辅助沟通工具。

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [21] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: 提出ILNet方法，通过逆学习注意力和动态锚点选择，提升多智能体轨迹预测的准确性和交互表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态和前馈模型的多智能体轨迹预测缺乏显式的时空交互协调，难以捕捉复杂行为意图，且固定锚点选择策略难以适应不同未来环境。

Method: 引入逆学习注意力机制动态编码邻近时刻的交互意图，实现复杂时空交互协调；采用可学习的动态锚点选择模块并行提取多个轨迹关键点作为锚点，提升模型灵活性和表达能力。

Result: ILNet在INTERACTION和Argoverse数据集上达成了最新的性能指标，尤其在复杂交互场景下，表现出更高准确率和更丰富多模态轨迹分布，同时模型参数更少。

Conclusion: ILNet通过模拟人类驾驶行为中的动态决策调整，实现了更有效的多智能体轨迹预测，为复杂交互场景提供了强有力的解决方案。

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [22] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种适用于相机陷阱捕获动物的模型无关主动学习方法，通过整合样本的不确定性和多样性，有效选择训练数据，实现用30%的数据达到全量训练的性能。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱捕获的野生动物数据量巨大，标注和训练成本高昂，需要一种有效的数据选择方法以减少标注工作量。

Method: 结合目标级和图像级的不确定性与多样性指标，采样最具代表性的训练数据，无需完全访问模型参数，适用于各种检测模型。

Result: 在基准动物数据集上实验表明，仅用30%的训练数据，所训练的动物检测模型性能达到或超过使用全部数据的效果。

Conclusion: 所提模型无关的主动学习方法在野生动物检测任务中有效提升了数据利用效率，降低了标注需求，促进自动化野生动物监测的发展。

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [23] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态关键点学习框架MK-Pose，通过融合RGB图像、点云和文本描述，实现类别级物体姿态估计，提升遮挡场景和跨类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB图像或点云的方法在处理物体遮挡和不同实例及类别的泛化方面存在困难。

Method: 提出MK-Pose框架，包含自监督关键点检测模块，利用注意力查询生成、软热图匹配和图关系建模；设计图增强特征融合模块，结合局部几何信息与全局上下文。

Result: 在CAMERA25和REAL275数据集上表现优异，且在HouseCat6D上展示跨数据集能力，优于现有最先进方法。

Conclusion: MK-Pose实现了无形状先验下的优越姿态估计性能，具备较强的鲁棒性和泛化能力。

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [24] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: 提出了一种名为Token Bottleneck (ToBo)的自监督学习方法，通过压缩场景信息为瓶颈token并利用少量提示patch预测后续场景，从而学习动态场景的时序视觉表示。


<details>
  <summary>Details</summary>
Motivation: 动态场景的紧凑且具有时间意识的视觉表示对于序列场景理解任务（如视觉跟踪和机器人操作）至关重要。现有方法尚未充分捕捉场景间的时序动态。

Method: 设计了一个简单直观的自监督学习流程，将参考场景压缩为瓶颈token；通过瓶颈token加上少量目标patch作为提示，预测后续场景，实现时序依赖的编码。

Result: 在视频标签传播、机器人操控等多种序列任务和仿真环境中，ToBo表现优于基线方法。将预训练模型应用于实际机器人上验证了其鲁棒性和有效性，同时证明了方法对不同模型规模的良好适应性。

Conclusion: ToBo有效地捕捉动态场景的时序信息，通过紧凑瓶颈token的方式提升了视觉表示的质量，适用于多种序列任务并具备良好的实用性和扩展性。

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [25] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: 提出了StixelNExT++方法，通过3D Stixels聚类精细分割物体，实现了场景信息的高效压缩和实时处理。


<details>
  <summary>Details</summary>
Motivation: 在单目感知系统中，需要一种高效、精确的场景表示方法以提升自动驾驶的感知能力。

Method: 基于已有的Stixel表示，推断3D Stixels，利用小的3D Stixel单元聚类实现更精细的物体分割，并设计轻量级神经网络，结合激光雷达的数据进行自动训练。

Result: 在Waymo数据集上，方法在30米范围内表现优异，计算时间低至每帧10毫秒，展现了实时性能和高效场景压缩能力。

Conclusion: StixelNExT++有效提升了单目系统的场景理解能力，适用于自动驾驶中的集体感知，有较好的实际应用潜力。

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [26] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 该论文提出了Concept-TRAK，一种针对扩散模型生成图像中具体概念的归因方法，提升了训练样本对特定元素（如风格或物体）的贡献识别能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现优异，但现有归因方法只能识别对整幅图像的影响，无法分辨对特定概念（如风格或对象）的贡献，导致版权和模型透明性问题难以解决。

Method: 提出Concept-TRAK，基于影响函数扩展：1）重新设计基于扩散后验采样的训练损失，实现样本特定的鲁棒归因；2）引入概念感知奖励函数，强调语义相关性。

Result: 在AbC基准测试中，Concept-TRAK较之前方法有显著提升。通过多个案例研究验证了其在识别知识产权保护内容、不安全内容、提示语工程和组合学习分析上的应用效果。

Conclusion: 概念级归因方法Concept-TRAK为负责任的生成式AI开发与治理提供了切实可行的洞见，有助于解决版权和模型透明性问题。

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [27] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出了SpatialReasoner框架，结合大语言模型驱动的空间推理和视觉属性增强的分层特征场，实现了基于开放词汇的3D视觉定位，显著提升了复杂环境中基于空间关系语言查询的目标定位效果。


<details>
  <summary>Details</summary>
Motivation: 现有语言场方法难以准确利用语言查询中的空间关系进行实例定位，主要因为对空间关系的推理不足。

Method: 通过微调大语言模型捕捉空间关系并推断目标、锚点及空间关系指令，结合视觉属性（不透明度与颜色）构建视觉属性增强的分层特征场，利用CLIP特征和SAM掩码提取语言和实例特征，并层级查询实现目标3D实例定位。

Result: 实验表明该框架能够无缝集成不同神经表示模型，提升3D视觉定位效果及空间推理能力，优于基线模型。

Conclusion: SpatialReasoner有效增强了3D视觉定位中的空间关系推理能力，是实现开放词汇3D视觉定位的重要进展。

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [28] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于散度的相似性函数（DSF）来联合建模多视图增强数据，显著提升了多视图对比学习的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多捕捉视图之间的两两关系，无法有效建模所有视图的联合结构，影响对多视图信息的充分利用。

Method: 引入DSF，将多视图增强数据集合作为分布来表示，并通过测量分布间的散度来评估相似性，从而捕捉联合结构。本方法避免了对温度参数的依赖。

Result: DSF在kNN分类和线性评估等多任务中均表现出性能提升，同时计算效率优于其它多视图方法。

Conclusion: DSF作为一种新的多视图相似性度量，有效提升了对比学习性能，并具备理论基础与实际应用优势。

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [29] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了Percep360，一种用于自动驾驶的全景图像生成方法，旨在解决全景数据采集复杂且耗时的问题，实现高质量且可控的全景图像生成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要完整的全景数据，但传统采集和标注过程复杂耗时，现有生成模型受限于固定数据分布，难以实现高质量和可控的全景生成。

Method: 提出了局部场景扩散方法（LSDM）和概率提示方法（PPM），前者通过空间连续的扩散过程解决信息损失和数据分布差异，后者动态选择控制信号实现生成的可控性。

Result: 生成的全景图像在无参考质量指标上优于原始拼接图像，且提升了下游BEV分割感知模型的性能。

Conclusion: Percep360实现了高质量且可控的自动驾驶用全景图像生成，有效改善了数据采集难题，生成数据提升了感知模型表现。

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [30] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: 提出了一种新的边缘检测损失函数EBT损失，将像素分为边缘、边界和纹理三类，赋予不同权重，改善了传统WBCE损失忽略边界结构的问题，提高了模型的边缘精度和边界定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有的加权二元交叉熵（WBCE）损失对所有非边缘像素一视同仁，忽视了边缘周围的结构差异，导致预测模糊。

Method: 提出边缘-边界-纹理（EBT）损失，通过三分类像素类别并赋予不同的监督权重，实现更有结构的学习，侧重边缘精度和边界定位。理论证明EBT是WBCE的推广。

Result: 在多个基准测试中，EBT损失在定量和感知效果上均优于传统方法，且对超参数不敏感，便于实际应用。

Conclusion: EBT损失通过差异化像素权重分配提高了边缘检测效果，具备良好的泛化性和实用性，减少了调参需求。

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [31] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST是一种通过时间片段Banzhaf交互实现人体动作生成的新型扩散模型，针对罕见语言提示问题提出细粒度的文本-动作匹配方法，显著提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从稀有语言提示生成动作时匹配粗糙，忽视动作冗余中的语义细节，限制了生成质量。

Method: 引入时间片段Banzhaf交互实现文本与动作片段的精确匹配，去除冗余；通过动作提示模块利用检索到的动作片段保证语义一致性。

Result: 在文本到动作检索和生成任务中，MOST的性能达到最新水平，特别在处理罕见提示时表现优异。

Conclusion: MOST通过细粒度的动作片段匹配与生成策略，有效解决了之前方法中的匹配和冗余问题，显著提升了文本驱动动作生成的质量与准确性。

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [32] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 本文提出了一种适用于点云3D语义分割的自适应边缘对比学习方法，通过考虑点的模糊程度来调整训练目标，提升模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有点采用均等惩罚，忽视了过渡区域中模糊点的特性，这些点由于高不确定性导致标签不可靠，硬约束难以得到最优模型。

Method: 设计了AMContrast3D方法，将对比学习融入模糊度估计框架，实现基于模糊等级的自适应目标；进一步提出AMContrast3D++，引入并行训练的两个分支和模糊度预测模块，利用掩码细化机制提升模糊点特征的可靠性。

Result: 在S3DIS和ScanNet等3D室内场景数据集上进行实验，结果表明该方法显著提升了语义分割性能和模型鲁棒性。

Conclusion: 该方法有效利用点的模糊信息实现自适应训练，能够提高3D点云语义分割的准确性和稳定性，具有较好的应用前景。

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [33] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种双摄像机系统(DCS)和曝曝光自适应融合网络(EAFNet)用于HDR视频重建，以解决曝光波动引起的闪烁问题，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统交替曝光方法在HDR视频重建中常造成参考图像曝光波动，引发闪烁，影响视频质量。为解决曝光不一致带来的问题，设计出新的采集和重建方法。

Method: 提出双摄像机系统，让一台摄像机拍摄一致参考序列，另一台拍摄补充信息的非参考序列。设计曝光自适应融合网络(EAFNet)，包含预对齐子网络选择性强化不同曝光的有效特征，非对称跨特征融合子网络通过参考主导的注意力图融合多尺度特征，重建子网络采用基于DWT的多尺度架构减少重影和提升细节。

Result: 通过大量实验，方法在多种数据集上达到最先进性能，证明了双摄像机系统和EAFNet在HDR视频重建中有效且具有巨大潜力。

Conclusion: 该研究展示了通过双摄像机采集和曝曝光自适应融合网络相结合，能显著改善HDR视频重建的质量，减少闪烁和重影，具有实用推广价值。

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [34] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: 该论文提出了CMDCL方法，通过跨模态因果学习解决长期动作识别中的视觉和文本偏见问题，提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 长期动作识别面临复杂的时间跨度和视觉/文本模态间的偏见，现有基于因果的方法缺乏跨模态建模，限制了性能提升。

Method: 构建跨模态结构因果模型，采用文本因果干预消除文本嵌入偏见，使用视觉因果干预消除视觉中的混淆变量，实现双重因果干预。

Result: 在Charades、Breakfast和COIN三个基准数据集上的实验结果表明，CMDCL方法有效提升了长期动作识别的性能。

Conclusion: 提出的CMDCL通过跨模态双重因果干预，提升了长期动作识别中视觉和文本模态的鲁棒性和准确性，验证了跨模态因果学习的有效性。

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [35] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Omni-Fuse的空间-光谱全融合网络，用于医疗高光谱图像分割，通过多维度特征融合和双向注意力机制提升分割性能，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前医疗高光谱图像在融合空间和光谱信息时存在高维度和光谱冗余的问题，影响分割效果。

Method: 设计了跨维度增强模块、光谱引导的空间查询选择及两阶段跨维度解码器，通过双向注意力机制精炼空间和光谱特征，并动态聚焦空间查询。

Result: 在两个显微高光谱数据集上，Omni-Fuse相比最先进方法，分割性能提升超过5.73%的DSC指标。

Conclusion: Omni-Fuse有效解决了高维光谱图像的空间与光谱信息融合难题，提升了医学高光谱图像的分割性能，具备实际应用潜力。

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [36] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 本文提出了一种视角依赖的投影方法（VDP），通过动态适应视角变化的空间几何特征，实现高效的3D点云到2D图像的映射，从而促进点云分割。


<details>
  <summary>Details</summary>
Motivation: 现有基于投影的方法采用视角无关的投影，依赖预设参数生成投射光线，限制了点云的感知能力及投影多样性，同时多重投影虽能增加空间多样性，但带来了计算上的冗余和效率低下。

Method: 提出VDP框架，根据3D点云分布生成数据驱动的动态投影，模拟烟花行为预测投射光线，生成信息丰富的单张图像输入。同时设计颜色正则化，强化语义像素特征，抑制非语义像素，以最大化2D空间利用率。

Result: PointVDP在计算开销较小的情况下，实现了轻量级投影，且在S3DIS和ScanNet数据集上获得了有竞争力的语义理解性能。

Conclusion: VDP方法通过视角依赖的智能投影，解决了传统方法的预设限制和计算冗余问题，为点云语义分割提供了一种高效且资源节约的解决方案。

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [37] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: 本文提出了EXAONE Path 2.0，一个基于切片级监督训练的病理基础模型，在仅使用3.7万张全切片图像的情况下，实现了在10个生物标志物预测任务上的最佳性能，并展示了出色的数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统的数字病理全切片图像处理方法依赖于补丁级自监督学习，忽略了复杂的领域特异性特征，且数据利用效率低，计算资源需求大。

Method: 使用切片级监督直接训练补丁级特征表示，避免了传统自监督学习依赖简单增强方法的限制。

Result: EXAONE Path 2.0在只有3.7万张全切片图像的训练集上，在10个生物标志物预测任务中实现了最先进的平均性能。

Conclusion: 通过切片级监督训练的病理基础模型能够更有效地捕捉复杂特征，显著提升数据利用效率和预测性能。

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [38] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本文提出一种基于稀疏点注释的密集关键点定位方法，解决了医学图像中密集标注困难的问题。


<details>
  <summary>Details</summary>
Motivation: 医学领域中标注稀疏且成本高，尤其需要像素级密集注释的任务更为困难，因此如何利用少量像素级注释进行学习具有重要意义。

Method: 将关键点定位任务视为稀疏热图回归，提出一种名为Crag and Tail loss的新型损失函数，用于有效利用稀疏正样本标签并减少假负样本影响。

Result: 通过大量消融实验验证了该方法在卵巢癌腹膜癌症关键点的精确密集定位中的效果。

Conclusion: 该方法有效解决了密集注释难以获得的场景下的学习问题，有助于推动相关医学研究的发展。

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [39] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: 提出了ClipGS，一种支持截断平面的高效高斯溅射框架，实现体积医学数据的交互式电影级渲染，提升渲染质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有电影级渲染技术计算成本高、渲染速度低，难以满足实际交互式可视化的需求。

Method: 设计了可学习截断方案自动调整高斯原语对截断平面的可见性，同时采用自适应调整模型动态调整高斯变形，提升渲染性能。

Result: 在五组体积医学数据上测试，取得平均36.635 PSNR、156 FPS渲染速度和16.1 MB模型大小，性能超越现有方法。

Conclusion: ClipGS成功实现了体积医学数据的高质量、高效率交互式电影级渲染，有望推动实际临床应用。

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [40] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的跨模态图像到点云配准方法Diff$^2$I2P，有效桥接图像与点云之间的模态差距，实现更精准的配准。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用度量学习对齐跨模态特征，忽视了图像和点云数据之间的固有模态差异，导致跨模态对应关系不准确。

Method: 引入深度条件扩散模型作为先验，通过控制侧分数蒸馏（CSD）优化变换参数，并设计可微分的可变形对应调优（DCT）模块和PnP求解器，实现端到端可微分的跨模态配准框架Diff$^2$I2P。

Result: 在7-Scenes基准数据集上，Diff$^2$I2P较现有最先进方法在配准召回率上提升超过7%。

Conclusion: 扩散先验有效促进图像与点云跨模态特征学习，显著增强配准性能，验证了Diff$^2$I2P框架的优越性。

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [41] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: 本文提出了一种新的任务CDR-CA，用于根据应用上下文细化多属性的多样性，并提出了多源DPP方法来实现该任务，提高了文本到图像检索的多样性和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统的结果多样化方法仅关注图像外观的多样性，且多样性指标因应用而异，限制了多样化技术的应用范围。针对这一问题，本文提出了根据应用上下文调整多属性多样性的任务CDR-CA。

Method: 提出了多源确定点过程（Multi-Source DPPs, MS-DPP）作为基线方法，将多个来源的属性多样性整合为单一的DPP模型，基于流形表示构建统一相似矩阵，并引入切线归一化（Tangent Normalization）以反映应用上下文。

Result: 通过大量实验验证了所提多源DPP方法在提升多属性多样性方面的有效性，能够更好地适应不同应用背景下的多样性需求。代码已开源供社区使用。

Conclusion: 本文提出的CDR-CA任务和多源DPP模型为文本到图像检索中的多样性优化提供了新的思路和工具，拓宽了多样化技术的应用范围，具有较大实际应用价值。

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [42] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 本文分析了扩散模型在图像恢复任务中先验和似然梯度的不稳定性，并提出SPGD方法以稳定梯度，提高恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像恢复方法在贝叶斯推断框架下，先验与似然梯度的相互作用存在冲突和波动，影响生成过程的稳定性和恢复效果。

Method: 提出稳定渐进梯度扩散（SPGD）技术，包括渐进似然预热策略以缓解梯度冲突，以及自适应方向动量平滑（ADM）以减少似然梯度波动。

Result: 在多种恢复任务中，SPGD显著提升生成过程稳定性，取得量化指标及视觉效果的最先进水平。

Conclusion: SPGD有效解决了扩散模型恢复过程中梯度不稳定问题，推动了图像恢复的性能提升。

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [43] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian是一种用于3D高斯点云压缩的训练自由方法，结合了混合精度量化和属性判别剪枝，实现高效灵活压缩。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯点云表示需要在保证渲染质量的同时，减少内存和计算资源消耗，尤其是在移动和边缘设备上，现有方法往往需要昂贵的重训练，灵活性不足。

Method: FlexGaussian结合了混合精度量化和属性判别剪枝技术，无需重训练即可实现3D高斯点的压缩，适应多样的压缩需求。

Result: 在不显著降低渲染质量（PSNR下降小于1dB）的情况下，实现了高达96.4%的压缩率，速度比现有无训练方法快1.7-2.1倍，比涉训练方法快10-100倍，适合部署在移动设备。

Conclusion: FlexGaussian提供了一种灵活、高效、无需训练的3D高斯点云压缩方案，适应性强且计算速度快，未来代码即将公开。

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [44] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: 提出了QUANet，通过数量导向文本提示和视觉文本数量对齐损失增强大视觉语言模型的计数能力，采用双流自适应计数解码器融合Transformer和CNN，提升无监督类无关计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本提示的视觉语言模型在目标计数任务中难以准确区分数量，需增强模型的数量感知能力。

Method: 设计了数量导向文本提示和视觉文本数量对齐损失，引入由Transformer流、CNN流及其交互适配器组成的双流自适应计数解码器，通过跨流数量排序损失优化预测排序。

Result: 在FSC-147、CARPK、PUCPR+、ShanghaiTech等多个标准数据集上，QUANet表现出了强大的泛化能力和零样本类无关计数性能。

Conclusion: QUANet有效提升了视觉语言模型的数量感知和计数准确性，实现了高效的多模态信息融合及泛化能力，适用于类别无关的目标计数任务。

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [45] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出了一种新颖的空间-时间图神经网络STG-Mamba，用于音乐驱动的舞蹈视频合成，通过音乐到骨架和骨架到视频的两段转换，显著提升了合成质量。


<details>
  <summary>Details</summary>
Motivation: 将音乐信息有效转换为逼真舞蹈视频，当前方法难以同时捕捉骨架的空间和时间依赖关系，且骨架到视频转换存在挑战。

Method: 构建STGM块实现音乐到骨架的空间-时间依赖捕捉；设计自监督正则化网络将骨架和条件图像转换为舞蹈视频；并收集大规模骨架视频数据集支持训练。

Result: 实验结果显示STG-Mamba在音乐到舞蹈视频的合成任务中性能优于现有方法，生成的视频更具连贯性和真实感。

Conclusion: STG-Mamba通过结合空间-时间图模型与自监督正则化，有效提升了音乐指导舞蹈视频合成的质量，具有很高的应用价值。

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [46] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出了一种层次化预训练策略，通过伪手语词（pseudo-glosses）和对比视频语言对齐，提高无词汇注释手语翻译的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的手语翻译方法存在视觉与文本表示差异大，难以有效学习的问题。亮点是采用伪手语词和大语言模型提升无注释方法的灵活性和效果。

Method: 设计层级特征提取，包括帧级、片段级和视频级，结合伪手语词和对比学习，实现视频与文本的对齐。

Result: 实验显示该方法提升了BLEU-4和ROUGE指标，且保持了模型效率。

Conclusion: 层次化预训练结合伪手语词和对比对齐策略显著提升无词汇手语翻译性能，具有实际应用潜力。

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [47] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉适配器和提示学习，利用部分最优传输和对比学习提升CLIP在医疗异常检测中的适应性的方法。


<details>
  <summary>Details</summary>
Motivation: 医疗异常检测面临成像方式多样、解剖变异大及标注数据有限的挑战，亟需提升模型对医疗图像的适应能力。

Method: 通过多提示并结合部分最优传输（POT）对局部特征进行对齐，同时应用对比学习（CL）加强类内凝聚力和类间分离，提升CLIP在医疗图像中的表现。

Result: 该方法在少样本、零样本和跨数据集场景均达到最新状态的性能，不依赖合成数据或存储库。

Conclusion: 提出的方法有效提升了CLIP模型在医疗影像异常检测任务中的适应性和精准度，具备广泛应用潜力。

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [48] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: 本文构建了芯片切割车道数据集（CHDL），提出了面向高精度工业场景的双路径视频预测模型DIFFUMA，在CHDL基准测试中取得显著领先。


<details>
  <summary>Details</summary>
Motivation: 半导体制造等高精度工业场景缺乏专用的时序图像数据集，制约了复杂过程的建模与预测研究。

Method: 构建CHDL数据集；设计DIFFUMA模型，通过并行的Mamba模块捕获长程时序上下文，利用扩散模块恢复细粒度空间细节。

Result: DIFFUMA在CHDL数据集上显著优于现有方法，MSE降低39%，SSIM从0.926提升至0.988，且在自然现象数据集上表现同样优异。

Conclusion: 该工作不仅提出了领先的视频预测模型，还发布了重要数据资源，促进工业AI领域的研究发展。

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [49] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: 提出了一种残差先验驱动的频率感知网络RPFNet，通过残差先验模块和频域融合模块实现高效的多模态图像融合，提升了融合图像的质量和高层视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建空间域长距离特征依赖时计算成本高且缺乏真实标注，影响多模态图像融合效果。

Method: 设计双分支特征提取框架：残差先验模块提取模态差异信息，频率域融合模块进行高效全局特征建模，辅以交叉促进模块实现局部与全局信息的双向交互，训练时采用辅助解码器、显著性结构损失及加权频率对比损失和SSIM损失约束。

Result: RPFNet有效融合了辨别特征，增强了纹理细节和显著目标，实验证明其在图像融合及高层视觉任务中表现优异。

Conclusion: RPFNet通过合理融合局部差异和全局频率信息，解决了计算成本和缺乏标注的难题，提升了多模态图像融合的性能和应用价值。

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [50] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 本文提出了FOLC-Net框架，采用联邦优化轻量级架构，提升MRI多视角疾病诊断性能，尤其在矢状面表现优异，实现92.44%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA模型在处理MRI不同解剖平面（轴状面、冠状面、矢状面）时性能下降，需提升各单视角及多视角诊断准确率。

Method: 提出FOLC-Net，结合Manta-ray觅食算法优化模型结构、全局模型克隆实现可扩展训练，及ConvNeXt增强模型适应性。

Result: FOLC-Net在轴状、冠状、矢状等多视角数据均展示出优异性能，矢状面准确率达92.44%，显著优于DL及DL+残差学习方法。

Conclusion: FOLC-Net有效解决了现有模型对单视角适应差和多视角性能不足的问题，提升了分散环境下医学图像诊断的准确性和鲁棒性，具备良好应用前景。

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [51] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: 本文提出了一种基于提示复杂度感知的缓存方法PCA，用于加速视频生成过程，同时保证生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成加速方法通过固定频率缓存输出，导致复杂场景中质量显著下降且门槛调节效率低且不稳定。

Method: 通过从输入提示中估计场景复杂度，动态调整缓存重用阈值，结合多元多项式特征扩展改进预测准确性，替换静态CFG缓存为动态机制以选择性重用输出。

Result: 在Wan2.1模型上实现了约2.79倍的加速，同时在多种场景下保持高视觉质量。

Conclusion: 提出的PCA缓存方法有效提升了视频生成的推理速度和质量控制的自适应性，克服了现有缓存机制的局限性。

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [52] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种弱监督文本与人物图像匹配的新方法，利用双粒度身份关联机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确预测复杂的一对多身份关系，限制了匹配性能的提升。

Method: 设计局部和全局双粒度身份关联机制，局部层面加强跨模态身份关系，全局层面构建动态身份关联网络并引入置信度动态调整机制；同时提出信息不对称样本对构建和一致性学习方法，增强模型对困难样本的识别和鲁棒性。

Result: 实验结果表明该方法显著提升了跨模态匹配的准确率。

Conclusion: 提出的方法有效解决了弱监督条件下文本与人物图像匹配中的身份关联难题，提升了匹配性能，具有较高的实用价值。

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [53] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: 本文提出了一种名为D2I的多模态大语言模型推理框架，通过训练时的规则格式奖励提升模态对齐和推理能力，测试时则切换为直观推理风格，从而无需额外标注和复杂奖励，显著提升了模型跨域表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法依赖额外数据标注和基于规则的复杂奖励，导致训练成本高且难以扩展，亟需低成本且高效的推理方法。

Method: 设计了Deliberate-to-Intuitive推理框架，训练阶段采用基于规则的格式奖励实现有意识的推理策略以提升模态对齐，测试阶段去除这些策略转为直观推理，从而体现模型学得的能力。

Result: D2I在多项领域内外的基准测试中均优于现有基线方法，表现出更强的推理能力和迁移能力。

Conclusion: 格式奖励在提升多模态大模型的推理技能转移中起关键作用，D2I框架实现了训练时推理深度与测试时响应灵活性的有效解耦，推动了多模态推理研究的发展。

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [54] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: 本文提出了一种针对濒危满语的高效OCR系统，通过对视觉语言大模型微调实现了合成数据到真实文档的有效迁移，显著超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 濒危语言满语缺乏能处理真实历史文档的OCR系统，限制了历史和语言研究的发展。

Method: 使用参数高效训练技术，在6万张合成满语单词图像上微调三种开源视觉语言模型（LLaMA-3.2-11B、Qwen2.5-VL-7B、Qwen2.5-VL-3B）。

Result: LLaMA-3.2-11B在合成数据上达成98.3%词准确率，字符错误率0.0024，真实手写文档上保持93.1%的准确率，明显优于传统CRNN基线。

Conclusion: 该方法实现了合成到真实文档的有效域迁移，提供了低成本、易部署的濒危语言OCR解决方案，促进数字人文领域的研究。

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [55] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 提出了一种基于2D全身骨架的轻量级共语音手势视频生成框架，结合扩散模型和骨架-音频特征融合，实现高保真度且音视频同步的视频生成，同时发布了包含405小时多样化数据集CSG-405。


<details>
  <summary>Details</summary>
Motivation: 共语音手势视频生成面临音频与视觉内容一对多映射、数据稀缺和计算成本高的挑战，亟需有效且轻量的生成方法。

Method: 采用2D全身骨架作为辅助条件，利用基于细粒度音频段和骨架特征融合的扩散模型预测骨架运动，再结合参考图像通过现成人体视频生成模型合成视频。

Result: 生成的视频在视觉质量和同步性方面超越了现有最先进方法，且具有良好的说话人和环境泛化能力。

Conclusion: 该方法有效解决了音频到视觉的映射难题，实现了高质量、同步的共语音手势视频生成，并通过首个大型公开数据集促进了该领域的研究发展。

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [56] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 本文提出了一个生成航空视角合成热成像图像的新方法，解决了热成像数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 热成像无人机图像在救援和野生动物监测等领域应用广泛，但缺乏大规模多样的热成像航空数据集，限制了深度学习模型的发展。

Method: 设计了一套程序化流程，将任意物体类别合成到现有热成像背景中，控制物体的位置、尺度、朝向，确保与背景视角一致，并扩展了HIT-UAV和MONET数据集，引入无人机和动物类别。

Result: 新扩展的数据集在目标检测任务中表现优异，尤其是热成像检测器优于可见光训练模型。

Conclusion: 提出的方法有效拓展了热成像无人机数据集的应用范围，强化了对航空视角的重要性理解。

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [57] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: 该论文提出了GreenHyperSpectra数据集，利用半监督和自监督机器学习方法，在跨传感器和跨生态系统的高光谱数据中有效预测植物性状，提高了跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统植物性状采集方法难以覆盖大尺度的生态变异，且高光谱数据标签稀缺且存在显著的领域转移，亟需设计通用且高效的跨域性状预测方法。

Method: 构建包含跨传感器和跨生态系统样本的GreenHyperSpectra预训练数据集，采用半监督和自监督学习框架，实现多输出回归模型的预训练，并进行内分布和外分布的评估。

Result: 预训练的模型在标签稀缺的情况下表现优于当前有监督基线方法，在光谱表征学习和性状预测精度上取得显著提升。

Conclusion: GreenHyperSpectra建立了一个通用的跨域植物性状预测基准框架，推动了表征学习与植物功能性状评估的交叉研究，促进了生态学和遥感领域的发展。

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [58] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的低光照图像增强方法HVI-CIDNet+，基于新定义的HVI颜色空间，提升了色彩还原和亮度调整效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于标准RGB和HSV颜色空间的低光照图像增强方法存在色偏、亮度伪影、红色噪声和黑色噪声等问题，亟需一种更有效的颜色空间和网络结构来解决这些缺陷。

Method: 提出了新颜色空间HVI（Horizontal/Vertical-Intensity），通过HV色彩图减少红色噪声，学习强度去除黑色噪声；基于HVI构建HVI-CIDNet+网络，利用预训练视觉-语言模型提取的先验知识，通过先验指导注意力块（PAB）实现语义内容恢复和颜色修正，同时采用区域细化块分别处理信息丰富和信息稀缺区域的亮度调整。

Result: 在10个基准数据集上的全面实验表明，HVI-CIDNet+在图像内容恢复、色彩还原和亮度调整方面均显著优于当前最先进的方法。

Conclusion: 新提出的基于HVI颜色空间的HVI-CIDNet+方法有效缓解了低光照图像中的色彩失真和噪声问题，提升了低光增强性能，具有较高的应用价值和推广潜力。

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [59] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: 本文提出了一种结合符号回归和轨迹引导图像到视频生成的新框架，实现了物理精准的视频预测。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和自回归视频生成模型虽然视觉效果逼真，但缺乏物理一致性，未能准确模拟现实世界的物体运动规律。

Method: 通过从输入视频提取运动轨迹，利用检索预训练机制增强符号回归能力，发现运动方程，并基于这些方程预测未来轨迹，进而引导视频生成，无需对现有模型微调。

Result: 在经典力学场景中（弹簧-质量系统、摆锤和抛体运动）成功恢复了真实运动方程，并提升了生成视频的物理一致性。

Conclusion: 所提框架有效融合物理机制与视频生成，解决了模型缺乏物理对齐的问题，显著提高了视频预测的物理准确性。

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [60] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: 本文提出了一种利用Vision Transformer（ViT）自注意力机制进行弱监督语义分割（WSSS）的端到端方法，通过多个[CLS]标记和随机掩码策略生成伪分割掩码，实验表明该方法准确性高，性能接近全监督模型。


<details>
  <summary>Details</summary>
Motivation: 传统的弱监督语义分割方法依赖外部模块如Class Activation Maps生成伪掩码，存在解释性和准确性不足的问题，因此需要一种直接利用模型内置机制提升伪掩码质量的方案。

Method: 方法采用稀疏ViT结构，训练多个与类别对应的[CLS]标记，应用随机掩码促进[CLS]与类别的对应关系；推理时聚合各[CLS]对应的自注意力图生成伪分割掩码。

Result: 在两个标准基准和三个专业数据集上的广泛实验表明，该方法生成的伪掩码准确且优于相关工作，并可用于训练语义分割模型，其性能接近全监督模型。

Conclusion: 该方法有效提高了WSSS中伪掩码的质量和解释性，显著减少对细粒度标注数据的依赖，为弱监督语义分割提供了一种具有竞争力的端到端解决方案。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [61] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为IAP的对抗性补丁攻击框架，通过感知感知的定位与扰动优化，生成在视觉上几乎不可察觉的对抗性补丁，有效提升目标攻击的成功率并绕过多种防御措施。


<details>
  <summary>Details</summary>
Motivation: 现有对抗补丁方法在目标攻击场景下效果不佳，或生成的补丁在视觉上不够隐蔽，容易被人眼或自动防御系统发现，缺乏隐蔽性。

Method: IAP方法包括两大核心：首先通过类别定位和敏感性图寻找合适的补丁放置位置，兼顾模型预测的易攻性与人眼的感知难度；其次采用感知正则化对抗损失和优先保持颜色一致性的梯度更新规则，优化生成高度隐形的扰动。

Result: 在多个图像基准和模型架构上，IAP在目标攻击设置下实现了具有竞争力的攻击成功率，同时补丁的隐蔽性较现有方法显著提升。此外，IAP生成的补丁能够有效绕过多种先进的补丁防御手段。

Conclusion: IAP通过感知感知的定位和扰动优化显著提升了对抗补丁的隐蔽性和攻击效果，为目标型对抗攻击提供了更具实用价值的解决方案。

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [62] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Blümel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: 研究基于两年半中400多名参与者的长期生物识别数据，利用先进的面部识别算法分析数据，发现日间评分波动大于长期波动，强调长期测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨长期生物识别特征变化，为提升生物识别技术的准确性与可靠性提供依据。

Method: 在受控环境下，使用多模态生物识别工具（如面部和指纹识别）对不同背景的400多名参与者进行定期测试，分析238,000多组符合GDPR的生物识别数据，应用先进面部识别算法研究长期对比评分。

Result: 发现个体生物识别评分在不同天之间波动显著大于整个测量期内的波动，反映出短期波动较大。

Conclusion: 强调在受控环境中对同一人进行长期测试的重要性，为生物识别数据分析和未来技术进步奠定基础。

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [63] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 本文提出了一种针对稀疏雷达点云的全景分割方法SemRaFiner，以提升自动驾驶的语义场景理解。


<details>
  <summary>Details</summary>
Motivation: 当前常用的摄像头和LiDAR传感器在恶劣天气下存在局限，且通常不提供运动信息；雷达传感器则能够提供运动信息，但点云数据稀疏且噪声较大。

Method: 提出SemRaFiner方法，针对稀疏雷达点云的密度变化优化特征提取，并通过专门的数据增强优化训练过程以改进实例分配。

Result: 实验证明，SemRaFiner在雷达全景分割任务上优于现有最先进的方法。

Conclusion: 该方法有效提升了稀疏雷达点云的全景分割性能，增强了自动驾驶系统的场景理解能力。

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [64] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文针对广义类别发现提出了一种基于自适应部件发现和学习的新方法APL，通过共享部件查询和DINO部件先验实现无监督部件一致性，采用全最小对比损失增强区分度和泛化能力，显著提升细粒度数据集表现。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法依赖DINO的CLS全局特征，存在区分能力和泛化性之间的权衡问题，需设计更有效的表征来兼顾两者。

Method: 提出APL方法，通过共享可学习部件查询和DINO部件先验实现不同相似图像间对象部件及对应关系的一致性，且不需额外标注；并设计全最小对比损失，突出区分性部件同时共享其他部件以促进知识迁移。

Result: 将APL替换GCD方法中的CLS特征后，在细粒度数据集上性能显著提升，表明其在提高区分度和泛化性方面效果良好。

Conclusion: APL有效解决了GCD中全局表征的权衡问题，通过部件级别的表示提升了模型区分细粒度类别的能力和知识迁移的泛化性，可广泛集成于各类GCD框架。

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [65] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出了一个包含丰富多属性标注的中国书法字符数据集MCCD，涵盖多种书法风格、朝代及书法家信息，为书法字符识别、作者鉴定和演变研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 由于中国书法字符风格随朝代及书法家而变化，且现有数据集稀缺且缺乏属性信息，阻碍了深入研究。

Method: 构建了包含7,765类共329,715个单字符图像的多属性书法数据集MCCD，涵盖10种书体、15个朝代及142位书法家的标注，并进行了单任务及多任务识别实验。

Result: 实验表明书法字符结构复杂，属性间关联增加识别难度，但MCCD数据集有效支持多样化任务并建立了基准性能。

Conclusion: MCCD填补了详细书法数据集的空白，为中国书法研究及相关领域发展提供了重要资源。

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [66] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,César Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hernández,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: 该研究利用深度学习和卫星图像识别棕榈树分布，推断了古代人类对新热带森林的长期影响，揭示人类管理区域比考古证据显示的范围大得多。


<details>
  <summary>Details</summary>
Motivation: 研究古代人类管理对新热带森林的长期影响，尤其是在高分辨率尺度下的具体作用仍难以理解，因此需要新的方法来识别古代人类影响区域。

Method: 提出基于深度学习的卫星影像棕榈树识别模型，结合聚类算法确定棕榈树簇，推测古代管理区域，并应用于哥伦比亚Sierra Nevada de Santa Marta的卫星数据，同时发布包含人工标注数据及考古遗址位置数据集。

Result: 发现考古遗址周围棕榈树显著增多，且最大棕榈簇面积比考古证据所示大约大两个数量级，表明古代人类对植被的影响持久且显著。

Conclusion: 古代原住民通过促进棕榈树生长，改变了局部生态，为建立大型基础设施提供便利，研究展示了人工智能与考古生态数据结合揭示人类环境交互的潜力。

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [67] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: 提出了CheXPO，一种结合置信度-相似性联合挖掘与反事实推理的胸部X光偏好优化策略，以提升视觉语言模型在医疗应用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗领域易出现幻觉，且现有临床反馈优化方法受训练样本不相关、数据分布不平衡和专家标注成本高的限制。

Method: 通过合成多任务胸部X光视觉指令数据集进行监督微调；利用token级置信度分析识别难例，并通过相似性检索扩展样本以平衡数据；结合合成反事实推理生成细粒度临床偏好，避免额外专家标注。

Result: CheXPO在仅使用5%监督微调样本的情况下，性能提升8.93%，在多种临床任务中达到最新水平。

Conclusion: CheXPO提供了一种可扩展、可解释的解决方案，有效缓解了视觉语言模型在放射学实际应用中的幻觉和数据挑战，提高了医疗AI的可靠性。

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [68] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 本文提出了一个基于深度学习的多领域MR-MR图像配准方法ProRSeg，针对前列腺癌MRgART治疗中的图像变形配准进行了训练和评估，表现出良好的跨领域配准性能及剂量累积评估能力。


<details>
  <summary>Details</summary>
Motivation: 准确的可变形图像配准（DIR）对于MR引导的适应性放疗中的轮廓传播和剂量累积至关重要，但存在不同MR设备域间配准泛化的挑战。

Method: 使用262对3T MR图像及加权分割一致性损失训练渐进式配准与分割方法ProRSeg，分别在同域、跨域及混合域数据集上进行临床靶区及器官轮廓传递精度测试，进行剂量累积验证。

Result: ProRSeg在膀胱配准上展现出域间一致的高Dice系数，直肠及靶区表现受域影响；跨域剂量累积显示大部分患者达到了临床剂量覆盖和器官保护指标。

Conclusion: ProRSeg方法具备多领域MR-MR图像配准能力及初步满足临床剂量累积评价需求，有望支持MRgART实践中的治疗顺应性评估。

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [69] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttopää,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: 本文提出了一个结合分子和图像数据的混合节肢动物样本数据集（MassID45），用于自动分类大规模无序昆虫样本，实现快速高效的昆虫多样性研究。


<details>
  <summary>Details</summary>
Motivation: 昆虫种类繁多但数量下降严重，传统的基于单个标本的图像分类方法难以应用于大规模生态调查中的无序混合样本，亟需新的高通量方法。

Method: 构建MassID45数据集，结合DNA条形码技术和高分辨率图像，对超过1.7万只个体进行分割掩码标注和分类标签赋予，使用人工加AI辅助工具处理大规模无序样本的图像和分子数据。

Result: 创建了一个涵盖无序样本及完整个体标本的多模态数据集，实现了对微小目标检测和实例分割的新突破，促进了昆虫群落的快速大规模表征。

Conclusion: MassID45数据集促进了生态学与机器学习领域的创新，为快速、准确的昆虫多样性自动分类提供了强大工具，推动了昆虫群落监测和保护研究的发展。

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [70] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的测试时适应方法FreeTTA，利用视语言模型的零样本预测和在线EM算法，提升了模型在跨域和分布外数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时适应方法通常依赖昂贵的训练过程或不切实际的数据假设，限制了其实用性和灵活性。

Method: FreeTTA通过引入一项在线EM算法，利用VLMs的零样本预测作为先验，迭代计算测试样本的后验概率，实现了对测试数据分布的显式建模，不依赖历史数据或训练过程。

Result: 在15个数据集上的跨域和分布外测试中，FreeTTA相比现有最先进方法实现了稳定且显著的性能提升。

Conclusion: FreeTTA为测试时适应提供了一个无需训练、普适且高效的解决方案，首次利用测试样本之间的内在关系改善单样本预测，展示了较强的实际应用潜力。

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [71] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种利用医学概念对齐的放射学报告生成框架MCA-RG，通过显式将视觉特征与病理和解剖学概念对齐，提升报告生成的准确性和临床相关性，在两个公共数据集上的表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在放射学报告生成中的临床应用受限，主要由于难以准确映射病理和解剖特征到文本描述，且语义不可知的特征提取影响报告的准确性。

Method: MCA-RG框架借助两个专业的医学概念库，分别对应病理和解剖特征，将视觉特征与这些医学概念对齐并进行增强。引入基于解剖的对比学习提高解剖特征泛化，采用匹配损失聚焦临床相关区域，同时使用特征门控机制过滤低质量特征，最后利用对齐后的特征指导报告生成。

Result: 在MIMIC-CXR和CheXpert Plus两个公开基准数据集上的实验表明，MCA-RG在放射学报告生成任务中性能显著优于现有方法，验证了其有效性。

Conclusion: 通过医学概念对齐和针对视觉特征的多策略优化，MCA-RG显著提升了放射学报告的生成质量和临床相关性，推动了自动化医疗报告生成技术的临床应用前景。

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [72] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文提出了适用于恶劣天气的基于LiDAR的集体感知多任务架构DenoiseCP-Net，通过在通信前进行噪声去除，显著降低带宽需求和推理延迟，同时保持检测准确率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知系统在恶劣天气和环境遮挡下传感器性能下降，影响安全性；集体感知虽然能共享信息弥补感知缺陷，但恶劣天气下的集体感知研究不足。

Method: 提出DenoiseCP-Net，将体素级噪声过滤和目标检测整合在统一的稀疏卷积骨干网中，避免两阶段计算冗余，减小通信开销。并通过模拟雨、雪、雾等天气扩展OPV2V数据集进行验证。

Result: DenoiseCP-Net在恶劣天气下实现接近完美的去噪效果，带宽需求减少高达23.6%，同时保持检测准确率不变并减少推理延迟。

Conclusion: 通过统一多任务架构进行噪声去除和目标检测，有效解决了恶劣天气中集体感知的带宽和延迟问题，提升了自动驾驶车辆的协作感知性能。

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [73] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 本文提出了一个大规模多模态数据集和一种新颖的多模态特征融合框架，用于提高非小细胞肺癌（NSCLC）免疫治疗患者的生存预测准确性。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者免疫治疗的准确预后对个性化治疗和改善预后至关重要，但缺乏大规模数据及有效的多模态特征融合策略成为挑战。

Method: 构建包含3D CT图像与临床数据的大型数据集，设计基于Slice-Depth Transformer和图形Transformer的跨模态掩码学习方法，实现各模态特征的有效融合和信息补全。

Result: 该方法在NSCLC生存预测任务中表现优异，超过现有多模态融合方法，建立了新的预后模型基准。

Conclusion: 所提框架有效提升了多模态特征的整合能力，为NSCLC免疫治疗患者提供了更准确的生存预后工具。

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [74] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: 本论文设计开发了一种基于OCR的发票表格高效提取系统，通过文本识别和定制后处理，实现对扫描发票中结构化表格数据的准确提取。


<details>
  <summary>Details</summary>
Motivation: 解决发票扫描件中噪声多、格式不标准，难以准确提取表格数据的问题。

Method: 利用Tesseract OCR进行文本识别，结合动态预处理、表格边界检测及行列映射的自定义后处理流程。

Result: 显著提升了发票表格数据提取的准确率和一致性，适用于自动化财务流程和数字存档等实际场景。

Conclusion: 该OCR驱动的表格提取管道有效应对了复杂发票格式，改善了数据抽取质量，具备良好的应用推广价值。

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [75] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于图神经网络与视觉变换器的病理切片图像分类及自动描述生成框架GNN-ViTCap，有效解决了冗余切片和未知位置的问题，显著提升了分类和描述性能。


<details>
  <summary>Details</summary>
Motivation: 病理显微镜全切片图像在癌症诊断中关键，但存在冗余切片和切片位置信息缺失问题，同时自动生成病理图像的文本描述也是难点。

Method: 通过视觉特征提取获得切片嵌入，利用深度聚类去除冗余切片并选取代表性切片，构建图神经网络捕捉局部和全局上下文，再将聚合图像嵌入映射到语言模型输入空间，实现联合分类与描述生成。

Result: 在BreakHis和PatchGastric数据集上，分类F1达到0.934，AUC为0.963；描述生成BLEU-4为0.811，METEOR为0.569，优于当前最先进方法。

Conclusion: GNN-ViTCap框架为显微镜病理图像诊断提供了高效且可靠的分类与自动描述生成新方案，具有广泛应用潜力。

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [76] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练病理基础模型的轻量级神经网络方法，实现了从HE染色组织学图像中高效准确预测细胞组成，无需进行昂贵的空间转录组测序。


<details>
  <summary>Details</summary>
Motivation: 数字病理和深度学习的发展推动了病理基础模型的出现，同时空间转录组技术使得在HE图像上实现基因表达分析成为可能。如何利用病理基础模型直接预测细胞组成，减少空间转录组的成本，是本文的研究动机。

Method: 本文利用预训练病理基础模型提取信息丰富的特征嵌入，训练轻量级的多层感知机回归器，基于cell2location获得的细胞类型丰度进行监督学习，从HE图像中预测细胞组成。

Result: 该方法在准确性上与现有方法Hist2Cell相当，但计算复杂度显著降低，展示了良好的效率和性能。

Conclusion: 本文方法成功实现了利用预训练病理基础模型和轻量级神经网络高效预测细胞组成，具备显著降低成本和计算资源需求的潜力，对数字病理和空间转录组数据整合具有重要意义。

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [77] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: 本文提出了一种基于图神经网络的几何深度学习框架，用于高效预测机械零件CAD网格的Laplace-Beltrami谱，显著减少了计算时间且保证了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于有限元方法（FEM）计算Laplace-Beltrami谱在处理大量机械零件CAD网格时计算复杂度高，效率不足，难以快速做出质量控制决策。

Method: 设计一种利用丰富网格特征（高斯曲率、平均曲率、主曲率等）的图神经网络架构，通过学习预测LB谱，替代传统FEM方法。

Result: 在真实机械CAD模型数据集上实验验证，方法将计算时间缩短约5倍，且准确度与FEM方法相当。

Conclusion: LB谱具有良好的可学习性，且所提深度学习方法在计算效率和准确性间实现了良好的权衡，适合大规模机械零件数据的快速谱估计。

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [78] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MST-Distill的新型跨模态知识蒸馏框架，通过多样化教师模型和动态路径选择，有效解决了传统方法的蒸馏路径选择和知识漂移问题，显著提升跨模态知识转移效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态知识蒸馏方法面临数据和统计异质性带来的挑战，难以充分利用教师模型中的互补先验知识，且存在蒸馏路径选择和知识漂移等关键问题。

Method: MST-Distill采用多样化教师模型组合和实例级动态路由网络，实现自适应蒸馏路径选择；引入独立训练的遮罩模块抑制模态间差异，减轻知识漂移。

Result: 在五个包含视觉、音频和文本的多模态数据集上，MST-Distill显著优于现有最先进的跨模态知识蒸馏方法。

Conclusion: 通过采用混合教师和动态路由机制，MST-Distill有效提升了跨模态知识蒸馏的性能，解决了传统方法的关键瓶颈，具有广泛的应用潜力。

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [79] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文探讨了结合上下文元数据提升多模态大模型对餐食图像进行营养分析的性能，提出了新的公开数据集ACETADA，并验证了上下文信息和推理增强方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中在如GPT-4等专有模型，且较少探索整合上下文元数据对营养分析的影响，缺乏对多种大语言模型的系统评估。

Method: 通过GPS地点类型、时间戳转餐次/日类型及食物项作为上下文元数据，结合八种大模型（含开放和闭源模型），采用多种推理修饰符（如Chain-of-Thought，Few-Shot等）测试其对营养成分估计的影响，基于新公开的ACETADA数据集进行评测。

Result: 结果显示加入上下文元数据能有效降低营养成分估计的平均绝对误差和百分比误差，且改进上下文信息整合后推理增强技术表现更优。

Conclusion: 上下文感知的多模态大模型在营养分析任务中具有显著提升潜力，结合语境信息和推理方法可大幅改善模型性能。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [80] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet利用深度学习解决图像中尺子测量尺度转换的难题，实现了鲁棒性强、泛化性好且适用于实时应用的尺度估计。


<details>
  <summary>Details</summary>
Motivation: 传统图像尺度估计受限于手工阈值和专用算法，难以应对多样化的尺子和拍摄条件，限制了实际应用的发展。

Method: 将尺子读数转化为关键点检测问题，采用几何级数参数表示尺子以抵抗透视变换，利用合成数据增强训练，并设计轻量级网络DeepGP进行高效参数回归。

Result: RulerNet在多种复杂真实环境下表现出准确、一致且高效的尺度估计能力，显著优于传统方法，支持移动端实时应用。

Conclusion: RulerNet是一种通用且高效的测量工具，适合与其他视觉组件结合，实现自动化且具尺度感知的分析，具有广泛应用前景。

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [81] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉问答定位的文本到图像生成评价指标，解决了复杂属性分配混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动化文本到图像生成评价方法在处理复杂的实体-属性语义时存在属性混淆问题，难以准确评估生成图像的质量。

Method: 利用视觉问答（VQA）定位策略，针对单一实体进行视觉与文本多模态的局部对齐，设计了局部化的人类评估协议，并提出了结合视觉定位和VQA的自动指标L-VQAScore。

Result: 在一个新的复杂构成对齐数据集上，L-VQAScore在与人工评判的相关性方面优于现有顶尖的文本到图像评价方法，表现出更强的细粒度实体-属性捕捉能力。

Conclusion: L-VQAScore作为一种结合视觉定位和VQA的自动评价指标，能够更精准且可扩展地替代人工主观评价，提升文本到图像生成模型的评估效果。

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [82] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: 本文提出了MotionMillion数据集和MotionMillion-Eval评测基准，显著提升了基于文本的零-shot人类动作生成的效果，实现了模型对复杂动作的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动人类动作生成方法受限于训练数据规模，零-shot泛化能力不足，且缺乏全面评估框架，限制了该领域的发展。

Method: 构建了包含2000小时、200万高质量动作序列的MotionMillion大规模数据集，设计了MotionMillion-Eval作为零-shot动作生成评测标准，并通过7B参数规模模型进行训练和验证。

Result: 在MotionMillion-Eval上，模型表现出对域外及复杂复合动作的强泛化能力，显著推进了零-shot人类动作生成技术。

Conclusion: 通过大规模数据集与全面评测基准，结合大规模模型训练，本文推动了文本到动作生成领域进入零-shot泛化新阶段。

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [83] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视觉-语言-视觉（VLV）自编码器框架，通过利用预训练的图像编码器、文本到图像扩散模型解码器及大型语言模型，实现了在低成本和少量数据条件下构建先进视觉语言模型的目标。


<details>
  <summary>Details</summary>
Motivation: 当前构建具有强大图像描述能力的视觉语言模型需要大量高质量的图文对，训练成本极高，限制了技术的普及和应用。

Method: 提出VLV自编码器框架，利用冻结的预训练文本到图像（T2I）扩散解码器设置信息瓶颈，使用连续嵌入方法从条件扩散模型提取知识，进而微调预训练大型语言模型将中间语言表示解码为详细描述。

Result: 成功构建出一个与GPT-4o和Gemini 2.0 Flash等顶尖模型相当的图像描述生成器，显示出出色的语义理解和高质量重建能力。

Conclusion: 通过最大化利用现有预训练模型并主要利用单模态图像数据，显著降低训练成本和对大规模图文对数据的需求，训练总费用低于1000美元，实现了成本效益极高的视觉语言模型构建方案。

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [84] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent是一个统一的智能超分辨率系统，能够将任何低分辨率图像提升至4K甚至更高分辨率，特别适用于严重降质的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 面对不同降质严重的低分辨率图像，传统的超分辨率方法难以通用且效果有限，4KAgent旨在开发一个可适应多场景的通用超分辨率系统。

Method: 4KAgent包括画像定制模块、视觉-语言感知代理及图像修复代理三大核心组件，通过递归执行-反思机制和专家混合策略，针对输入图像动态制定并执行恢复方案；同时内嵌专业人脸修复通道。

Result: 4KAgent在11个任务类别、26个多样化基准数据集上表现出色，涵盖自然图像、肖像、AI生成内容、卫星图像及医疗成像，超越多项主流感知和保真度评价指标的现有最先进水平。

Conclusion: 通过引入基于多专家和递归反思的智能代理框架，4KAgent不仅提升了超分辨率技术的通用性和效果，还为视觉自动化代理研究提供了新范式，促进相关领域的创新发展。

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [85] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文研究了利用预训练的文本到图像扩散模型作为视觉编码器，以提升多模态大语言模型的图像问答能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型普遍使用CLIP作为视觉编码器，但CLIP难以捕捉细粒度图像细节，限制了问答的效果。

Method: 分析扩散模型的内部表示，发现其语义丰富且能实现强图文对齐；利用文本条件聚焦于输入问题相关区域；提出了结合CLIP和扩散特征的简单融合策略，并解决了大语言模型从扩散提示中泄露信息的问题。

Result: 在通用视觉问答和专门的多模态大语言模型基准上进行评估，结果显示扩散模型特征能显著提升视觉理解能力，尤其在需要空间和组合推理的视觉任务中表现优异。

Conclusion: 扩散模型作为视觉编码器在多模态语言模型中的潜力巨大，通过融合CLIP和扩散特征能有效提升图像问答性能，具有广阔的应用前景。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
*Gheorghe Comanici,Eric Bieber,Mike Schaekermann,Ice Pasupat,Noveen Sachdeva,Inderjit Dhillon,Marcel Blistein,Ori Ram,Dan Zhang,Evan Rosen,Luke Marris,Sam Petulla,Colin Gaffney,Asaf Aharoni,Nathan Lintz,Tiago Cardal Pais,Henrik Jacobsson,Idan Szpektor,Nan-Jiang Jiang,Krishna Haridasan,Ahmed Omran,Nikunj Saunshi,Dara Bahri,Gaurav Mishra,Eric Chu,Toby Boyd,Brad Hekman,Aaron Parisi,Chaoyi Zhang,Kornraphop Kawintiranon,Tania Bedrax-Weiss,Oliver Wang,Ya Xu,Ollie Purkiss,Uri Mendlovic,Ilaï Deutel,Nam Nguyen,Adam Langley,Flip Korn,Lucia Rossazza,Alexandre Ramé,Sagar Waghmare,Helen Miller,Vaishakh Keshava,Ying Jian,Xiaofan Zhang,Raluca Ada Popa,Kedar Dhamdhere,Blaž Bratanič,Kyuyeun Kim,Terry Koo,Ferran Alet,Yi-ting Chen,Arsha Nagrani,Hannah Muckenhirn,Zhiyuan Zhang,Corbin Quick,Filip Pavetić,Duc Dung Nguyen,Joao Carreira,Michael Elabd,Haroon Qureshi,Fabian Mentzer,Yao-Yuan Yang,Danielle Eisenbud,Anmol Gulati,Ellie Talius,Eric Ni,Sahra Ghalebikesabi,Edouard Yvinec,Alaa Saade,Thatcher Ulrich,Lorenzo Blanco,Dan A. Calian,Muhuan Huang,Aäron van den Oord,Naman Goyal,Terry Chen,Praynaa Rawlani,Christian Schallhart,Swachhand Lokhande,Xianghong Luo,Jyn Shan,Ceslee Montgomery,Victoria Krakovna,Federico Piccinini,Omer Barak,Jingyu Cui,Yiling Jia,Mikhail Dektiarev,Alexey Kolganov,Shiyu Huang,Zhe Chen,Xingyu Wang,Jessica Austin,Peter de Boursac,Evgeny Sluzhaev,Frank Ding,Huijian Li,Surya Bhupatiraju,Mohit Agarwal,Sławek Kwasiborski,Paramjit Sandhu,Patrick Siegler,Ahmet Iscen,Eyal Ben-David,Shiraz Butt,Miltos Allamanis,Seth Benjamin,Robert Busa-Fekete,Felix Hernandez-Campos,Sasha Goldshtein,Matt Dibb,Weiyang Zhang,Annie Marsden,Carey Radebaugh,Stephen Roller,Abhishek Nayyar,Jacob Austin,Tayfun Terzi,Bhargav Kanagal Shamanna,Pete Shaw,Aayush Singh,Florian Luisier,Artur Mendonça,Vaibhav Aggarwal,Larisa Markeeva,Claudio Fantacci,Sergey Brin,HyunJeong Choe,Guanyu Wang,Hartwig Adam,Avigail Dabush,Tatsuya Kiyono,Eyal Marcus,Jeremy Cole,Theophane Weber,Hongrae Lee,Ronny Huang,Alex Muzio,Leandro Kieliger,Maigo Le,Courtney Biles,Long Le,Archit Sharma,Chengrun Yang,Avery Lamp,Dave Dopson,Nate Hurley,Katrina,Xu,Zhihao Shan,Shuang Song,Jiewen Tan,Alexandre Senges,George Zhang,Chong You,Yennie Jun,David Raposo,Susanna Ricco,Xuan Yang,Weijie Chen,Prakhar Gupta,Arthur Szlam,Kevin Villela,Chun-Sung Ferng,Daniel Kasenberg,Chen Liang,Rui Zhu,Arunachalam Narayanaswamy,Florence Perot,Paul Pucciarelli,Anna Shekhawat,Alexey Stern,Rishikesh Ingale,Stefani Karp,Sanaz Bahargam,Adrian Goedeckemeyer,Jie Han,Sicheng Li,Andrea Tacchetti,Dian Yu,Abhishek Chakladar,Zhiying Zhang,Mona El Mahdy,Xu Gao,Dale Johnson,Samrat Phatale,AJ Piergiovanni,Hyeontaek Lim,Clement Farabet,Carl Lebsack,Theo Guidroz,John Blitzer,Nico Duduta,David Madras,Steve Li,Daniel von Dincklage,Xin Li,Mahdis Mahdieh,George Tucker,Ganesh Jawahar,Owen Xiao,Danny Tarlow,Robert Geirhos,Noam Velan,Daniel Vlasic,Kalesha Bullard,SK Park,Nishesh Gupta,Kellie Webster,Ayal Hitron,Jieming Mao,Julian Eisenschlos,Laurel Prince,Nina D'Souza,Kelvin Zheng,Sara Nasso,Gabriela Botea,Carl Doersch,Caglar Unlu,Chris Alberti,Alexey Svyatkovskiy,Ankita Goel,Krzysztof Choromanski,Pan-Pan Jiang,Richard Nguyen,Four Flynn,Daria Ćurko,Peter Chen,Nicholas Roth,Kieran Milan,Caleb Habtegebriel,Shashi Narayan,Michael Moffitt,Jake Marcus,Thomas Anthony,Brendan McMahan,Gowoon Cheon,Ruibo Liu,Megan Barnes,Lukasz Lew,Rebeca Santamaria-Fernandez,Mayank Upadhyay,Arjun Akula,Arnar Mar Hrafnkelsson,Alvaro Caceres,Andrew Bunner,Michal Sokolik,Subha Puttagunta,Lawrence Moore,Berivan Isik,Weilun Chen,Jay Hartford,Lawrence Chan,Pradeep Shenoy,Dan Holtmann-Rice,Jane Park,Fabio Viola,Alex Salcianu,Sujeevan Rajayogam,Ian Stewart-Binks,Zelin Wu,Richard Everett,Xi Xiong,Pierre-Antoine Manzagol,Gary Leung,Carl Saroufim,Bo Pang,Dawid Wegner,George Papamakarios,Jennimaria Palomaki,Helena Pankov,Guangda Lai,Guilherme Tubone,Shubin Zhao,Theofilos Strinopoulos,Seth Neel,Mingqiu Wang,Joe Kelley,Li Li,Pingmei Xu,Anitha Vijayakumar,Andrea D'olimpio,Omer Levy,Massimo Nicosia,Grigory Rozhdestvenskiy,Ni Lao,Sirui Xie,Yash Katariya,Jon Simon,Sanjiv Kumar,Florian Hartmann,Michael Kilgore,Jinhyuk Lee,Aroma Mahendru,Roman Ring,Tom Hennigan,Fiona Lang,Colin Cherry,David Steiner,Dawsen Hwang,Ray Smith,Pidong Wang,Jeremy Chen,Ming-Hsuan Yang,Sam Kwei,Philippe Schlattner,Donnie Kim,Ganesh Poomal Girirajan,Nikola Momchev,Ayushi Agarwal,Xingyi Zhou,Ilkin Safarli,Zachary Garrett,AJ Pierigiovanni,Sarthak Jauhari,Alif Raditya Rochman,Shikhar Vashishth,Quan Yuan,Christof Angermueller,Jon Blanton,Xinying Song,Nitesh Bharadwaj Gundavarapu,Thi Avrahami,Maxine Deines,Subhrajit Roy,Manish Gupta,Christopher Semturs,Shobha Vasudevan,Aditya Srikanth Veerubhotla,Shriya Sharma,Josh Jacob,Zhen Yang,Andreas Terzis,Dan Karliner,Auriel Wright,Tania Rojas-Esponda,Ashley Brown,Abhijit Guha Roy,Pawan Dogra,Andrei Kapishnikov,Peter Young,Wendy Kan,Vinodh Kumar Rajendran,Maria Ivanova,Salil Deshmukh,Chia-Hua Ho,Mike Kwong,Stav Ginzburg,Annie Louis,KP Sawhney,Slav Petrov,Jing Xie,Yunfei Bai,Georgi Stoyanov,Alex Fabrikant,Rajesh Jayaram,Yuqi Li,Joe Heyward,Justin Gilmer,Yaqing Wang,Radu Soricut,Luyang Liu,Qingnan Duan,Jamie Hayes,Maura O'Brien,Gaurav Singh Tomar,Sivan Eiger,Bahar Fatemi,Jeffrey Hui,Catarina Barros,Adaeze Chukwuka,Alena Butryna,Saksham Thakur,Austin Huang,Zhufeng Pan,Haotian Tang,Serkan Cabi,Tulsee Doshi,Michiel Bakker,Sumit Bagri,Ruy Ley-Wild,Adam Lelkes,Jennie Lees,Patrick Kane,David Greene,Shimu Wu,Jörg Bornschein,Gabriela Surita,Sarah Hodkinson,Fangtao Li,Chris Hidey,Sébastien Pereira,Sean Ammirati,Phillip Lippe,Adam Kraft,Pu Han,Sebastian Gerlach,Zifeng Wang,Liviu Panait,Feng Han,Brian Farris,Yingying Bi,Hannah DeBalsi,Miaosen Wang,Gladys Tyen,James Cohan,Susan Zhang,Jarred Barber,Da-Woon Chung,Jaeyoun Kim,Markus Kunesch,Steven Pecht,Nami Akazawa,Abe Friesen,James Lyon,Ali Eslami,Junru Wu,Jie Tan,Yue Song,Ravi Kumar,Chris Welty,Ilia Akolzin,Gena Gibson,Sean Augenstein,Arjun Pillai,Nancy Yuen,Du Phan,Xin Wang,Iain Barr,Heiga Zen,Nan Hua,Casper Liu,Jilei,Wang,Tanuj Bhatia,Hao Xu,Oded Elyada,Pushmeet Kohli,Mirek Olšák,Ke Chen,Azalia Mirhoseini,Noam Shazeer,Shoshana Jakobovits,Maggie Tran,Nolan Ramsden,Tarun Bharti,Fred Alcober,Yunjie Li,Shilpa Shetty,Jing Chen,Dmitry Kalashnikov,Megha Nawhal,Sercan Arik,Hanwen Chen,Michiel Blokzijl,Shubham Gupta,James Rubin,Rigel Swavely,Sophie Bridgers,Ian Gemp,Chen Su,Arun Suggala,Juliette Pluto,Mary Cassin,Alain Vaucher,Kaiyang Ji,Jiahao Cai,Andrew Audibert,Animesh Sinha,David Tian,Efrat Farkash,Amy Hua,Jilin Chen,Duc-Hieu Tran,Edward Loper,Nicole Brichtova,Lara McConnaughey,Ballie Sandhu,Robert Leland,Doug DeCarlo,Andrew Over,James Huang,Xing Wu,Connie Fan,Eric Li,Yun Lei,Deepak Sharma,Cosmin Paduraru,Luo Yu,Matko Bošnjak,Phuong Dao,Min Choi,Sneha Kudugunta,Jakub Adamek,Carlos Guía,Ali Khodaei,Jie Feng,Wenjun Zeng,David Welling,Sandeep Tata,Christina Butterfield,Andrey Vlasov,Seliem El-Sayed,Swaroop Mishra,Tara Sainath,Shentao Yang,RJ Skerry-Ryan,Jeremy Shar,Robert Berry,Arunkumar Rajendran,Arun Kandoor,Andrea Burns,Deepali Jain,Tom Stone,Wonpyo Park,Shibo Wang,Albin Cassirer,Guohui Wang,Hayato Kobayashi,Sergey Rogulenko,Vineetha Govindaraj,Mikołaj Rybiński,Nadav Olmert,Colin Evans,Po-Sen Huang,Kelvin Xu,Premal Shah,Terry Thurk,Caitlin Sikora,Mu Cai,Jin Xie,Elahe Dabir,Saloni Shah,Norbert Kalb,Carrie Zhang,Shruthi Prabhakara,Amit Sabne,Artiom Myaskovsky,Vikas Raunak,Blanca Huergo,Behnam Neyshabur,Jon Clark,Ye Zhang,Shankar Krishnan,Eden Cohen,Dinesh Tewari,James Lottes,Yumeya Yamamori,Hui,Li,Mohamed Elhawaty,Ada Maksutaj Oflazer,Adrià Recasens,Sheryl Luo,Duy Nguyen,Taylor Bos,Kalyan Andra,Ana Salazar,Ed Chi,Jeongwoo Ko,Matt Ginsberg,Anders Andreassen,Anian Ruoss,Todor Davchev,Elnaz Davoodi,Chenxi Liu,Min Kim,Santiago Ontanon,Chi Ming To,Dawei Jia,Rosemary Ke,Jing Wang,Anna Korsun,Moran Ambar,Ilya Kornakov,Irene Giannoumis,Toni Creswell,Denny Zhou,Yi Su,Ishaan Watts,Aleksandr Zaks,Evgenii Eltyshev,Ziqiang Feng,Sidharth Mudgal,Alex Kaskasoli,Juliette Love,Kingshuk Dasgupta,Sam Shleifer,Richard Green,Sungyong Seo,Chansoo Lee,Dale Webster,Prakash Shroff,Ganna Raboshchuk,Isabel Leal,James Manyika,Sofia Erell,Daniel Murphy,Zhisheng Xiao,Anton Bulyenov,Julian Walker,Mark Collier,Matej Kastelic,Nelson George,Sushant Prakash,Sailesh Sidhwani,Alexey Frolov,Steven Hansen,Petko Georgiev,Tiberiu Sosea,Chris Apps,Aishwarya Kamath,David Reid,Emma Cooney,Charlotte Magister,Oriana Riva,Alec Go,Pu-Chin Chen,Sebastian Krause,Nir Levine,Marco Fornoni,Ilya Figotin,Nick Roy,Parsa Mahmoudieh,Vladimir Magay,Mukundan Madhavan,Jin Miao,Jianmo Ni,Yasuhisa Fujii,Ian Chou,George Scrivener,Zak Tsai,Siobhan Mcloughlin,Jeremy Selier,Sandra Lefdal,Jeffrey Zhao,Abhijit Karmarkar,Kushal Chauhan,Shivanker Goel,Zhaoyi Zhang,Vihan Jain,Parisa Haghani,Mostafa Dehghani,Jacob Scott,Erin Farnese,Anastasija Ilić,Steven Baker,Julia Pawar,Li Zhong,Josh Camp,Yoel Zeldes,Shravya Shetty,Anand Iyer,Vít Listík,Jiaxian Guo,Luming Tang,Mark Geller,Simon Bucher,Yifan Ding,Hongzhi Shi,Carrie Muir,Dominik Grewe,Ramy Eskander,Octavio Ponce,Boqing Gong,Derek Gasaway,Samira Khan,Umang Gupta,Angelos Filos,Weicheng Kuo,Klemen Kloboves,Jennifer Beattie,Christian Wright,Leon Li,Alicia Jin,Sandeep Mariserla,Miteyan Patel,Jens Heitkaemper,Dilip Krishnan,Vivek Sharma,David Bieber,Christian Frank,John Lambert,Paul Caron,Martin Polacek,Mai Giménez,Himadri Choudhury,Xing Yu,Sasan Tavakkol,Arun Ahuja,Franz Och,Rodolphe Jenatton,Wojtek Skut,Bryan Richter,David Gaddy,Andy Ly,Misha Bilenko,Megh Umekar,Ethan Liang,Martin Sevenich,Mandar Joshi,Hassan Mansoor,Rebecca Lin,Sumit Sanghai,Abhimanyu Singh,Xiaowei Li,Sudheendra Vijayanarasimhan,Zaheer Abbas,Yonatan Bitton,Hansa Srinivasan,Manish Reddy Vuyyuru,Alexander Frömmgen,Yanhua Sun,Ralph Leith,Alfonso Castaño,DJ Strouse,Le Yan,Austin Kyker,Satish Kambala,Mary Jasarevic,Thibault Sellam,Chao Jia,Alexander Pritzel,Raghavender R,Huizhong Chen,Natalie Clay,Sudeep Gandhe,Sean Kirmani,Sayna Ebrahimi,Hannah Kirkwood,Jonathan Mallinson,Chao Wang,Adnan Ozturel,Kuo Lin,Shyam Upadhyay,Vincent Cohen-Addad,Sean Purser-haskell,Yichong Xu,Ebrahim Songhori,Babi Seal,Alberto Magni,Almog Gueta,Tingting Zou,Guru Guruganesh,Thais Kagohara,Hung Nguyen,Khalid Salama,Alejandro Cruzado Ruiz,Justin Frye,Zhenkai Zhu,Matthias Lochbrunner,Simon Osindero,Wentao Yuan,Lisa Lee,Aman Prasad,Lam Nguyen Thiet,Daniele Calandriello,Victor Stone,Qixuan Feng,Han Ke,Maria Voitovich,Geta Sampemane,Lewis Chiang,Ling Wu,Alexander Bykovsky,Matt Young,Luke Vilnis,Ishita Dasgupta,Aditya Chawla,Qin Cao,Bowen Liang,Daniel Toyama,Szabolcs Payrits,Anca Stefanoiu,Dimitrios Vytiniotis,Ankesh Anand,Tianxiao Shen,Blagoj Mitrevski,Michael Tschannen,Sreenivas Gollapudi,Aishwarya P S,José Leal,Zhe Shen,Han Fu,Wei Wang,Arvind Kannan,Doron Kukliansky,Sergey Yaroshenko,Svetlana Grant,Umesh Telang,David Wood,Alexandra Chronopoulou,Alexandru Ţifrea,Tao Zhou,Tony,Nguy\~ên,Muge Ersoy,Anima Singh,Meiyan Xie,Emanuel Taropa,Woohyun Han,Eirikur Agustsson,Andrei Sozanschi,Hui Peng,Alex Chen,Yoel Drori,Efren Robles,Yang Gao,Xerxes Dotiwalla,Ying Chen,Anudhyan Boral,Alexei Bendebury,John Nham,Chris Tar,Luis Castro,Jiepu Jiang,Canoee Liu,Felix Halim,Jinoo Baek,Andy Wan,Jeremiah Liu,Yuan Cao,Shengyang Dai,Trilok Acharya,Ruoxi Sun,Fuzhao Xue,Saket Joshi,Morgane Lustman,Yongqin Xian,Rishabh Joshi,Deep Karkhanis,Nora Kassner,Jamie Hall,Xiangzhuo Ding,Gan Song,Gang Li,Chen Zhu,Yana Kulizhskaya,Bin Ni,Alexey Vlaskin,Solomon Demmessie,Lucio Dery,Salah Zaiem,Yanping Huang,Cindy Fan,Felix Gimeno,Ananth Balashankar,Koji Kojima,Hagai Taitelbaum,Maya Meng,Dero Gharibian,Sahil Singla,Wei Chen,Ambrose Slone,Guanjie Chen,Sujee Rajayogam,Max Schumacher,Suyog Kotecha,Rory Blevins,Qifei Wang,Mor Hazan Taege,Alex Morris,Xin Liu,Fayaz Jamil,Richard Zhang,Pratik Joshi,Ben Ingram,Tyler Liechty,Ahmed Eleryan,Scott Baird,Alex Grills,Gagan Bansal,Shan Han,Kiran Yalasangi,Shawn Xu,Majd Al Merey,Isabel Gao,Felix Weissenberger,Igor Karpov,Robert Riachi,Ankit Anand,Gautam Prasad,Kay Lamerigts,Reid Hayes,Jamie Rogers,Mandy Guo,Ashish Shenoy,Qiong,Hu,Kyle He,Yuchen Liu,Polina Zablotskaia,Sagar Gubbi,Yifan Chang,Jay Pavagadhi,Kristian Kjems,Archita Vadali,Diego Machado,Yeqing Li,Renshen Wang,Dipankar Ghosh,Aahil Mehta,Dana Alon,George Polovets,Alessio Tonioni,Nate Kushman,Joel D'sa,Lin Zhuo,Allen Wu,Rohin Shah,John Youssef,Jiayu Ye,Justin Snyder,Karel Lenc,Senaka Buthpitiya,Matthew Tung,Jichuan Chang,Tao Chen,David Saxton,Jenny Lee,Lydia Lihui Zhang,James Qin,Prabakar Radhakrishnan,Maxwell Chen,Piotr Ambroszczyk,Metin Toksoz-Exley,Yan Zhong,Nitzan Katz,Brendan O'Donoghue,Tamara von Glehn,Adi Gerzi Rosenthal,Aga Świetlik,Xiaokai Zhao,Nick Fernando,Jinliang Wei,Jieru Mei,Sergei Vassilvitskii,Diego Cedillo,Pranjal Awasthi,Hui Zheng,Koray Kavukcuoglu,Itay Laish,Joseph Pagadora,Marc Brockschmidt,Christopher A. Choquette-Choo,Arunkumar Byravan,Yifeng Lu,Xu Chen,Mia Chen,Kenton Lee,Rama Pasumarthi,Sijal Bhatnagar,Aditya Shah,Qiyin Wu,Zhuoyuan Chen,Zack Nado,Bartek Perz,Zixuan Jiang,David Kao,Ganesh Mallya,Nino Vieillard,Lantao Mei,Sertan Girgin,Mandy Jordan,Yeongil Ko,Alekh Agarwal,Yaxin Liu,Yasemin Altun,Raoul de Liedekerke,Anastasios Kementsietsidis,Daiyi Peng,Dangyi Liu,Utku Evci,Peter Humphreys,Austin Tarango,Xiang Deng,Yoad Lewenberg,Kevin Aydin,Chengda Wu,Bhavishya Mittal,Tsendsuren Munkhdalai,Kleopatra Chatziprimou,Rodrigo Benenson,Uri First,Xiao Ma,Jinning Li,Armand Joulin,Hamish Tomlinson,Tingnan Zhang,Milad Nasr,Zhi Hong,Michaël Sander,Lisa Anne Hendricks,Anuj Sharma,Andrew Bolt,Eszter Vértes,Jiri Simsa,Tomer Levinboim,Olcan Sercinoglu,Divyansh Shukla,Austin Wu,Craig Swanson,Danny Vainstein,Fan Bu,Bo Wang,Ryan Julian,Charles Yoon,Sergei Lebedev,Antonious Girgis,Bernd Bandemer,David Du,Todd Wang,Xi Chen,Ying Xiao,Peggy Lu,Natalie Ha,Vlad Ionescu,Simon Rowe,Josip Matak,Federico Lebron,Andreas Steiner,Lalit Jain,Manaal Faruqui,Nicolas Lacasse,Georgie Evans,Neesha Subramaniam,Dean Reich,Giulia Vezzani,Aditya Pandey,Joe Stanton,Tianhao Zhou,Liam McCafferty,Henry Griffiths,Verena Rieser,Soheil Hassas Yeganeh,Eleftheria Briakou,Lu Huang,Zichuan Wei,Liangchen Luo,Erik Jue,Gabby Wang,Victor Cotruta,Myriam Khan,Jongbin Park,Qiuchen Guo,Peiran Li,Rong Rong,Diego Antognini,Anastasia Petrushkina,Chetan Tekur,Eli Collins,Parul Bhatia,Chester Kwak,Wenhu Chen,Arvind Neelakantan,Immanuel Odisho,Sheng Peng,Vincent Nallatamby,Vaibhav Tulsyan,Fabian Pedregosa,Peng Xu,Raymond Lin,Yulong Wang,Emma Wang,Sholto Douglas,Reut Tsarfaty,Elena Gribovskaya,Renga Aravamudhan,Manu Agarwal,Mara Finkelstein,Qiao Zhang,Elizabeth Cole,Phil Crone,Sarmishta Velury,Anil Das,Chris Sauer,Luyao Xu,Danfeng Qin,Chenjie Gu,Dror Marcus,CJ Zheng,Wouter Van Gansbeke,Sobhan Miryoosefi,Haitian Sun,YaGuang Li,Charlie Chen,Jae Yoo,Pavel Dubov,Alex Tomala,Adams Yu,Paweł Wesołowski,Alok Gunjan,Eddie Cao,Jiaming Luo,Nikhil Sethi,Arkadiusz Socala,Laura Graesser,Tomas Kocisky,Arturo BC,Minmin Chen,Edward Lee,Sophie Wang,Weize Kong,Qiantong Xu,Nilesh Tripuraneni,Yiming Li,Xinxin Yu,Allen Porter,Paul Voigtlaender,Biao Zhang,Arpi Vezer,Sarah York,Qing Wei,Geoffrey Cideron,Mark Kurzeja,Seungyeon Kim,Benny Li,Angéline Pouget,Hyo Lee,Kaspar Daugaard,Yang Li,Dave Uthus,Aditya Siddhant,Paul Cavallaro,Sriram Ganapathy,Maulik Shah,Rolf Jagerman,Jeff Stanway,Piermaria Mendolicchio,Li Xiao,Kayi Lee,Tara Thompson,Shubham Milind Phal,Jason Chase,Sun Jae Lee,Adrian N Reyes,Disha Shrivastava,Zhen Qin,Roykrong Sukkerd,Seth Odoom,Lior Madmoni,John Aslanides,Jonathan Herzig,Elena Pochernina,Sheng Zhang,Parker Barnes,Daisuke Ikeda,Qiujia Li,Shuo-yiin Chang,Shakir Mohamed,Jim Sproch,Richard Powell,Bidisha Samanta,Domagoj Ćevid,Anton Kovsharov,Shrestha Basu Mallick,Srinivas Tadepalli,Anne Zheng,Kareem Ayoub,Andreas Noever,Christian Reisswig,Zhuo Xu,Junhyuk Oh,Martin Matysiak,Tim Blyth,Shereen Ashraf,Julien Amelot,Boone Severson,Michele Bevilacqua,Motoki Sano,Ethan Dyer,Ofir Roval,Anu Sinha,Yin Zhong,Sagi Perel,Tea Sabolić,Johannes Mauerer,Willi Gierke,Mauro Verzetti,Rodrigo Cabrera,Alvin Abdagic,Steven Hemingray,Austin Stone,Jong Lee,Farooq Ahmad,Karthik Raman,Lior Shani,Jonathan Lai,Orhan Firat,Nathan Waters,Eric Ge,Mo Shomrat,Himanshu Gupta,Rajeev Aggarwal,Tom Hudson,Bill Jia,Simon Baumgartner,Palak Jain,Joe Kovac,Junehyuk Jung,Ante Žužul,Will Truong,Morteza Zadimoghaddam,Songyou Peng,Marco Liang,Rachel Sterneck,Balaji Lakshminarayanan,Machel Reid,Oliver Woodman,Tong Zhou,Jianling Wang,Vincent Coriou,Arjun Narayanan,Jay Hoover,Yenai Ma,Apoorv Jindal,Clayton Sanford,Doug Reid,Swaroop Ramaswamy,Alex Kurakin,Roland Zimmermann,Yana Lunts,Dragos Dena,Zalán Borsos,Vered Cohen,Shujian Zhang,Will Grathwohl,Robert Dadashi,Morgan Redshaw,Joshua Kessinger,Julian Odell,Silvano Bonacina,Zihang Dai,Grace Chen,Ayush Dubey,Pablo Sprechmann,Mantas Pajarskas,Wenxuan Zhou,Niharika Ahuja,Tara Thomas,Martin Nikoltchev,Matija Kecman,Bharath Mankalale,Andrey Ryabtsev,Jennifer She,Christian Walder,Jiaming Shen,Lu Li,Carolina Parada,Sheena Panthaplackel,Okwan Kwon,Matt Lawlor,Utsav Prabhu,Yannick Schroecker,Marc'aurelio Ranzato,Pete Blois,Iurii Kemaev,Ting Yu,Dmitry,Lepikhin,Hao Xiong,Sahand Sharifzadeh,Oleaser Johnson,Jeremiah Willcock,Rui Yao,Greg Farquhar,Sujoy Basu,Hidetoshi Shimokawa,Nina Anderson,Haiguang Li,Khiem Pham,Yizhong Liang,Sebastian Borgeaud,Alexandre Moufarek,Hideto Kazawa,Blair Kutzman,Marcin Sieniek,Sara Smoot,Ruth Wang,Natalie Axelsson,Nova Fallen,Prasha Sundaram,Yuexiang Zhai,Varun Godbole,Petros Maniatis,Alek Wang,Ilia Shumailov,Santhosh Thangaraj,Remi Crocker,Nikita Gupta,Gang Wu,Phil Chen,Gellért Weisz,Celine Smith,Mojtaba Seyedhosseini,Boya Fang,Xiyang Luo,Roey Yogev,Zeynep Cankara,Andrew Hard,Helen Ran,Rahul Sukthankar,George Necula,Gaël Liu,Honglong Cai,Praseem Banzal,Daniel Keysers,Sanjay Ghemawat,Connie Tao,Emma Dunleavy,Aditi Chaudhary,Wei Li,Maciej Mikuła,Chen-Yu Lee,Tiziana Refice,Krishna Somandepalli,Alexandre Fréchette,Dan Bahir,John Karro,Keith Rush,Sarah Perrin,Bill Rosgen,Xiaomeng Yang,Clara Huiyi Hu,Mahmoud Alnahlawi,Justin Mao-Jones,Roopal Garg,Hoang Nguyen,Bat-Orgil Batsaikhan,Iñaki Iturrate,Anselm Levskaya,Avi Singh,Ashyana Kachra,Tony Lu,Denis Petek,Zheng Xu,Mark Graham,Lukas Zilka,Yael Karov,Marija Kostelac,Fangyu Liu,Yaohui Guo,Weiyue Wang,Bernd Bohnet,Emily Pitler,Tony Bruguier,Keisuke Kinoshita,Chrysovalantis Anastasiou,Nilpa Jha,Ting Liu,Jerome Connor,Phil Wallis,Philip Pham,Eric Bailey,Shixin Li,Heng-Tze Cheng,Sally Ma,Haiqiong Li,Akanksha Maurya,Kate Olszewska,Manfred Warmuth,Christy Koh,Dominik Paulus,Siddhartha Reddy Jonnalagadda,Enrique Piqueras,Ali Elqursh,Geoff Brown,Hadar Shemtov,Loren Maggiore,Fei Xia,Ryan Foley,Beka Westberg,George van den Driessche,Livio Baldini Soares,Arjun Kar,Michael Quinn,Siqi Zuo,Jialin Wu,Kyle Kastner,Anna Bortsova,Aijun Bai,Ales Mikhalap,Luowei Zhou,Jennifer Brennan,Vinay Ramasesh,Honglei Zhuang,John Maggs,Johan Schalkwyk,Yuntao Xu,Hui Huang,Andrew Howard,Sasha Brown,Linting Xue,Gloria Shen,Brian Albert,Neha Jha,Daniel Zheng,Varvara Krayvanova,Spurthi Amba Hombaiah,Olivier Lacombe,Gautam Vasudevan,Dan Graur,Tian Xie,Meet Gandhi,Bangju Wang,Dustin Zelle,Harman Singh,Dahun Kim,Sébastien Cevey,Victor Ungureanu,Natasha Noy,Fei Liu,Annie Xie,Fangxiaoyu Feng,Katerina Tsihlas,Daniel Formoso,Neera Vats,Quentin Wellens,Yinan Wang,Niket Kumar Bhumihar,Samrat Ghosh,Matt Hoffman,Tom Lieber,Oran Lang,Kush Bhatia,Tom Paine,Aroonalok Pyne,Ronny Votel,Madeleine Clare Elish,Benoit Schillings,Alex Panagopoulos,Haichuan Yang,Adam Raveret,Zohar Yahav,Shuang Liu,Warren Chen,Dalia El Badawy,Nishant Agrawal,Mohammed Badawi,Mahdi Mirzazadeh,Carla Bromberg,Fan Ye,Chang Liu,Tatiana Sholokhova,George-Cristian Muraru,Gargi Balasubramaniam,Jonathan Malmaud,Alen Carin,Danilo Martins,Irina Jurenka,Pankil Botadra,Dave Lacey,Richa Singh,Mariano Schain,Dan Zheng,Isabelle Guyon,Victor Lavrenko,Seungji Lee,Xiang Zhou,Demis Hassabis,Jeshwanth Challagundla,Derek Cheng,Nikhil Mehta,Matthew Mauger,Michela Paganini,Pushkar Mishra,Kate Lee,Zhang Li,Lexi Baugher,Ondrej Skopek,Max Chang,Amir Zait,Gaurav Menghani,Lizzetth Bellot,Guangxing Han,Jean-Michel Sarr,Sharat Chikkerur,Himanshu Sahni,Rohan Anil,Arun Narayanan,Chandu Thekkath,Daniele Pighin,Hana Strejček,Marko Velic,Fred Bertsch,Manuel Tragut,Keran Rong,Alicia Parrish,Kai Bailey,Jiho Park,Isabela Albuquerque,Abhishek Bapna,Rajesh Venkataraman,Alec Kosik,Johannes Griesser,Zhiwei Deng,Alek Andreev,Qingyun Dou,Kevin Hui,Fanny Wei,Xiaobin Yu,Lei Shu,Avia Aharon,David Barker,Badih Ghazi,Sebastian Flennerhag,Chris Breaux,Yuchuan Liu,Matthew Bilotti,Josh Woodward,Uri Alon,Stephanie Winkler,Tzu-Kuo Huang,Kostas Andriopoulos,João Gabriel Oliveira,Penporn Koanantakool,Berkin Akin,Michael Wunder,Cicero Nogueira dos Santos,Mohammad Hossein Bateni,Lin Yang,Dan Horgan,Beer Changpinyo,Keyvan Amiri,Min Ma,Dayeong Lee,Lihao Liang,Anirudh Baddepudi,Tejasi Latkar,Raia Hadsell,Jun Xu,Hairong Mu,Michael Han,Aedan Pope,Snchit Grover,Frank Kim,Ankit Bhagatwala,Guan Sun,Yamini Bansal,Amir Globerson,Alireza Nazari,Samira Daruki,Hagen Soltau,Jane Labanowski,Laurent El Shafey,Matt Harvey,Yanif Ahmad,Elan Rosenfeld,William Kong,Etienne Pot,Yi-Xuan Tan,Aurora Wei,Victoria Langston,Marcel Prasetya,Petar Veličković,Richard Killam,Robin Strudel,Darren Ni,Zhenhai Zhu,Aaron Archer,Kavya Kopparapu,Lynn Nguyen,Emilio Parisotto,Hussain Masoom,Sravanti Addepalli,Jordan Grimstad,Hexiang Hu,Joss Moore,Avinatan Hassidim,Le Hou,Mukund Raghavachari,Jared Lichtarge,Adam R. Brown,Hilal Dib,Natalia Ponomareva,Justin Fu,Yujing Zhang,Altaf Rahman,Joana Iljazi,Edouard Leurent,Gabriel Dulac-Arnold,Cosmo Du,Chulayuth Asawaroengchai,Larry Jin,Ela Gruzewska,Ziwei Ji,Benigno Uria,Daniel De Freitas,Paul Barham,Lauren Beltrone,Víctor Campos,Jun Yan,Neel Kovelamudi,Arthur Nguyen,Elinor Davies,Zhichun Wu,Zoltan Egyed,Kristina Toutanova,Nithya Attaluri,Hongliang Fei,Peter Stys,Siddhartha Brahma,Martin Izzard,Siva Velusamy,Scott Lundberg,Vincent Zhuang,Kevin Sequeira,Adam Santoro,Ehsan Amid,Ophir Aharoni,Shuai Ye,Mukund Sundararajan,Lijun Yu,Yu-Cheng Ling,Stephen Spencer,Hugo Song,Josip Djolonga,Christo Kirov,Sonal Gupta,Alessandro Bissacco,Clemens Meyer,Mukul Bhutani,Andrew Dai,Weiyi Wang,Siqi Liu,Ashwin Sreevatsa,Qijun Tan,Maria Wang,Lucy Kim,Yicheng Wang,Alex Irpan,Yang Xiao,Stanislav Fort,Yifan He,Alex Gurney,Bryan Gale,Yue Ma,Monica Roy,Viorica Patraucean,Taylan Bilal,Golnaz Ghiasi,Anahita Hosseini,Melvin Johnson,Zhuowan Li,Yi Tay,Benjamin Beyret,Katie Millican,Josef Broder,Mayank Lunayach,Danny Swisher,Eugen Vušak,David Parkinson,MH Tessler,Adi Mayrav Gilady,Richard Song,Allan Dafoe,Yves Raimond,Masa Yamaguchi,Itay Karo,Elizabeth Nielsen,Kevin Kilgour,Mike Dusenberry,Rajiv Mathews,Jiho Choi,Siyuan Qiao,Harsh Mehta,Sahitya Potluri,Chris Knutsen,Jialu Liu,Tat Tan,Kuntal Sengupta,Keerthana Gopalakrishnan,Abodunrinwa Toki,Mencher Chiang,Mike Burrows,Grace Vesom,Zafarali Ahmed,Ilia Labzovsky,Siddharth Vashishtha,Preeti Singh,Ankur Sharma,Ada Ma,Jinyu Xie,Pranav Talluri,Hannah Forbes-Pollard,Aarush Selvan,Joel Wee,Loic Matthey,Tom Funkhouser,Parthasarathy Gopavarapu,Lev Proleev,Cheng Li,Matt Thomas,Kashyap Kolipaka,Zhipeng Jia,Ashwin Kakarla,Srinivas Sunkara,Joan Puigcerver,Suraj Satishkumar Sheth,Emily Graves,Chen Wang,Sadh MNM Khan,Kai Kang,Shyamal Buch,Fred Zhang,Omkar Savant,David Soergel,Kevin Lee,Linda Friso,Xuanyi Dong,Rahul Arya,Shreyas Chandrakaladharan,Connor Schenck,Greg Billock,Tejas Iyer,Anton Bakalov,Leslie Baker,Alex Ruiz,Angad Chandorkar,Trieu Trinh,Matt Miecnikowski,Yanqi Zhou,Yangsibo Huang,Jiazhong Nie,Ali Shah,Ashish Thapliyal,Sam Haves,Lun Wang,Uri Shaham,Patrick Morris-Suzuki,Soroush Radpour,Leonard Berrada,Thomas Strohmann,Chaochao Yan,Jingwei Shen,Sonam Goenka,Tris Warkentin,Petar Dević,Dan Belov,Albert Webson,Madhavi Yenugula,Puranjay Datta,Jerry Chang,Nimesh Ghelani,Aviral Kumar,Vincent Perot,Jessica Lo,Yang Song,Herman Schmit,Jianmin Chen,Vasilisa Bashlovkina,Xiaoyue Pan,Diana Mincu,Paul Roit,Isabel Edkins,Andy Davis,Yujia Li,Ben Horn,Xinjian Li,Pradeep Kumar S,Eric Doi,Wanzheng Zhu,Sri Gayatri Sundara Padmanabhan,Siddharth Verma,Jasmine Liu,Heng Chen,Mihajlo Velimirović,Malcolm Reynolds,Priyanka Agrawal,Nick Sukhanov,Abhinit Modi,Siddharth Goyal,John Palowitch,Nima Khajehnouri,Wing Lowe,David Klinghoffer,Sharon Silver,Vinh Tran,Candice Schumann,Francesco Piccinno,Xi Liu,Mario Lučić,Xiaochen Yang,Sandeep Kumar,Ajay Kannan,Ragha Kotikalapudi,Mudit Bansal,Fabian Fuchs,Javad Hosseini,Abdelrahman Abdelhamed,Dawn Bloxwich,Tianhe Yu,Ruoxin Sang,Gregory Thornton,Karan Gill,Yuchi Liu,Virat Shejwalkar,Jason Lin,Zhipeng Yan,Kehang Han,Thomas Buschmann,Michael Pliskin,Zhi Xing,Susheel Tatineni,Junlin Zhang,Sissie Hsiao,Gavin Buttimore,Marcus Wu,Zefei Li,Geza Kovacs,Legg Yeung,Tao Huang,Aaron Cohen,Bethanie Brownfield,Averi Nowak,Mikel Rodriguez,Tianze Shi,Hado van Hasselt,Kevin Cen,Deepanway Ghoshal,Kushal Majmundar,Weiren Yu,Warren,Chen,Danila Sinopalnikov,Hao Zhang,Vlado Galić,Di Lu,Zeyu Zheng,Maggie Song,Gary Wang,Gui Citovsky,Swapnil Gawde,Isaac Galatzer-Levy,David Silver,Ivana Balazevic,Dipanjan Das,Kingshuk Majumder,Yale Cong,Praneet Dutta,Dustin Tran,Hui Wan,Junwei Yuan,Daniel Eppens,Alanna Walton,Been Kim,Harry Ragan,James Cobon-Kerr,Lu Liu,Weijun Wang,Bryce Petrini,Jack Rae,Rakesh Shivanna,Yan Xiong,Chace Lee,Pauline Coquinot,Yiming Gu,Lisa Patel,Blake Hechtman,Aviel Boag,Orion Jankowski,Alex Wertheim,Alex Lee,Paul Covington,Hila Noga,Sam Sobell,Shanthal Vasanth,William Bono,Chirag Nagpal,Wei Fan,Xavier Garcia,Kedar Soparkar,Aybuke Turker,Nathan Howard,Sachit Menon,Yuankai Chen,Vikas Verma,Vladimir Pchelin,Harish Rajamani,Valentin Dalibard,Ana Ramalho,Yang Guo,Kartikeya Badola,Seojin Bang,Nathalie Rauschmayr,Julia Proskurnia,Sudeep Dasari,Xinyun Chen,Mikhail Sushkov,Anja Hauth,Pauline Sho,Abhinav Singh,Bilva Chandra,Allie Culp,Max Dylla,Olivier Bachem,James Besley,Heri Zhao,Timothy Lillicrap,Wei Wei,Wael Al Jishi,Ning Niu,Alban Rrustemi,Raphaël Lopez Kaufman,Ryan Poplin,Jewel Zhao,Minh Truong,Shikhar Bharadwaj,Ester Hlavnova,Eli Stickgold,Cordelia Schmid,Georgi Stephanov,Zhaoqi Leng,Frederick Liu,Léonard Hussenot,Shenil Dodhia,Juliana Vicente Franco,Lesley Katzen,Abhanshu Sharma,Sarah Cogan,Zuguang Yang,Aniket Ray,Sergi Caelles,Shen Yan,Ravin Kumar,Daniel Gillick,Renee Wong,Joshua Ainslie,Jonathan Hoech,Séb Arnold,Dan Abolafia,Anca Dragan,Ben Hora,Grace Hu,Alexey Guseynov,Yang Lu,Chas Leichner,Jinmeng Rao,Abhimanyu Goyal,Nagabhushan Baddi,Daniel Hernandez Diaz,Tim McConnell,Max Bain,Jake Abernethy,Qiqi Yan,Rylan Schaeffer,Paul Vicol,Will Thompson,Montse Gonzalez Arenas,Mathias Bellaiche,Pablo Barrio,Stefan Zinke,Riccardo Patana,Pulkit Mehta,JK Kearns,Avraham Ruderman,Scott Pollom,David D'Ambrosio,Cath Hope,Yang Yu,Andrea Gesmundo,Kuang-Huei Lee,Aviv Rosenberg,Yiqian Zhou,Yaoyiran Li,Drew Garmon,Yonghui Wu,Safeen Huda,Gil Fidel,Martin Baeuml,Jian Li,Phoebe Kirk,Rhys May,Tao Tu,Sara Mc Carthy,Toshiyuki Fukuzawa,Miranda Aperghis,Chih-Kuan Yeh,Toshihiro Yoshino,Bo Li,Austin Myers,Kaisheng Yao,Ben Limonchik,Changwan Ryu,Rohun Saxena,Alex Goldin,Ruizhe Zhao,Rocky Rhodes,Tao Zhu,Divya Tyam,Heidi Howard,Nathan Byrd,Hongxu Ma,Yan Wu,Ryan Mullins,Qingze Wang,Aida Amini,Sebastien Baur,Yiran Mao,Subhashini Venugopalan,Will Song,Wen Ding,Paul Collins,Sashank Reddi,Megan Shum,Andrei Rusu,Luisa Zintgraf,Kelvin Chan,Sheela Goenka,Mathieu Blondel,Michael Collins,Renke Pan,Marissa Giustina,Nikolai Chinaev,Christian Schuler,Ce Zheng,Jonas Valfridsson,Alyssa Loo,Alex Yakubovich,Jamie Smith,Tao Jiang,Rich Munoz,Gabriel Barcik,Rishabh Bansal,Mingyao Yang,Yilun Du,Pablo Duque,Mary Phuong,Alexandra Belias,Kunal Lad,Zeyu Liu,Tal Schuster,Karthik Duddu,Jieru Hu,Paige Kunkle,Matthew Watson,Jackson Tolins,Josh Smith,Denis Teplyashin,Garrett Bingham,Marvin Ritter,Marco Andreetto,Divya Pitta,Mohak Patel,Shashank Viswanadha,Trevor Strohman,Catalin Ionescu,Jincheng Luo,Yogesh Kalley,Jeremy Wiesner,Dan Deutsch,Derek Lockhart,Peter Choy,Rumen Dangovski,Chawin Sitawarin,Cat Graves,Tanya Lando,Joost van Amersfoort,Ndidi Elue,Zhouyuan Huo,Pooya Moradi,Jean Tarbouriech,Henryk Michalewski,Wenting Ye,Eunyoung Kim,Alex Druinsky,Florent Altché,Xinyi Chen,Artur Dwornik,Da-Cheng Juan,Rivka Moroshko,Horia Toma,Jarrod Kahn,Hai Qian,Maximilian Sieb,Irene Cai,Roman Goldenberg,Praneeth Netrapalli,Sindhu Raghuram,Yuan Gong,Lijie Fan,Evan Palmer,Yossi Matias,Valentin Gabeur,Shreya Pathak,Tom Ouyang,Don Metzler,Geoff Bacon,Srinivasan Venkatachary,Sridhar Thiagarajan,Alex Cullum,Eran Ofek,Vytenis Sakenas,Mohamed Hammad,Cesar Magalhaes,Mayank Daswani,Oscar Chang,Ashok Popat,Ruichao Li,Komal Jalan,Yanhan Hou,Josh Lipschultz,Antoine He,Wenhao Jia,Pier Giuseppe Sessa,Prateek Kolhar,William Wong,Sumeet Singh,Lukas Haas,Jay Whang,Hanna Klimczak-Plucińska,Georges Rotival,Grace Chung,Yiqing Hua,Anfal Siddiqui,Nicolas Serrano,Dongkai Chen,Billy Porter,Libin Bai,Keshav Shivam,Sho Arora,Partha Talukdar,Tom Cobley,Sangnie Bhardwaj,Evgeny Gladchenko,Simon Green,Kelvin Guu,Felix Fischer,Xiao Wu,Eric Wang,Achintya Singhal,Tatiana Matejovicova,James Martens,Hongji Li,Roma Patel,Elizabeth Kemp,Jiaqi Pan,Lily Wang,Blake JianHang Chen,Jean-Baptiste Alayrac,Navneet Potti,Erika Gemzer,Eugene Ie,Kay McKinney,Takaaki Saeki,Edward Chou,Pascal Lamblin,SQ Mah,Zach Fisher,Martin Chadwick,Jon Stritar,Obaid Sarvana,Andrew Hogue,Artem Shtefan,Hadi Hashemi,Yang Xu,Jindong Gu,Sharad Vikram,Chung-Ching Chang,Sabela Ramos,Logan Kilpatrick,Weijuan Xi,Jenny Brennan,Yinghao Sun,Abhishek Jindal,Ionel Gog,Dawn Chen,Felix Wu,Jason Lee,Sudhindra Kopalle,Srinadh Bhojanapalli,Oriol Vinyals,Natan Potikha,Burcu Karagol Ayan,Yuan Yuan,Michael Riley,Piotr Stanczyk,Sergey Kishchenko,Bing Wang,Dan Garrette,Antoine Yang,Vlad Feinberg,CJ Carey,Javad Azizi,Viral Shah,Erica Moreira,Chongyang Shi,Josh Feldman,Elizabeth Salesky,Thomas Lampe,Aneesh Pappu,Duhyeon Kim,Jonas Adler,Avi Caciularu,Brian Walker,Yunhan Xu,Yochai Blau,Dylan Scandinaro,Terry Huang,Sam El-Husseini,Abhishek Sinha,Lijie Ren,Taylor Tobin,Patrik Sundberg,Tim Sohn,Vikas Yadav,Mimi Ly,Emily Xue,Jing Xiong,Afzal Shama Soudagar,Sneha Mondal,Nikhil Khadke,Qingchun Ren,Ben Vargas,Stan Bileschi,Sarah Chakera,Cindy Wang,Boyu Wang,Yoni Halpern,Joe Jiang,Vikas Sindhwani,Petre Petrov,Pranavaraj Ponnuramu,Sanket Vaibhav Mehta,Yu Watanabe,Betty Chan,Matheus Wisniewski,Trang Pham,Jingwei Zhang,Conglong Li,Dario de Cesare,Art Khurshudov,Alex Vasiloff,Melissa Tan,Zoe Ashwood,Bobak Shahriari,Maryam Majzoubi,Garrett Tanzer,Olga Kozlova,Robin Alazard,James Lee-Thorp,Nguyet Minh Phu,Isaac Tian,Junwhan Ahn,Andy Crawford,Lauren Lax,Yuan,Shangguan,Iftekhar Naim,David Ross,Oleksandr Ferludin,Tongfei Guo,Andrea Banino,Hubert Soyer,Xiaoen Ju,Dominika Rogozińska,Ishaan Malhi,Marcella Valentine,Daniel Balle,Apoorv Kulshreshtha,Maciej Kula,Yiwen Song,Sophia Austin,John Schultz,Roy Hirsch,Arthur Douillard,Apoorv Reddy,Michael Fink,Summer Yue,Khyatti Gupta,Adam Zhang,Norman Rink,Daniel McDuff,Lei Meng,András György,Yasaman Razeghi,Ricky Liang,Kazuki Osawa,Aviel Atias,Matan Eyal,Tyrone Hill,Nikolai Grigorev,Zhengdong Wang,Nitish Kulkarni,Rachel Soh,Ivan Lobov,Zachary Charles,Sid Lall,Kazuma Hashimoto,Ido Kessler,Victor Gomes,Zelda Mariet,Danny Driess,Alessandro Agostini,Canfer Akbulut,Jingcao Hu,Marissa Ikonomidis,Emily Caveness,Kartik Audhkhasi,Saurabh Agrawal,Ioana Bica,Evan Senter,Jayaram Mudigonda,Kelly Chen,Jingchen Ye,Xuanhui Wang,James Svensson,Philipp Fränken,Josh Newlan,Li Lao,Eva Schnider,Sami Alabed,Joseph Kready,Jesse Emond,Afief Halumi,Tim Zaman,Chengxi Ye,Naina Raisinghani,Vilobh Meshram,Bo Chang,Ankit Singh Rawat,Axel Stjerngren,Sergey Levi,Rui Wang,Xiangzhu Long,Mitchelle Rasquinha,Steven Hand,Aditi Mavalankar,Lauren Agubuzu,Sudeshna Roy,Junquan Chen,Jarek Wilkiewicz,Hao Zhou,Michal Jastrzebski,Qiong Hu,Agustin Dal Lago,Ramya Sree Boppana,Wei-Jen Ko,Jennifer Prendki,Yao Su,Zhi Li,Eliza Rutherford,Girish Ramchandra Rao,Ramona Comanescu,Adrià Puigdomènech,Qihang Chen,Dessie Petrova,Christine Chan,Vedrana Milutinovic,Felipe Tiengo Ferreira,Chin-Yi Cheng,Ming Zhang,Tapomay Dey,Sherry Yang,Ramesh Sampath,Quoc Le,Howard Zhou,Chu-Cheng Lin,Hoi Lam,Christine Kaeser-Chen,Kai Hui,Dean Hirsch,Tom Eccles,Basil Mustafa,Shruti Rijhwani,Morgane Rivière,Yuanzhong Xu,Junjie Wang,Xinyang Geng,Xiance Si,Arjun Khare,Cheolmin Kim,Vahab Mirrokni,Kamyu Lee,Khuslen Baatarsukh,Nathaniel Braun,Lisa Wang,Pallavi LV,Richard Tanburn,Yuvein,Zhu,Fangda Li,Setareh Ariafar,Dan Goldberg,Ken Burke,Daniil Mirylenka,Meiqi Guo,Olaf Ronneberger,Hadas Natalie Vogel,Liqun Cheng,Nishita Shetty,Johnson Jia,Thomas Jimma,Corey Fry,Ted Xiao,Martin Sundermeyer,Ryan Burnell,Yannis Assael,Mario Pinto,JD Chen,Rohit Sathyanarayana,Donghyun Cho,Jing Lu,Rishabh Agarwal,Sugato Basu,Lucas Gonzalez,Dhruv Shah,Meng Wei,Dre Mahaarachchi,Rohan Agrawal,Tero Rissa,Yani Donchev,Ramiro Leal-Cavazos,Adrian Hutter,Markus Mircea,Alon Jacovi,Faruk Ahmed,Jiageng Zhang,Shuguang Hu,Bo-Juen Chen,Jonni Kanerva,Guillaume Desjardins,Andrew Lee,Nikos Parotsidis,Asier Mujika,Tobias Weyand,Jasper Snoek,Jo Chick,Kai Chen,Paul Chang,Ethan Mahintorabi,Zi Wang,Tolly Powell,Orgad Keller,Abhirut Gupta,Claire Sha,Kanav Garg,Nicolas Heess,Ágoston Weisz,Cassidy Hardin,Bartek Wydrowski,Ben Coleman,Karina Zainullina,Pankaj Joshi,Alessandro Epasto,Terry Spitz,Binbin Xiong,Kai Zhao,Arseniy Klimovskiy,Ivy Zheng,Johan Ferret,Itay Yona,Waleed Khawaja,Jean-Baptiste Lespiau,Maxim Krikun,Siamak Shakeri,Timothee Cour,Bonnie Li,Igor Krivokon,Dan Suh,Alex Hofer,Jad Al Abdallah,Nikita Putikhin,Oscar Akerlund,Silvio Lattanzi,Anurag Kumar,Shane Settle,Himanshu Srivastava,Folawiyo Campbell-Ajala,Edouard Rosseel,Mihai Dorin Istin,Nishanth Dikkala,Anand Rao,Nick Young,Kate Lin,Dhruva Bhaswar,Yiming Wang,Jaume Sanchez Elias,Kritika Muralidharan,James Keeling,Dayou Du,Siddharth Gopal,Gregory Dibb,Charles Blundell,Manolis Delakis,Jacky Liang,Marco Tulio Ribeiro,Georgi Karadzhov,Guillermo Garrido,Ankur Bapna,Jiawei Cao,Adam Sadovsky,Pouya Tafti,Arthur Guez,Coline Devin,Yixian Di,Jinwei Xing,Chuqiao,Xu,Hanzhao Lin,Chun-Te Chu,Sameera Ponda,Wesley Helmholz,Fan Yang,Yue Gao,Sara Javanmardi,Wael Farhan,Alex Ramirez,Ricardo Figueira,Khe Chai Sim,Yuval Bahat,Ashwin Vaswani,Liangzhe Yuan,Gufeng Zhang,Leland Rechis,Hanjun Dai,Tayo Oguntebi,Alexandra Cordell,Eugénie Rives,Kaan Tekelioglu,Naveen Kumar,Bing Zhang,Aurick Zhou,Nikolay Savinov,Andrew Leach,Alex Tudor,Sanjay Ganapathy,Yanyan Zheng,Mirko Rossini,Vera Axelrod,Arnaud Autef,Yukun Zhu,Zheng Zheng,Mingda Zhang,Baochen Sun,Jie Ren,Nenad Tomasev,Nithish Kannan,Amer Sinha,Charles Chen,Louis O'Bryan,Alex Pak,Aditya Kusupati,Weel Yang,Deepak Ramachandran,Patrick Griffin,Seokhwan Kim,Philipp Neubeck,Craig Schiff,Tammo Spalink,Mingyang Ling,Arun Nair,Ga-Young Joung,Linda Deng,Avishkar Bhoopchand,Lora Aroyo,Tom Duerig,Jordan Griffith,Gabe Barth-Maron,Jake Ades,Alex Haig,Ankur Taly,Yunting Song,Paul Michel,Dave Orr,Dean Weesner,Corentin Tallec,Carrie Grimes Bostock,Paul Niemczyk,Andy Twigg,Mudit Verma,Rohith Vallu,Henry Wang,Marco Gelmi,Kiranbir Sodhia,Aleksandr Chuklin,Omer Goldman,Jasmine George,Liang Bai,Kelvin Zhang,Petar Sirkovic,Efrat Nehoran,Golan Pundak,Jiaqi Mu,Alice Chen,Alex Greve,Paulo Zacchello,David Amos,Heming Ge,Eric Noland,Colton Bishop,Jeffrey Dudek,Youhei Namiki,Elena Buchatskaya,Jing Li,Dorsa Sadigh,Masha Samsikova,Dan Malkin,Damien Vincent,Robert David,Rob Willoughby,Phoenix Meadowlark,Shawn Gao,Yan Li,Raj Apte,Amit Jhindal,Stein Xudong Lin,Alex Polozov,Zhicheng Wang,Tomas Mery,Anirudh GP,Varun Yerram,Sage Stevens,Tianqi Liu,Noah Fiedel,Charles Sutton,Matthew Johnson,Xiaodan Song,Kate Baumli,Nir Shabat,Muqthar Mohammad,Hao Liu,Marco Selvi,Yichao Zhou,Mehdi Hafezi Manshadi,Chu-ling Ko,Anthony Chen,Michael Bendersky,Jorge Gonzalez Mendez,Nisarg Kothari,Amir Zandieh,Yiling Huang,Daniel Andor,Ellie Pavlick,Idan Brusilovsky,Jitendra Harlalka,Sally Goldman,Andrew Lampinen,Guowang Li,Asahi Ushio,Somit Gupta,Lei Zhang,Chuyuan Kelly Fu,Madhavi Sewak,Timo Denk,Jed Borovik,Brendan Jou,Avital Zipori,Prateek Jain,Junwen Bai,Thang Luong,Jonathan Tompson,Alice Li,Li Liu,George Powell,Jiajun Shen,Alex Feng,Grishma Chole,Da Yu,Yinlam Chow,Tongxin Yin,Eric Malmi,Kefan Xiao,Yash Pande,Shachi Paul,Niccolò Dal Santo,Adil Dostmohamed,Sergio Guadarrama,Aaron Phillips,Thanumalayan Sankaranarayana Pillai,Gal Yona,Amin Ghafouri,Preethi Lahoti,Benjamin Lee,Dhruv Madeka,Eren Sezener,Simon Tokumine,Adrian Collister,Nicola De Cao,Richard Shin,Uday Kalra,Parker Beak,Emily Nottage,Ryo Nakashima,Ivan Jurin,Vikash Sehwag,Meenu Gaba,Junhao Zeng,Kevin R. McKee,Fernando Pereira,Tamar Yakar,Amayika Panda,Arka Dhar,Peilin Zhong,Daniel Sohn,Mark Brand,Lars Lowe Sjoesund,Viral Carpenter,Sharon Lin,Shantanu Thakoor,Marcus Wainwright,Ashwin Chaugule,Pranesh Srinivasan,Muye Zhu,Bernett Orlando,Jack Weber,Ayzaan Wahid,Gilles Baechler,Apurv Suman,Jovana Mitrović,Gabe Taubman,Honglin Yu,Helen King,Josh Dillon,Cathy Yip,Dhriti Varma,Tomas Izo,Levent Bolelli,Borja De Balle Pigem,Julia Di Trapani,Fotis Iliopoulos,Adam Paszke,Nishant Ranka,Joe Zou,Francesco Pongetti,Jed McGiffin,Alex Siegman,Rich Galt,Ross Hemsley,Goran Žužić,Victor Carbune,Tao Li,Myle Ott,Félix de Chaumont Quitry,David Vilar Torres,Yuri Chervonyi,Tomy Tsai,Prem Eruvbetine,Samuel Yang,Matthew Denton,Jake Walker,Slavica Andačić,Idan Heimlich Shtacher,Vittal Premachandran,Harshal Tushar Lehri,Cip Baetu,Damion Yates,Lampros Lamprou,Mariko Iinuma,Ioana Mihailescu,Ben Albrecht,Shachi Dave,Susie Sargsyan,Bryan Perozzi,Lucas Manning,Chiyuan Zhang,Denis Vnukov,Igor Mordatch,Raia Hadsell Wolfgang Macherey,Ryan Kappedal,Jim Stephan,Aditya Tripathi,Klaus Macherey,Jun Qian,Abhishek Bhowmick,Shekoofeh Azizi,Rémi Leblond,Shiva Mohan Reddy Garlapati,Timothy Knight,Matthew Wiethoff,Wei-Chih Hung,Anelia Angelova,Georgios Evangelopoulos,Pawel Janus,Dimitris Paparas,Matthew Rahtz,Ken Caluwaerts,Vivek Sampathkumar,Daniel Jarrett,Shadi Noghabi,Antoine Miech,Chak Yeung,Geoff Clark,Henry Prior,Fei Zheng,Jean Pouget-Abadie,Indro Bhattacharya,Kalpesh Krishna,Will Bishop,Zhe Yuan,Yunxiao Deng,Ashutosh Sathe,Kacper Krasowiak,Ciprian Chelba,Cho-Jui Hsieh,Kiran Vodrahalli,Buhuang Liu,Thomas Köppe,Amr Khalifa,Lubo Litchev,Pichi Charoenpanit,Reed Roberts,Sachin Yadav,Yasumasa Onoe,Desi Ivanov,Megha Mohabey,Vighnesh Birodkar,Nemanja Rakićević,Pierre Sermanet,Vaibhav Mehta,Krishan Subudhi,Travis Choma,Will Ng,Luheng He,Kathie Wang,Tasos Kementsietsidis,Shane Gu,Mansi Gupta,Andrew Nystrom,Mehran Kazemi,Timothy Chung,Nacho Cano,Nikhil Dhawan,Yufei Wang,Jiawei Xia,Trevor Yacovone,Eric Jia,Mingqing Chen,Simeon Ivanov,Ashrith Sheshan,Sid Dalmia,Paweł Stradomski,Pengcheng Yin,Salem Haykal,Congchao Wang,Dennis Duan,Neslihan Bulut,Greg Kochanski,Liam MacDermed,Namrata Godbole,Shitao Weng,Jingjing Chen,Rachana Fellinger,Ramin Mehran,Daniel Suo,Hisham Husain,Tong He,Kaushal Patel,Joshua Howland,Randall Parker,Kelvin Nguyen,Sharath Maddineni,Chris Rawles,Mina Khan,Shlomi Cohen-Ganor,Amol Mandhane,Xinyi Wu,Chenkai Kuang,Iulia Comşa,Ramya Ganeshan,Hanie Sedghi,Adam Bloniarz,Nuo Wang Pierse,Anton Briukhov,Petr Mitrichev,Anita Gergely,Serena Zhan,Allan Zhou,Nikita Saxena,Eva Lu,Josef Dean,Ashish Gupta,Nicolas Perez-Nieves,Renjie Wu,Cory McLean,Wei Liang,Disha Jindal,Anton Tsitsulin,Wenhao Yu,Kaiz Alarakyia,Tom Schaul,Piyush Patil,Peter Sung,Elijah Peake,Hongkun Yu,Feryal Behbahani,JD Co-Reyes,Alan Ansell,Sean Sun,Clara Barbu,Jonathan Lee,Seb Noury,James Allingham,Bilal Piot,Mohit Sharma,Christopher Yew,Ivan Korotkov,Bibo Xu,Demetra Brady,Goran Petrovic,Shibl Mourad,Claire Cui,Aditya Gupta,Parker Schuh,Saarthak Khanna,Anna Goldie,Abhinav Arora,Vadim Zubov,Amy Stuart,Mark Epstein,Yun Zhu,Jianqiao Liu,Yury Stuken,Ziyue Wang,Karolis Misiunas,Dee Guo,Ashleah Gill,Ale Hartman,Zaid Nabulsi,Aurko Roy,Aleksandra Faust,Jason Riesa,Ben Withbroe,Mengchao Wang,Marco Tagliasacchi,Andreea Marzoca,James Noraky,Serge Toropov,Malika Mehrotra,Bahram Raad,Sanja Deur,Steve Xu,Marianne Monteiro,Zhongru Wu,Yi Luan,Sam Ritter,Nick Li,Håvard Garnes,Yanzhang He,Martin Zlocha,Jifan Zhu,Matteo Hessel,Will Wu,Spandana Raj Babbula,Chizu Kawamoto,Yuanzhen Li,Mehadi Hassen,Yan Wang,Brian Wieder,James Freedman,Yin Zhang,Xinyi Bai,Tianli Yu,David Reitter,XiangHai Sheng,Mateo Wirth,Aditya Kini,Dima Damen,Mingcen Gao,Rachel Hornung,Michael Voznesensky,Brian Roark,Adhi Kuncoro,Yuxiang Zhou,Rushin Shah,Anthony Brohan,Kuangyuan Chen,James Wendt,David Rim,Paul Kishan Rubenstein,Jonathan Halcrow,Michelle Liu,Ty Geri,Yunhsuan Sung,Jane Shapiro,Shaan Bijwadia,Chris Duvarney,Christina Sorokin,Paul Natsev,Reeve Ingle,Pramod Gupta,Young Maeng,Ndaba Ndebele,Kexin Zhu,Valentin Anklin,Katherine Lee,Yuan Liu,Yaroslav Akulov,Shaleen Gupta,Guolong Su,Flavien Prost,Tianlin Liu,Vitaly Kovalev,Pol Moreno,Martin Scholz,Sam Redmond,Zongwei Zhou,Alex Castro-Ros,André Susano Pinto,Dia Kharrat,Michal Yarom,Rachel Saputro,Jannis Bulian,Ben Caine,Ji Liu,Abbas Abdolmaleki,Shariq Iqbal,Tautvydas Misiunas,Mikhail Sirotenko,Shefali Garg,Guy Bensky,Huan Gui,Xuezhi Wang,Raphael Koster,Mike Bernico,Da Huang,Romal Thoppilan,Trevor Cohn,Ben Golan,Wenlei Zhou,Andrew Rosenberg,Markus Freitag,Tynan Gangwani,Vincent Tsang,Anand Shukla,Xiaoqi Ren,Minh Giang,Chi Zou,Andre Elisseeff,Charline Le Lan,Dheeru Dua,Shuba Lall,Pranav Shyam,Frankie Garcia,Sarah Nguyen,Michael Guzman,AJ Maschinot,Marcello Maggioni,Ming-Wei Chang,Karol Gregor,Lotte Weerts,Kumaran Venkatesan,Bogdan Damoc,Leon Liu,Jan Wassenberg,Lewis Ho,Becca Roelofs,Majid Hadian,François-Xavier Aubet,Yu Liang,Sami Lachgar,Danny Karmon,Yong Cheng,Amelio Vázquez-Reina,Angie Chen,Zhuyun Dai,Andy Brock,Shubham Agrawal,Chenxi Pang,Peter Garst,Mariella Sanchez-Vargas,Ivor Rendulic,Aditya Ayyar,Andrija Ražnatović,Olivia Ma,Roopali Vij,Neha Sharma,Ashwin Balakrishna,Bingyuan Liu,Ian Mackinnon,Sorin Baltateanu,Petra Poklukar,Gabriel Ibagon,Colin Ji,Hongyang Jiao,Isaac Noble,Wojciech Stokowiec,Zhihao Li,Jeff Dean,David Lindner,Mark Omernick,Kristen Chiafullo,Mason Dimarco,Vitor Rodrigues,Vittorio Selo,Garrett Honke,Xintian,Wu,Wei He,Adam Hillier,Anhad Mohananey,Vihari Piratla,Chang Ye,Chase Malik,Sebastian Riedel,Samuel Albanie,Zi Yang,Kenny Vassigh,Maria Bauza,Sheng Li,Yiqing Tao,Nevan Wichers,Andrii Maksai,Abe Ittycheriah,Ross Mcilroy,Bryan Seybold,Noah Goodman,Romina Datta,Steven M. Hernandez,Tian Shi,Yony Kochinski,Anna Bulanova,Ken Franko,Mikita Sazanovich,Nicholas FitzGerald,Praneeth Kacham,Shubha Srinivas Raghvendra,Vincent Hellendoorn,Alexander Grushetsky,Julian Salazar,Angeliki Lazaridou,Jason Chang,Jan-Thorsten Peter,Sushant Kafle,Yann Dauphin,Abhishek Rao,Filippo Graziano,Izhak Shafran,Yuguo Liao,Tianli Ding,Geng Yan,Grace Chu,Zhao Fu,Vincent Roulet,Gabriel Rasskin,Duncan Williams,Shahar Drath,Alex Mossin,Raphael Hoffmann,Jordi Orbay,Francesco Bertolini,Hila Sheftel,Justin Chiu,Siyang Xue,Yuheng Kuang,Ferjad Naeem,Swaroop Nath,Nana Nti,Phil Culliton,Kashyap Krishnakumar,Michael Isard,Pei Sun,Ayan Chakrabarti,Nathan Clement,Regev Cohen,Arissa Wongpanich,GS Oh,Ashwin Murthy,Hao Zheng,Jessica Hamrick,Oskar Bunyan,Suhas Ganesh,Nitish Gupta,Roy Frostig,John Wieting,Yury Malkov,Pierre Marcenac,Zhixin,Lai,Xiaodan Tang,Mohammad Saleh,Fedir Zubach,Chinmay Kulkarni,Huanjie Zhou,Vicky Zayats,Nan Ding,Anshuman Tripathi,Arijit Pramanik,Patrik Zochbauer,Harish Ganapathy,Vedant Misra,Zach Behrman,Hugo Vallet,Mingyang Zhang,Mukund Sridhar,Ye Jin,Mohammad Babaeizadeh,Siim Põder,Megha Goel,Divya Jain,Tajwar Nasir,Shubham Mittal,Tim Dozat,Diego Ardila,Aliaksei Severyn,Fabio Pardo,Sammy Jerome,Siyang Qin,Louis Rouillard,Amir Yazdanbakhsh,Zizhao Zhang,Shivani Agrawal,Kaushik Shivakumar,Caden Lu,Praveen Kallakuri,Rachita Chhaparia,Kanishka Rao,Charles Kwong,Asya Fadeeva,Shitij Nigam,Yan Virin,Yuan Zhang,Balaji Venkatraman,Beliz Gunel,Marc Wilson,Huiyu Wang,Abhinav Gupta,Xiaowei Xu,Adrien Ali Taïga,Kareem Mohamed,Doug Fritz,Daniel Rodriguez,Zoubin Ghahramani,Harry Askham,Lior Belenki,James Zhao,Rahul Gupta,Krzysztof Jastrzębski,Takahiro Kosakai,Kaan Katircioglu,Jon Schneider,Rina Panigrahy,Konstantinos Bousmalis,Peter Grabowski,Prajit Ramachandran,Chaitra Hegde,Mihaela Rosca,Angelo Scorza Scarpati,Kyriakos Axiotis,Ying Xu,Zach Gleicher,Assaf Hurwitz Michaely,Mandar Sharma,Sanil Jain,Christoph Hirnschall,Tal Marian,Xuhui Jia,Kevin Mather,Kilol Gupta,Linhai Qiu,Nigamaa Nayakanti,Lucian Ionita,Steven Zheng,Lucia Loher,Kurt Shuster,Igor Petrovski,Roshan Sharma,Rahma Chaabouni,Angel Yeh,James An,Arushi Gupta,Steven Schwarcz,Seher Ellis,Sam Conway-Rahman,Javier Snaider,Alex Zhai,James Atwood,Daniel Golovin,Liqian Peng,Te I,Vivian Xia,Salvatore Scellato,Mahan Malihi,Arthur Bražinskas,Vlad-Doru Ion,Younghoon Jun,James Swirhun,Soroosh Mariooryad,Jiao Sun,Steve Chien,Rey Coaguila,Ariel Brand,Yi Gao,Tom Kwiatkowski,Roee Aharoni,Cheng-Chun Lee,Mislav Žanić,Yichi Zhang,Dan Ethier,Vitaly Nikolaev,Pranav Nair,Yoav Ben Shalom,Hen Fitoussi,Jai Gupta,Hongbin Liu,Dee Cattle,Tolga Bolukbasi,Ben Murdoch,Fantine Huot,Yin Li,Chris Hahn*

Main category: cs.CL

TL;DR: Gemini 2.X系列模型包括性能强大的Gemini 2.5 Pro和高效的Gemini 2.5 Flash，覆盖了从低成本高效能到前沿表现的多样需求。


<details>
  <summary>Details</summary>
Motivation: 开发一个在编码、推理、多模态理解等方面表现出色，且适用于多种应用场景（如长视频处理和复杂问题解决）的模型系列。

Method: 推出了Gemini 2.5 Pro、Gemini 2.5 Flash以及早期的Gemini 2.0 Flash和Flash-Lite，这些模型结合长上下文、多模态和推理能力，优化性能与计算成本的平衡。

Result: Gemini 2.5 Pro达到了当前最先进的编码和推理性能，支持长达3小时视频处理；Gemini 2.5 Flash以较低计算资源提供优异推理功能；Gemini 2.0 Flash及Flash-Lite在低延迟和低成本下提供高效性能。

Conclusion: Gemini 2.X系列覆盖了性能与成本的完整帕累托前沿，助力用户探索复杂智能代理问题解决的新边界。

Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and
Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite
models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA
performance on frontier coding and reasoning benchmarks. In addition to its
incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that
excels at multimodal understanding and it is now able to process up to 3 hours
of video content. Its unique combination of long context, multimodal and
reasoning capabilities can be combined to unlock new agentic workflows. Gemini
2.5 Flash provides excellent reasoning abilities at a fraction of the compute
and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high
performance at low latency and cost. Taken together, the Gemini 2.X model
generation spans the full Pareto frontier of model capability vs cost, allowing
users to explore the boundaries of what is possible with complex agentic
problem solving.

</details>


### [87] [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
*Neil Rathi,Dan Jurafsky,Kaitlyn Zhou*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在多语言环境中的语言不确定性表达及其带来的过度自信问题，发现模型在跨语言上普遍存在过度自信，且不同语言的表达差异导致用户依赖行为不同，凸显了多语言语言模型校准的复杂性和安全评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在全球范围的部署，确保模型在不同语言中的响应能够准确传达不确定性和局限性，避免用户过度依赖模型生成的内容，成为亟需解决的问题。

Method: 作者在五种语言中分析了模型生成的认知标记（表明确定或不确定的表达）的分布，测量了不同语言用户对这些表达的依赖程度，并比较了跨语言的差异。

Result: 研究发现，模型在所有语言中均表现出过度自信，且不同语言的认知标记使用存在显著差异，如日语中不确定性表达最多，德语和汉语中确定性表达最多；同时用户虽然在所有语言中都会依赖自信表达，但在不同语言中对不确定表达的依赖程度不同，如日语用户更倾向于依赖不确定表达。

Conclusion: 多语言环境下大型语言模型在不确定性表达上存在高风险的过度自信，用户过度依赖模型生成内容的问题普遍存在，因此需要针对不同文化和语言背景进行模型安全性和语言校准的评估。

Abstract: As large language models (LLMs) are deployed globally, it is crucial that
their responses are calibrated across languages to accurately convey
uncertainty and limitations. Previous work has shown that LLMs are
linguistically overconfident in English, leading users to overrely on confident
generations. However, the usage and interpretation of epistemic markers (e.g.,
'It's definitely,' 'I think') can differ sharply across languages. Here, we
study the risks of multilingual linguistic (mis)calibration, overconfidence,
and overreliance across five languages to evaluate the safety of LLMs in a
global context.
  We find that overreliance risks are high across all languages. We first
analyze the distribution of LLM-generated epistemic markers, and observe that
while LLMs are cross-linguistically overconfident, they are also sensitive to
documented linguistic variation. For example, models generate the most markers
of uncertainty in Japanese and the most markers of certainty in German and
Mandarin. We then measure human reliance rates across languages, finding that
while users strongly rely on confident LLM generations in all languages,
reliance behaviors differ cross-linguistically: for example, users rely
significantly more on expressions of uncertainty in Japanese than in English.
Taken together, these results indicate high risk of reliance on overconfident
model generations across languages. Our findings highlight the challenges of
multilingual linguistic calibration and stress the importance of culturally and
linguistically contextualized model safety evaluations.

</details>


### [88] [ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time](https://arxiv.org/abs/2507.06313)
*Kiarash Zahirnia,Zahra Golpayegani,Walid Ahmad,Yang Liu*

Main category: cs.CL

TL;DR: 提出了一种在测试时扩展Transformer模型上下文长度的方法ETT，利用重叠小子序列进行高效微调，实现线性计算开销和固定内存需求，有效提升长序列处理能力。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列时计算和内存开销呈二次增长，限制了大语言模型在长序列任务中的应用。

Method: ETT方法通过将长输入切分为重叠的小子序列，在测试时微调部分模型参数，尤其是第二层FFN模块，实现上下文长度的扩展，同时保持常数内存和线性计算复杂度。

Result: 在LongBench数据集上，将GPT-Large和Phi-2的上下文长度从1k扩展到32k，提升模型准确率最高达30%。微调第二层FFN效果优于全模型微调。

Conclusion: ETT方法有效解决了Transformer模型长序列计算瓶颈，通过局部微调实现了高效的上下文扩展，有助于提升大语言模型的长期依赖处理能力。

Abstract: Transformer-based Language Models' computation and memory overhead increase
quadratically as a function of sequence length. The quadratic cost poses
challenges when employing LLMs for processing long sequences. In this work, we
introduce \ourmodelacronym~(Extend at Test-Time), method for extending the
context length of short context Transformer-based LLMs, with constant memory
requirement and linear computation overhead. ETT enable the extension of the
context length at test-time by efficient fine-tuning the model's parameters on
the input context, chunked into overlapping small subsequences. We evaluate ETT
on LongBench by extending the context length of GPT-Large and Phi-2 up to 32
times, increasing from 1k to 32k tokens. This results in up to a 30 percent
improvement in the model's accuracy. We also study how context can be stored in
LLM's weights effectively and efficiently. Through a detailed ablation study,
we examine which Transformer modules are most beneficial to fine-tune at
test-time. Interestingly, we find that fine-tuning the second layer of the FFNs
is more effective than full fine-tuning, leading to a further improvement in
the models' accuracy.

</details>


### [89] [Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?](https://arxiv.org/abs/2507.06335)
*Casey Kennington,David Schlangen*

Main category: cs.CL

TL;DR: 本文探讨将形式语义、分布式语义和基于视觉的语义模型统一的方法，提出了基于词分类器的语义模型作为潜在途径。


<details>
  <summary>Details</summary>
Motivation: 现有的计算语义理论各有优缺点，融合视觉知识和符号方法的需求推动寻找统一的语义模型。

Method: 回顾已有文献，结合认知科学新研究，提出并实验验证了词分类器模型在统一三种语义理论中的可行性。

Result: 通过小实验展示词分类器模型在交互式对话环境中的有效性及其与形式和分布式模型的整合潜力。

Conclusion: 基于词分类器的模型有望成为统一形式、分布式及基于视觉语义理论的桥梁，为计算语义提供新的路径。

Abstract: Formal, Distributional, and Grounded theories of computational semantics each
have their uses and their drawbacks. There has been a shift to ground models of
language by adding visual knowledge, and there has been a call to enrich models
of language with symbolic methods to gain the benefits from formal,
distributional, and grounded theories. In this paper, we attempt to make the
case that one potential path forward in unifying all three semantic fields is
paved with the words-as-classifier model, a model of word-level grounded
semantics that has been incorporated into formalisms and distributional
language models in the literature, and it has been well-tested within
interactive dialogue settings. We review that literature, motivate the
words-as-classifiers model with an appeal to recent work in cognitive science,
and describe a small experiment. Finally, we sketch a model of semantics
unified through words-as-classifiers.

</details>


### [90] [Evaluating Morphological Alignment of Tokenizers in 70 Languages](https://arxiv.org/abs/2507.06378)
*Catherine Arnett,Marisa Hudspeth,Brendan O'Connor*

Main category: cs.CL

TL;DR: 本文扩展了MorphScore工具以支持70种语言，评估分词器的形态学边界对齐能力，并分析其与下游任务性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前难以有效评估分词器质量，尤其是其在保持词素边界上的表现。

Method: 升级MorphScore以支持更多语言和更灵活的评估方式，并对五种预训练语言模型在七个任务上的表现与形态学对齐分数进行了相关分析。

Result: 形态学边界对齐与模型在下游任务中的性能相关性较低，无法很好解释性能差异。

Conclusion: 形态学对齐单一指标不足以衡量分词器的整体质量及其对模型性能的影响。

Abstract: While tokenization is a key step in language modeling, with effects on model
training and performance, it remains unclear how to effectively evaluate
tokenizer quality. One proposed dimension of tokenizer quality is the extent to
which tokenizers preserve linguistically meaningful subwords, aligning token
boundaries with morphological boundaries within a word. We expand MorphScore
(Arnett & Bergen, 2025), which previously covered 22 languages, to support a
total of 70 languages. The updated MorphScore offers more flexibility in
evaluation and addresses some of the limitations of the original version. We
then correlate our alignment scores with downstream task performance for five
pre-trained languages models on seven tasks, with at least one task in each of
the languages in our sample. We find that morphological alignment does not
explain very much variance in model performance, suggesting that morphological
alignment alone does not measure dimensions of tokenization quality relevant to
model performance.

</details>


### [91] [Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles](https://arxiv.org/abs/2507.06393)
*Matilde Marcolli,Riny Huijbregts,Richard K. Larson*

Main category: cs.CL

TL;DR: 本文提出了句法结构中的头功能将代数结构推广为超代数，利用彩色操作子系统形式统一句法结构和语义角色的生成，整合了内部合并运动规则。


<details>
  <summary>Details</summary>
Motivation: 为了统一描述句法结构（包括头、补语、限定语和附加修饰语）及其语义角色分配，并规范句法运动规则，作者引入了更丰富的代数工具。

Method: 作者将头函数视为从群结构到超群结构的扩展，建立彩色操作子的芽生成系统，通过彩色合并和滤波规则形式化句法结构的生成及内部运动，连接代数结构与句法层级的关系。

Result: 成功将句法头及其相关结构形式化为彩色操作子系统，统一了运动规则和句法层级限制（如阶段、内部合并、扩展投射原则等），并描述了语义角色与阶段结构之间的兼容运动。

Conclusion: 利用超代数和彩色操作子框架，本文实现了对句法生成结构及其运动机制的统一数学描述，有助于深入理解句法和语义的内在关系。

Abstract: We show that head functions on syntactic objects extend the magma structure
to a hypermagma, with the c-command relation compatible with the magma
operation and the m-command relation with the hypermagma. We then show that the
structure of head and complement and specifier, additional modifier positions,
and the structure of phases in the Extended Projection can be formulated as a
bud generating system of a colored operad, in a form similar to the structure
of theta roles. We also show that, due to the special form of the colored
operad generators, the filtering of freely generated syntactic objects by these
coloring rules can be equivalently formulated as a filtering in the course of
structure formation via a colored Merge, which can in turn be related to the
hypermagma structure. The rules on movement by Internal Merge with respect to
phases, the Extended Projection Principle, Empty Category Principle, and Phase
Impenetrability Condition are all subsumed into the form of the colored operad
generators. Movement compatibilities between the phase structure and the theta
roles assignments can then be formulated in terms of the respective colored
operads and a transduction of colored operads.

</details>


### [92] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: 该论文提出了PERK方法，通过在测试时使用轻量级模型适配器的梯度更新，有效编码长上下文信息，实现了对噪声信息的准确推理。


<details>
  <summary>Details</summary>
Motivation: 现有元学习方法虽然能通过测试时学习直接编码上下文实现推理，但内存消耗巨大，难以应用于长上下文场景。

Method: PERK采用两层嵌套优化循环，内层快速编码上下文到低秩适配器，外层学习利用更新的适配器准确回忆和推理相关信息。

Result: 在多个长上下文推理任务中，PERK对比标准提示基线，较小模型性能提升最高达90%，大型模型提升达27%，且在推理复杂度和上下文长度外推上表现更稳健。

Conclusion: PERK虽然训练时内存消耗较大，但推理过程中比提示基线更高效，展示了在长上下文推理任务中的良好扩展性和效果。

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [93] [Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling](https://arxiv.org/abs/2507.06419)
*Pankayaraj Pathmanathan,Furong Huang*

Main category: cs.CL

TL;DR: 本文提出了一种无需先验知识的奖励模型失败模式发现方法及自我改进框架REFORM，通过对抗样本增强训练数据，显著提升奖励模型在分布转移和攻击扰动下的鲁棒性，同时保持奖励质量和下游策略性能。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在捕捉人类偏好和对齐大语言模型时，容易在分布转移或对抗扰动下失败，且现有方法依赖先验知识限制了实际应用。

Method: 提出基于奖励引导的受控解码发现失败模式，设计REFORM框架利用奖励模型自身生成错误评分响应作为对抗样本，增强训练数据以修正模型偏差。

Result: 在Anthropic HH和PKU Beavertails两个数据集上，REFORM显著提升了奖励模型鲁棒性，保持了奖励质量和下游策略训练性能，并去除了虚假相关性。

Conclusion: REFORM为奖励模型提供了一种实用且有效的自我增强途径，提升模型对分布变化和攻击的适应能力，推动了大语言模型对齐技术的发展。

Abstract: Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.

</details>


### [94] [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
*Shun Wang,Tyler Loakman,Youbo Lei,Yi Liu,Bohao Yang,Yuting Zhao,Dong Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于稀疏自编码器的字典学习方法，将大型语言模型（LLMs）分解为单义特征，提升了模型的可解释性和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型被视为黑箱，缺乏可解释性，限制了信任度和性能提升空间。

Method: 采用稀疏自编码器进行字典学习，分离出单义神经元特征，并自动检测模型内部的误解，重新构造带注释的提示。

Result: 该方法显著提升了在数学推理和隐喻检测等下游任务上的表现。

Conclusion: 通过对LLM内部特征的有效分解和误解修正，能提升模型的可解释性和实际应用性能。

Abstract: Large Language Models (LLMs) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective LLM decomposition method using a dictionary-learning
approach with sparse autoencoders. This helps extract monosemantic features
from polysemantic LLM neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by LLMs. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.

</details>


### [95] [Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling](https://arxiv.org/abs/2507.06435)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.CL

TL;DR: 本文利用动态嵌入主题模型(DETM)分析1995年至2023年联合国气候变化框架公约政策语言的演变，揭示了政策重点的转移。


<details>
  <summary>Details</summary>
Motivation: 传统人工编码方法效率低且难以捕捉全球气候政策语言的复杂动态，需要高效自动化工具。

Method: 采用动态嵌入主题模型(DETM)对联合国气候变化政策文本进行时间序列分析，识别主题随时间变化的动态特征。

Result: 模型发现了政策重点从温室气体和国际公约转向实施、技术合作、能力建设、资金和全球协议的趋势，验证了DETM在处理复杂政策文本数据中的有效性和可扩展性。

Conclusion: DETM是一种有效的工具，可用于分析全球政策话语的动态演变，未来有望扩展到其他政策领域研究。

Abstract: Understanding how policy language evolves over time is critical for assessing
global responses to complex challenges such as climate change. Temporal
analysis helps stakeholders, including policymakers and researchers, to
evaluate past priorities, identify emerging themes, design governance
strategies, and develop mitigation measures. Traditional approaches, such as
manual thematic coding, are time-consuming and limited in capturing the
complex, interconnected nature of global policy discourse. With the increasing
relevance of unsupervised machine learning, these limitations can be addressed,
particularly under high-volume, complex, and high-dimensional data conditions.
In this work, we explore a novel approach that applies the dynamic embedded
topic model (DETM) to analyze the evolution of global climate policy discourse.
A probabilistic model designed to capture the temporal dynamics of topics over
time. We collected a corpus of United Nations Framework Convention on Climate
Change (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the
postponement of COP26 as a result of the COVID-19 pandemic. The model reveals
shifts from early emphases on greenhouse gases and international conventions to
recent focuses on implementation, technical collaboration, capacity building,
finance, and global agreements. Section 3 presents the modeling pipeline,
including preprocessing, model training, and visualization of temporal word
distributions. Our results show that DETM is a scalable and effective tool for
analyzing the evolution of global policy discourse. Section 4 discusses the
implications of these findings and we concluded with future directions and
refinements to extend this approach to other policy domains.

</details>


### [96] [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
*Zhenhailong Wang,Xuehang Guo,Sofia Stoica,Haiyang Xu,Hongru Wang,Hyeonjeong Ha,Xiusi Chen,Yangyi Chen,Ming Yan,Fei Huang,Heng Ji*

Main category: cs.CL

TL;DR: 提出了一种名为PAPO的感知感知策略优化方法，通过引入隐式感知损失，提高了多模态推理任务中大语言模型的视觉感知能力和推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习带可验证奖励（RLVR）方法主要针对纯文本领域设计，导致在多模态推理任务中表现不佳，特别是在视觉输入的感知方面存在较大错误。

Method: 基于GRPO方法，PAPO引入了隐式感知损失（KL散度项）作为内部监督信号，鼓励模型在学习推理的同时增强视觉感知能力，无需额外数据或外部奖励模型，并通过双重熵损失解决了损失攻击问题。

Result: PAPO在多模态基准测试中总体性能提升4.4%，高度依赖视觉的任务中提升近8.0%，感知错误减少30.5%，显著提升了视觉感知和推理效果。

Conclusion: PAPO实现了感知能力与推理能力的深度融合，推动了一种新的视觉引导强化学习框架的发展，为多模态推理任务提供了有效的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.

</details>


### [97] [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
*Xin Su,Sungduk Yu,Phillip Howard,Steven Bethard*

Main category: cs.CL

TL;DR: 本文提出了一种基于SCATE框架的时间规范化新方法，通过大语言模型生成可执行代码，实现了复杂时间表达式的准确解析，并通过自动数据增强训练小模型，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于ISO-TimeML的时间规范化方法表达能力有限，难以处理复杂时间表达式，影响下游应用效果，需要一种更灵活精准的时间表达式解析方法。

Method: 将时间规范化任务重新定义为基于SCATE框架的代码生成任务，利用大语言模型生成可执行SCATE代码，结合自动数据增强生成大量经代码级验证的注释数据，用以训练本地模型。

Result: 利用增强数据训练的小型本地模型在时间规范化任务中表现优异，甚至超过了其大语言模型父模型，实现了准确、可解释且实用的时间解析。

Conclusion: 基于SCATE代码生成和数据增强的方法有效提升了时间规范化的表达能力和性能，为实际应用中的时间语义解析提供了强有力工具。

Abstract: Time normalization is the task of converting natural language temporal
expressions into machine-readable representations. It underpins many downstream
applications in information retrieval, question answering, and clinical
decision-making. Traditional systems based on the ISO-TimeML schema limit
expressivity and struggle with complex constructs such as compositional,
event-relative, and multi-span time expressions. In this work, we introduce a
novel formulation of time normalization as a code generation task grounded in
the SCATE framework, which defines temporal semantics through symbolic and
compositional operators. We implement a fully executable SCATE Python library
and demonstrate that large language models (LLMs) can generate executable SCATE
code. Leveraging this capability, we develop an automatic data augmentation
pipeline using LLMs to synthesize large-scale annotated data with code-level
validation. Our experiments show that small, locally deployable models trained
on this augmented data can achieve strong performance, outperforming even their
LLM parents and enabling practical, accurate, and interpretable time
normalization.

</details>


### [98] [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
*Dustin Wang,Rui-Jie Zhu,Steven Abreu,Yong Shan,Taylor Kergan,Yuqi Pan,Yuhong Chou,Zheng Li,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文系统评价了不同线性注意力模型在独立和混合架构中的表现，提出了有效的混合比例和结构建议。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在处理长序列时的二次复杂度和内存限制，探索更高效的线性与全注意力混合架构。

Method: 训练并开源72个不同规模和组合比例的模型，涵盖六种线性注意力变体，进行语言建模和记忆召回任务的广泛基准测试。

Result: 发现某些在线性模型中表现优异的方法在混合模型中不一定表现最好，记忆召回显著依赖全注意力层的比例，性能提升明显在3:1的临界点以下。

Conclusion: 选择性门控、层次递归和受控遗忘是混合模型效果的关键，推荐采用HGRN-2或GatedDeltaNet，混合比例介于3:1至6:1，以实现高效且具Transformer级别召回的模型。

Abstract: Transformers face quadratic complexity and memory issues with long sequences,
prompting the adoption of linear attention mechanisms using fixed-size hidden
states. However, linear models often suffer from limited recall performance,
leading to hybrid architectures that combine linear and full attention layers.
Despite extensive hybrid architecture research, the choice of linear attention
component has not been deeply explored. We systematically evaluate various
linear attention models across generations - vector recurrences to advanced
gating mechanisms - both standalone and hybridized. To enable this
comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M
parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six
linear attention variants across five hybridization ratios. Benchmarking on
standard language modeling and recall tasks reveals that superior standalone
linear models do not necessarily excel in hybrids. While language modeling
remains stable across linear-to-full attention ratios, recall significantly
improves with increased full attention layers, particularly below a 3:1 ratio.
Our study highlights selective gating, hierarchical recurrence, and controlled
forgetting as critical for effective hybrid models. We recommend architectures
such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1
to achieve Transformer-level recall efficiently. Our models are open-sourced at
https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.

</details>


### [99] [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489)
*Stephen Obadinma,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本文首次系统研究了大型语言模型中语言置信度在对抗攻击下的鲁棒性，提出了一种新的攻击框架，发现当前置信度估计方法易受攻击且防御方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型输出的置信度具有透明度和可靠性对于高风险应用中人机交互的安全至关重要，因此需要评估其在对抗攻击下的鲁棒性。

Method: 提出了结合扰动和越狱两种方法的攻击框架，对多种提示策略、模型大小及应用领域进行测试，以评估语言置信度的脆弱性及现有防御措施的效果。

Result: 发现攻击能显著破坏置信度估计，导致回答频繁改变，且目前的置信度提取方法易受攻击；常用防御策略效果有限甚至适得其反。

Conclusion: 需要设计更鲁棒的置信度表达机制，防止即使是细微语义不变的修改也能误导模型的置信度，保障人机交互中的透明度和安全性。

Abstract: Robust verbal confidence generated by large language models (LLMs) is crucial
for the deployment of LLMs to ensure transparency, trust, and safety in
human-AI interactions across many high-stakes applications. In this paper, we
present the first comprehensive study on the robustness of verbal confidence
under adversarial attacks. We introduce a novel framework for attacking verbal
confidence scores through both perturbation and jailbreak-based methods, and
show that these attacks can significantly jeopardize verbal confidence
estimates and lead to frequent answer changes. We examine a variety of
prompting strategies, model sizes, and application domains, revealing that
current confidence elicitation methods are vulnerable and that commonly used
defence techniques are largely ineffective or counterproductive. Our findings
underscore the urgent need to design more robust mechanisms for confidence
expression in LLMs, as even subtle semantic-preserving modifications can lead
to misleading confidence in responses.

</details>


### [100] [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
*Russell Taylor,Benjamin Herbert,Michael Sana*

Main category: cs.CL

TL;DR: 本文提出了一种结合大型语言模型与专门文字游戏生成技术的英语到法语双关语翻译新方法，主要采用三阶段流程，包括对比学习反馈、多模态嵌入以及多智能体生成判别框架，目标是捕捉文字游戏的语言创造力与幽默感。


<details>
  <summary>Details</summary>
Motivation: 翻译文字游戏尤其是双关语时，传统的人类翻译和机器翻译系统都面临巨大挑战，尤其是要保持语言幽默和文化含义。

Method: 采用三阶段方法：第一，基于多种先进语言模型和新的对比学习数据集反馈建立基线；第二，结合语音和语义嵌入实施引导式思路流水线；第三，利用多智能体生成-判别框架进行双关语的评估和再生成。

Result: 该方法在CLEF JOKER 2025任务二中获得第一、二名，结果经法语母语专家手动评估，表现优异。

Conclusion: 通过结合语言学知识和计算语言学技术，研究推动了利用语言模型处理语义歧义、语音相似性及文化隐含理解的能力，填补了翻译研究与计算语言学间的空白。

Abstract: Translating wordplay across languages presents unique challenges that have
long confounded both professional human translators and machine translation
systems. This research proposes a novel approach for translating puns from
English to French by combining state-of-the-art large language models with
specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a
baseline using multiple frontier large language models with feedback based on a
new contrastive learning dataset. Second, we implement a guided
chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we
implement a multi-agent generator-discriminator framework for evaluating and
regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's
primary objective is to capture the linguistic creativity and humor of the
source text wordplay, rather than simply duplicating its vocabulary. Our best
runs earned first and second place in the CLEF JOKER 2025 Task 2 competition
where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational
linguistics by implementing linguistically-informed techniques for wordplay
translation, advancing our understanding of how language models can be
leveraged to handle the complex interplay between semantic ambiguity, phonetic
similarity, and the implicit cultural and linguistic awareness needed for
successful humor.

</details>


### [101] [SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](https://arxiv.org/abs/2507.06517)
*Zicong Tang,Shi Luohe,Zuchao Li,Baoyuan Qi,Guoming Liu,Lefei Zhang,Ping Wang*

Main category: cs.CL

TL;DR: 本文提出了SpindleKV方法，通过结合基于注意力权重的淘汰和基于码本的替代，实现对大语言模型KV缓存的高效压缩，尤其兼顾浅层和深层，解决了以往方法在分组查询注意力上的困境，大幅减少空间占用同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的KV缓存弥补严重增长，现有淘汰方法对深层有效但对浅层不足，且存在分组查询注意力问题，亟需改进的KV缓存压缩策略。

Method: 在深层应用基于注意力权重的淘汰，在浅层采用基于相似性和合并策略学习的码本替换，同时解决分组查询注意力（GQA）的困境。

Result: 在两个基准和三种LLM上，SpindleKV相比基线方法表现出更好的KV缓存压缩效果，同时保持或者提升模型性能。

Conclusion: SpindleKV有效均衡浅深层缓存压缩问题，显著降低KV缓存需求，兼顾模型效率与表现，展示了KV缓存优化的新方向。

Abstract: Large Language Models (LLMs) have achieved impressive accomplishments in
recent years. However, the increasing memory consumption of KV cache has
possessed a significant challenge to the inference system. Eviction methods
have revealed the inherent redundancy within the KV cache, demonstrating its
potential for reduction, particularly in deeper layers. However, KV cache
reduction for shallower layers has been found to be insufficient. Based on our
observation that, the KV cache exhibits a high degree of similarity. Based on
this observation, we proposed a novel KV cache reduction method, SpindleKV,
which balances both shallow and deep layers. For deep layers, we employ an
attention weight based eviction method, while for shallow layers, we apply a
codebook based replacement approach which is learnt by similarity and merging
policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma
faced by other attention based eviction methods. Experiments on two common
benchmarks with three different LLMs shown that SpindleKV obtained better KV
cache reduction effect compared to baseline methods, while preserving similar
or even better model performance.

</details>


### [102] [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
*Huisheng Wang,Zhuoshi Pan,Hangjing Zhang,Mingxiao Liu,Hanqing Gao,H. Vicky Zhao*

Main category: cs.CL

TL;DR: 本论文提出了一种名为InvestAlign的框架，通过利用理论上简单投资问题的解决方案生成高质量的监督微调数据集，从而训练大语言模型（LLMs）以更高效地模拟投资者的羊群行为决策过程。


<details>
  <summary>Details</summary>
Motivation: 在行为金融学中，如何使大语言模型与投资者在羊群行为下的决策过程保持一致是一大挑战。监督微调通常需要大量真实用户数据，但数据稀缺且收集成本高，存在隐私风险。

Method: 提出InvestAlign框架，通过理论优化的简单投资问题生成数据，避免复杂场景数据的难以获取。使用这些数据训练LLMs，加快参数收敛速度。基于此，开发了微调后的LLM代理InvestAgent。

Result: InvestAlign生成的数据使模型训练比使用真实数据更快收敛。InvestAgent在简单和复杂投资问题中，与真实用户数据的行为表现更加接近，优于未微调模型。

Conclusion: InvestAlign为解决复杂最优投资问题和行为金融中的羊群行为决策提供了一种高效且有效的方法，能更好地对齐大语言模型与真实投资者的决策行为。代码已公开。

Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes
under herd behavior is a critical challenge in behavioral finance, which
grapples with a fundamental limitation: the scarcity of real-user data needed
for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM
outputs and human behavioral patterns, its reliance on massive authentic data
imposes substantial collection costs and privacy risks. We propose InvestAlign,
a novel framework that constructs high-quality SFT datasets by leveraging
theoretical solutions to similar and simple optimal investment problems rather
than complex scenarios. Our theoretical analysis demonstrates that training
LLMs with InvestAlign-generated data achieves faster parameter convergence than
using real-user data, suggesting superior learning efficiency. Furthermore, we
develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which
demonstrates significantly closer alignment to real-user data than pre-SFT
models in both simple and complex investment problems. This highlights our
proposed InvestAlign as a promising approach with the potential to address
complex optimal investment problems and align LLMs with investor
decision-making processes under herd behavior. Our code is publicly available
at https://github.com/thu-social-network-research-group/InvestAlign.

</details>


### [103] [Large Language Model for Extracting Complex Contract Information in Industrial Scenes](https://arxiv.org/abs/2507.06539)
*Yunyang Cao,Yanjun Li,Silong Dai*

Main category: cs.CL

TL;DR: 该论文提出了一种针对工业场景复杂合同信息抽取的高质量数据集构建方法，并基于该数据集对大语言模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 工业合同文本复杂，信息抽取难度大，需高质量数据支撑以提升抽取模型的准确性和鲁棒性。

Method: 通过对工业合同文本进行聚类分析，利用GPT-4和GPT-3.5提取关键信息生成高质量标注数据；采用构造新文本的数据增强方法，利用GPT-3.5基于随机关键词生成非结构化合同文本；最后基于该高质量数据集对大语言模型进行微调，同时辅以LoRA、数据平衡和数据增强提升模型性能。

Result: 模型在保证高领域召回率和精确率的同时，具备优秀的整体表现和解析效率。LoRA、数据平衡和数据增强显著提升了模型准确率和鲁棒性。

Conclusion: 该方法为工业合同信息抽取任务提供了一种新颖高效的解决方案，显著提升了抽取模型表现及应用价值。

Abstract: This paper proposes a high-quality dataset construction method for complex
contract information extraction tasks in industrial scenarios and fine-tunes a
large language model based on this dataset. Firstly, cluster analysis is
performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to
extract key information from the original contract data, obtaining high-quality
data annotations. Secondly, data augmentation is achieved by constructing new
texts, and GPT-3.5 generates unstructured contract texts from randomly combined
keywords, improving model robustness. Finally, the large language model is
fine-tuned based on the high-quality dataset. Experimental results show that
the model achieves excellent overall performance while ensuring high field
recall and precision and considering parsing efficiency. LoRA, data balancing,
and data augmentation effectively enhance model accuracy and robustness. The
proposed method provides a novel and efficient solution for industrial contract
information extraction tasks.

</details>


### [104] [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
*Juan B. Gutiérrez*

Main category: cs.CL

TL;DR: 本文提出了一种包含人类与大语言模型平等节点的论述网络模型，跟踪陈述的流通并定义了“无效化”现象，揭示其四种危害，通过数学模型和开源算法验证了提高模型可靠性的路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型造成的错误不止是孤立的幻觉，而是一种节点间信息流通中出现的“无效化”，需要新的理论框架和方法来理解和提升生成内容的可靠性。

Method: 提出了包含人类与大语言模型的论述网络数学模型，分析无效化引发的四种危害；设计了开源的"Flaws-of-Others (FOO)"算法，用以模拟代理间的相互批评和合议机制。

Result: 模型显示仅有偏离和自我修正时错误率较低，加入制造虚假内容后错误率增加；引入同行评审机制能使系统趋近于以真实为主导的状态。

Conclusion: 提升大语言模型的可靠性不应依赖单一模型的完美，而应通过构建相互监督的多模型网络体系，利用网络结构保持内容的真实性和准确性。

Abstract: Large-language models turn writing into a live exchange between humans and
software. We capture this new medium with a discursive-network model that
treats people and LLMs as equal nodes and tracks how their statements
circulate. Broadening the focus from isolated hallucinations, we define
invalidation (any factual, logical, or structural breach) and show it follows
four hazards: drift from truth, self-repair, fresh fabrication, and external
detection. A general mathematical model of discursive networks is developed to
provide valuable insights: A network governed only by drift and self-repair
stabilizes at a modest error rate; adding fabrication reproduces the high rates
seen in current LLMs. Giving each false claim even a small chance of peer
review shifts the system to a truth-dominant state. We operationalize peer
review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a
configurable loop in which any set of agents critique one another while a
harmoniser merges their verdicts. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
wiring imperfect ones into networks that keep each other honest.

</details>


### [105] [Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis](https://arxiv.org/abs/2507.06571)
*Srihari K B,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该论文提出了一个结合大规模多模态知识图谱与生成式AI的统一食品领域问答框架。


<details>
  <summary>Details</summary>
Motivation: 当前食品问答系统缺乏结构化知识支持及图像与文本的高效融合，导致回答质量和多样性不足。

Method: 构建包含1.3万菜谱、3千原料、14万关系及1.4万图像的多模态知识图谱，采用40个模板及LLaVA/DeepSeek生成4万问答对，通过对Meta LLaMA和Stable Diffusion模型的联合微调提升表现，同时结合CLIP-based错配检测与LLaVA驱动的幻觉检测确保事实和视觉准确性，采用混合检索-生成策略优化图像复用率和合成问答的充分性。

Result: 微调后BERTScore提升16.2%，FID降低37.8%，CLIP对齐率提升31.1%；错配检测准确率由35.2%降至7.3%，图像复用准确率达到94.1%，问答合成充分性85%。

Conclusion: 结构化知识与多模态生成技术的结合显著提升了食品领域问答系统的可靠性和多样性。

Abstract: We propose a unified food-domain QA framework that combines a large-scale
multimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000
recipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate
40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint
fine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves
BERTScore by 16.2\%, reduces FID by 37.8\%, and boosts CLIP alignment by
31.1\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\% to 7.3\%) and
LLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid
retrieval-generation strategy achieves 94.1\% accurate image reuse and 85\%
adequacy in synthesis. Our results demonstrate that structured knowledge and
multimodal generation together enhance reliability and diversity in food QA.

</details>


### [106] [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
*Liliang Ren,Congcong Chen,Haoran Xu,Young Jin Kim,Adam Atkinson,Zheng Zhan,Jiankai Sun,Baolin Peng,Liyuan Liu,Shuohang Wang,Hao Cheng,Jianfeng Gao,Weizhu Chen,Yelong Shen*

Main category: cs.CL

TL;DR: 本文提出了门控记忆单元（GMU），用于不同层之间高效共享记忆状态，并基于此设计了SambaY架构，显著提升解码效率和长上下文性能。


<details>
  <summary>Details</summary>
Motivation: 现有的状态空间模型层之间未充分利用表示共享机制，限制了效率提升的可能性。

Method: 引入GMU机制实现跨层内存共享，结合Samba自解码器与GMU构成SambaY混合解码器架构。

Result: SambaY保留线性预填充复杂度，性能优于YOCO基线，解决大规模计算下的损失问题，且在多项推理任务上表现优异，同时解码吞吐量最高提升10倍。

Conclusion: GMU机制有效提升了状态空间模型的表示共享效率，SambaY架构在提升性能和效率方面展现出显著优势，为大规模语言模型架构设计提供新思路。

Abstract: Recent advances in language modeling have demonstrated the effectiveness of
State Space Models (SSMs) for efficient sequence modeling. While hybrid
architectures such as Samba and the decoder-decoder architecture, YOCO, have
shown promising performance gains over Transformers, prior works have not
investigated the efficiency potential of representation sharing between SSM
layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet
effective mechanism for efficient memory sharing across layers. We apply it to
create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in
the cross-decoder to share memory readout states from a Samba-based
self-decoder. SambaY significantly enhances decoding efficiency, preserves
linear pre-filling time complexity, and boosts long-context performance, all
while eliminating the need for explicit positional encoding. Through extensive
scaling experiments, we demonstrate that our model exhibits a significantly
lower irreducible loss compared to a strong YOCO baseline, indicating superior
performance scalability under large-scale compute regimes. Our largest model
enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves
significantly better performance than Phi4-mini-Reasoning on reasoning tasks
such as Math500, AIME24/25, and GPQA Diamond without any reinforcement
learning, while delivering up to 10x higher decoding throughput on 2K-length
prompts with 32K generation length under the vLLM inference framework. We
release our training codebase on open-source data at
https://github.com/microsoft/ArchScale.

</details>


### [107] [FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation](https://arxiv.org/abs/2507.06622)
*Boshko Koloski,Senja Pollak,Roberto Navigli,Blaž Škrlj*

Main category: cs.CL

TL;DR: 该论文提出了一种名为FuDoBa的新方法，结合了大型语言模型（LLM）的嵌入和领域特定的结构化知识，生成低维且任务相关的文档表示，提高了分类性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的高维嵌入在领域特定应用中效率低下且过于泛化，需要一种更高效且具解释性的表示方法。

Method: FuDoBa通过贝叶斯优化融合LLM嵌入与本地和外部知识库（如WikiData）的结构化知识，获得低维、任务相关的表示，并结合AutoML分类器进行训练。

Result: 在两个领域的六个数据集上，FuDoBa表现与或优于仅使用LLM嵌入的基线方法，提升了分类性能且训练复杂度降低。

Conclusion: FuDoBa有效地结合了LLM嵌入与结构化知识，提供了一种低维、可解释且高效的文档表示学习方案，适用于领域特定的文本分类任务。

Abstract: Building on the success of Large Language Models (LLMs), LLM-based
representations have dominated the document representation landscape, achieving
great performance on the document embedding benchmarks. However, the
high-dimensional, computationally expensive embeddings from LLMs tend to be
either too generic or inefficient for domain-specific applications. To address
these limitations, we introduce FuDoBa a Bayesian optimisation-based method
that integrates LLM-based embeddings with domain-specific structured knowledge,
sourced both locally and from external repositories like WikiData. This fusion
produces low-dimensional, task-relevant representations while reducing training
complexity and yielding interpretable early-fusion weights for enhanced
classification performance. We demonstrate the effectiveness of our approach on
six datasets in two domains, showing that when paired with robust AutoML-based
classifiers, our proposed representation learning approach performs on par
with, or surpasses, those produced solely by the proprietary LLM-based
embedding baselines.

</details>


### [108] [Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review](https://arxiv.org/abs/2507.06623)
*James Stewart-Evans,Emma Wilson,Tessa Langley,Andrew Prayle,Angela Hands,Karen Exley,Jo Leonardi-Bee*

Main category: cs.CL

TL;DR: 本文探讨了使用大语言模型（LLMs）和审查协议来加速文献综述中的数据提取过程，通过案例研究评估了两种提取方法的准确率和性能表现。


<details>
  <summary>Details</summary>
Motivation: 数据提取过程资源消耗大，研究人员希望通过在线大语言模型和审查协议加速该过程。

Method: 使用Claude 3.5 Sonnet模型，结合审查协议，对10个证据来源进行数据提取和审查，评估两种基于协议的提取方法的准确率和性能。

Result: 两种方法对简单明确的引用信息提取准确率较高（83.3%和100%），但对复杂主观数据准确率较低（9.6%和15.8%）；总体上精确率较高（>90%），但召回率和F1分数较低。LLM反馈了部分可改进项，但对故意设置的错误识别能力有限（仅5%）。

Conclusion: 基于审查协议的快速数据提取方法需要更全面的性能评估，建议研究人员在使用LLM进行数据提取时对其性能进行评估和报告，LLM反馈有助于改进审查协议设计。

Abstract: The data extraction stages of reviews are resource-intensive, and researchers
may seek to expediate data extraction using online (large language models) LLMs
and review protocols. Claude 3.5 Sonnet was used to trial two approaches that
used a review protocol to prompt data extraction from 10 evidence sources
included in a case study scoping review. A protocol-based approach was also
used to review extracted data. Limited performance evaluation was undertaken
which found high accuracy for the two extraction approaches (83.3% and 100%)
when extracting simple, well-defined citation details; accuracy was lower (9.6%
and 15.8%) when extracting more complex, subjective data items. Considering all
data items, both approaches had precision >90% but low recall (<25%) and F1
scores (<40%). The context of a complex scoping review, open response types and
methodological approach likely impacted performance due to missed and
misattributed data. LLM feedback considered the baseline extraction accurate
and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of
38 (21.1%) to key findings data items were considered to potentially add value.
However, when repeating the process with a dataset featuring deliberate errors,
only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for
expediency require more robust performance evaluation across a range of LLMs
and review contexts with comparison to conventional prompt engineering
approaches. We recommend researchers evaluate and report LLM performance if
using them similarly to conduct data extraction or review extracted data. LLM
feedback contributed to protocol adaptation and may assist future review
protocol drafting.

</details>


### [109] [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
*Gennadii Iakovlev*

Main category: cs.CL

TL;DR: 本文提出了一种利用人工智能检测议员相互提及和情感倾向的新型精英两极分化度量方法，构建了一个反映政党间敌意的指数，并通过英国、匈牙利和意大利的纵向数据验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前精英政治两极分化的测量方法不足，缺乏对议员间相互评价的情感深度分析，难以捕捉精英间的真实敌对态势。

Method: 利用人工智能技术识别议员在议会演讲中互相提及的行为，识别发言者和对象，并分析评价中的情感倾向，从而构建互相敌意指数以衡量精英两极分化。

Result: 构建的两极分化指数能有效反映政党间的敌意度，数据覆盖英国近40年，匈牙利和意大利20年，指数对选举、危机及政党权力更替等事件反应敏感，表现出良好的面效度。

Conclusion: 该方法为构建欧洲范围内长期精英两极分化时序数据提供了基础，能够为研究政治极化提供更精细且动态的指标支持。

Abstract: This project introduces a new measure of elite polarization via actor and
subject detection using artificial intelligence. I identify when politicians
mention one another in parliamentary speeches, note who is speaking and who is
being addressed, and assess the emotional temperature behind these evaluations.
This maps how elites evaluate their various out-parties, allowing us to create
an index of mutual out-party hostility, that is, elite polarization. While I
analyzed polarization data over the past four decades for the UK, and two
decades for Hungary and Italy, my approach lays the groundwork for a
twenty-year, EU-wide time-series dataset on elite polarization. I obtain the
results that can be aggregated by party and quarter. The resulting index
demonstrates a good face validity: it reacts to events such as electoral
campaigns, country- and party-level crises, and to parties losing and assuming
power.

</details>


### [110] [CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs](https://arxiv.org/abs/2507.06715)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: 本文提出了CLI-RAG框架，利用大语言模型通过分层切分和双阶段检索，实现结构化且临床相关的文本生成，显著提升临床病程记录的时序和语义对齐度。


<details>
  <summary>Details</summary>
Motivation: 临床文本数据高度非结构化且分散，且病程记录文本长度及语义密度大，使得传统的提示方法难以有效利用大语言模型生成高质量临床文本。

Method: 提出CLI-RAG框架，采用分层切分策略保持临床文档结构，并设计全球和局部双阶段检索机制，先筛选相关笔记类型，再提取关键内容，增强信息相关性。

Result: 在MIMIC-III数据集上生成结构化的病程记录，实验结果显示生成文本在时间和语义对齐上优于真实医生记录，对齐评分达到87.7%，高于80.7%的基线；生成结果在多个大语言模型间表现一致。

Conclusion: CLI-RAG有效利用领域特定的检索增强生成策略，提升了临床文本生成的质量和一致性，为临床应用中的文本生成提供了可靠、可复现的方法。

Abstract: Large language models (LLMs), including zero-shot and few-shot paradigms,
have shown promising capabilities in clinical text generation. However,
real-world applications face two key challenges: (1) patient data is highly
unstructured, heterogeneous, and scattered across multiple note types and (2)
clinical notes are often long and semantically dense, making naive prompting
infeasible due to context length constraints and the risk of omitting
clinically relevant information.
  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a
domain-specific framework for structured and clinically grounded text
generation using LLMs. It incorporates a novel hierarchical chunking strategy
that respects clinical document structure and introduces a task-specific
dual-stage retrieval mechanism. The global stage identifies relevant note types
using evidence-based queries, while the local stage extracts high-value content
within those notes creating relevance at both document and section levels.
  We apply the system to generate structured progress notes for individual
hospital visits using 15 clinical note types from the MIMIC-III dataset.
Experiments show that it preserves temporal and semantic alignment across
visits, achieving an average alignment score of 87.7%, surpassing the 80.7%
baseline from real clinician-authored notes. The generated outputs also
demonstrate high consistency across LLMs, reinforcing deterministic behavior
essential for reproducibility, reliability, and clinical trust.

</details>


### [111] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 本文通过分析大型语言模型中确定与不确定输出的层间概率动态，发现不确定性对推理动态影响有限，但更高性能模型处理不确定性的方式不同。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型如何内部表示和处理预测中的不确定性，对于检测不确定性和防止幻觉生成至关重要，但当前对不确定性影响模型处理隐藏状态的研究不足。

Method: 使用Tuned Lens（一种Logit Lens变体）分析11个数据集和5个模型中最终预测词的层级概率轨迹，比较确定与不确定输出的动态。

Result: 结果显示确定和不确定预测的概率轨迹大体一致，均在相似层出现信心水平突然提升；但更高能力模型可能以不同方式处理不确定性。

Conclusion: 不确定性似乎不显著影响推理动态，简单方法检测推理时不确定性存在局限，提出了通过可解释性方法深入研究不确定性影响推理的新思路。

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


### [112] [KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution](https://arxiv.org/abs/2507.06753)
*Ye Kyaw Thu,Thura Aung,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 本文首次将Kolmogorov-Arnold卷积应用于句子分类任务，涉及仇恨言论检测、新闻分类和语言识别。


<details>
  <summary>Details</summary>
Motivation: 解决句子分类中多任务、多类别以及数据不平衡问题，提高模型的性能和解释性。

Method: 比较不同词嵌入（随机、fastText，静态与微调），并结合标准CNN、CNN-KAN以及KAConvText与不同分类头（MLP、KAN）进行实验。

Result: KAConvText-MLP配合微调fastText嵌入在三项任务均取得优异成绩，准确率分别为91.23%、92.66%和99.82%。

Conclusion: KAConvText结合微调词嵌入和MLP分类头有效提升了句子分类性能，同时KAN分类头增强模型解释性。

Abstract: This paper presents the first application of Kolmogorov-Arnold Convolution
for Text (KAConvText) in sentence classification, addressing three tasks:
imbalanced binary hate speech detection, balanced multiclass news
classification, and imbalanced multiclass ethnic language identification. We
investigate various embedding configurations, comparing random to fastText
embeddings in both static and fine-tuned settings, with embedding dimensions of
100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs
and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we
investigated KAConvText with different classification heads - MLP and KAN,
where using KAN head supports enhanced interpretability. Results show that
KAConvText-MLP with fine-tuned fastText embeddings achieves the best
performance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,
92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%
accuracy (F1-score = 0.9982) for language identification.

</details>


### [113] [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
*Mohammad Ghiasvand Mohammadkhani,Hamid Beigy*

Main category: cs.CL

TL;DR: 该论文提出了一种基于检查表工程的训练免费多语言大语言模型评估框架CE-Judge，表现优于基线且与GPT-4o相当。


<details>
  <summary>Details</summary>
Motivation: 当前多语言文本自动评价依赖专有模型或大量训练数据，成本高且效率低，亟需无需训练的多语言评估方法。

Method: 提出基于检查表直觉的训练免费框架CE-Judge，利用开源大语言模型进行多语言文本评价。

Result: 在多语言和三个基准数据集上，CE-Judge在点对点和对对对设置下表现超过基线且接近GPT-4o。

Conclusion: CE-Judge为多语言文本评价提供了一种高效、成本低且无需训练的可行方法，在性能上可匹敌先进的专有模型。

Abstract: Automated text evaluation has long been a central issue in Natural Language
Processing (NLP). Recently, the field has shifted toward using Large Language
Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While
promising and easily adaptable across tasks, this approach has seen limited
exploration in multilingual contexts. Existing multilingual studies often rely
on proprietary models or require extensive training data for fine-tuning,
raising concerns about cost, time, and efficiency. In this paper, we propose
Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free
framework that uses checklist intuition for multilingual evaluation with an
open-source model. Experiments across multiple languages and three benchmark
datasets, under both pointwise and pairwise settings, show that our method
generally surpasses the baselines and performs on par with the GPT-4o model.

</details>


### [114] [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
*Seonwu Kim,Yohan Na,Kihun Kim,Hanhee Cho,Geun Lim,Mintae Kim,Seongik Park,Ki Hyun Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CL

TL;DR: 本文探讨了使用领域自适应持续预训练（DACP）方法提升小型大语言模型（sLLMs）在企业应用中的表现，验证了该方法在多模型和多服务领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 许多企业缺乏部署和维护大规模语言模型的基础设施，小型模型因成本和资源限制成为实际选择，但性能有限，需要提升其针对特定领域的表现。

Method: 采用领域自适应持续预训练（DACP）方法，对不同基础模型进行持续域适应训练，并在多个真实企业服务领域进行实验和评估。

Result: DACP应用后的小型模型在目标领域表现显著提升，同时保持了模型的通用能力，证明了该方法的有效性和实用性。

Conclusion: DACP为企业部署小型语言模型提供了成本效益高且可扩展的解决方案，具有较强的商业应用价值。

Abstract: The emergence of open-source large language models (LLMs) has expanded
opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result,
small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has
been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the
effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations,
we demonstrate that DACP-applied sLLMs achieve substantial gains in target
domain performance while preserving general capabilities, offering a
cost-efficient and scalable solution for enterprise-level deployment.

</details>


### [115] [Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams](https://arxiv.org/abs/2507.06803)
*Matthew Anderson Hendricks,Alice Cicirello*

Main category: cs.CL

TL;DR: 本文提出了一种基于文档语料库和输入文档，实现自动生成动力系统计算模型的策略，结合SysML图、NLP和大语言模型技术，加速动力学系统设计与部署。


<details>
  <summary>Details</summary>
Motivation: 加快工程动力系统的设计和部署过程，自动化生成准确的动力系统计算模型，减少人工工作量并提升性能。

Method: 采用五步策略，利用SysML图提取组件间依赖、属性和操作信息，结合NLP技术和大语言模型提高SysML图的自动生成质量，并通过代码生成与模型生成实现复杂动力系统的计算模型构建。

Result: 通过多个案例验证了自动生成SysML图的适用性，并通过从文本到模型的完整示例（简单摆）展示了该方法相较仅用大语言模型提升了性能。

Conclusion: 该方法通用性强，适用于不同系统、领域及计算软件，显著提升了动力系统建模的自动化和效率。

Abstract: This paper contributes to speeding up the design and deployment of
engineering dynamical systems by proposing a strategy for exploiting domain and
expert knowledge for the automated generation of dynamical system computational
model starting from a corpus of document relevant to the dynamical system of
interest and an input document describing the specific system. This strategy is
implemented in five steps and, crucially, it uses system modeling language
diagrams (SysML) to extract accurate information about the dependencies,
attributes, and operations of components. Natural Language Processing (NLP)
strategies and Large Language Models (LLMs) are employed in specific tasks to
improve intermediate outputs of the SySML diagrams automated generation, such
as: list of key nouns; list of extracted relationships; list of key phrases and
key relationships; block attribute values; block relationships; and BDD diagram
generation. The applicability of automated SysML diagram generation is
illustrated with different case studies. The computational models of complex
dynamical systems from SysML diagrams are then obtained via code generation and
computational model generation steps. In the code generation step, NLP
strategies are used for summarization, while LLMs are used for validation only.
The proposed approach is not limited to a specific system, domain, or
computational software. The applicability of the proposed approach is shown via
an end-to-end example from text to model of a simple pendulum, showing improved
performance compared to results yielded by LLMs only.

</details>


### [116] [Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework](https://arxiv.org/abs/2507.06829)
*Zenan Xu,Zexuan Qiu,Guanhua Huang,Kun Li,Siheng Li,Chenchen Zhang,Kejiao Li,Qi Yi,Yuhao Jiang,Bo Zhou,Fengzong Lian,Zhanhui Kang*

Main category: cs.CL

TL;DR: 本文提出了一种结合顺序推理与并行推理的协同推断框架，通过引入语义熵作为内在质量指标，实现动态控制和推理过程早期终止，提高推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理扩展中存在效率低下或调优复杂的问题，顺序推理和并行推理各有局限，亟需一种灵活且高效的协同推断方法。

Method: 提出语义熵指标衡量并行模型回答的语义多样性，利用该指标动态控制协同推断过程，实现结合顺序与并行推理的推理轨迹管理和早期终止。

Result: 语义熵与准确性呈显著负相关，证明其作为推理质量的指标有效，有助于提升推断过程的效率与准确率。

Conclusion: 结合顺序和并行推理的测试时协同推断框架通过语义熵实现了推理动态管理，解决了现有方法在推理扩展中的不足，推动大语言模型向更高效准确的推理发展。

Abstract: Recent advances in large language models (LLMs) have accelerated progress
toward artificial general intelligence, with inference-time scaling emerging as
a key technique. Contemporary approaches leverage either sequential reasoning
(iteratively extending chains of thought) or parallel reasoning (generating
multiple solutions simultaneously) to scale inference. However, both paradigms
face fundamental limitations: sequential scaling typically relies on arbitrary
token budgets for termination, leading to inefficiency or premature cutoff;
while parallel scaling often lacks coordination among parallel branches and
requires intrusive fine-tuning to perform effectively. In light of these
challenges, we aim to design a flexible test-time collaborative inference
framework that exploits the complementary strengths of both sequential and
parallel reasoning paradigms. Towards this goal, the core challenge lies in
developing an efficient and accurate intrinsic quality metric to assess model
responses during collaborative inference, enabling dynamic control and early
termination of the reasoning trace. To address this challenge, we introduce
semantic entropy (SE), which quantifies the semantic diversity of parallel
model responses and serves as a robust indicator of reasoning quality due to
its strong negative correlation with accuracy...

</details>


### [117] [Shifting from Ranking to Set Selection for Retrieval Augmented Generation](https://arxiv.org/abs/2507.06838)
*Dahyun Lee,Yongrae Jo,Haeju Park,Moontae Lee*

Main category: cs.CL

TL;DR: 本论文提出了一种集合式段落选择方法SETR，用于改进增强检索生成任务中多跳问答的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有增强检索生成方法通常仅根据单个段落的相关性进行重新排序，难以满足复杂多跳查询的信息需求，缺乏整体的综合考虑。

Method: 通过Chain-of-Thought推理明确查询的信息需求，SETR选择一组最优段落，使得这组段落集合能共同满足信息需求，提升多跳问答的检索质量。

Result: 实验结果表明，SETR在多跳RAG基准测试中，在答案正确率和检索质量上均优于现有基于大模型的重排序器和开源基线方法。

Conclusion: SETR作为一种有效且高效的集合式重排序方法，为增强检索生成系统中的多跳问答检索任务提供了优于传统单独重排序器的解决方案。

Abstract: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved
passages are not only individually relevant but also collectively form a
comprehensive set. Existing approaches primarily rerank top-k passages based on
their individual relevance, often failing to meet the information needs of
complex queries in multi-hop question answering. In this work, we propose a
set-wise passage selection approach and introduce SETR, which explicitly
identifies the information requirements of a query through Chain-of-Thought
reasoning and selects an optimal set of passages that collectively satisfy
those requirements. Experiments on multi-hop RAG benchmarks show that SETR
outperforms both proprietary LLM-based rerankers and open-source baselines in
terms of answer correctness and retrieval quality, providing an effective and
efficient alternative to traditional rerankers in RAG systems. The code is
available at https://github.com/LGAI-Research/SetR

</details>


### [118] [Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights](https://arxiv.org/abs/2507.06893)
*Alexandra Abbas,Celia Waggoner,Justin Olive*

Main category: cs.CL

TL;DR: 本文总结了维护开源AI评估库inspect_evals八个月的实践经验，提出了解决AI评估实施与维护中关键挑战的方法。


<details>
  <summary>Details</summary>
Motivation: AI评估对于衡量大语言模型的能力和安全性至关重要，但其实施和维护面临诸多挑战。

Method: 提出了结构化的群体管理框架、统计重采样和交叉模型比较方法以及系统的质量控制流程。

Result: 实现了社区贡献的规模化管理，实现了不确定性量化的统计比较，保证了评估结果的可复现性。

Conclusion: AI评估需要专门的基础设施、统计严谨性和社区协作，超出了传统软件开发的范畴。

Abstract: AI evaluations have become critical tools for assessing large language model
capabilities and safety. This paper presents practical insights from eight
months of maintaining $inspect\_evals$, an open-source repository of 70+
community-contributed AI evaluations. We identify key challenges in
implementing and maintaining AI evaluations and develop solutions including:
(1) a structured cohort management framework for scaling community
contributions, (2) statistical methodologies for optimal resampling and
cross-model comparison with uncertainty quantification, and (3) systematic
quality control processes for reproducibility. Our analysis reveals that AI
evaluation requires specialized infrastructure, statistical rigor, and
community coordination beyond traditional software development practices.

</details>


### [119] [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
*Luca Mariotti,Veronica Guidetti,Federica Mandreoli*

Main category: cs.CL

TL;DR: 该论文提出了一种名为SCoRE的句子级关系抽取系统，结合了对比学习和贝叶斯kNN分类器，实现了无需微调的预训练大语言模型切换，能有效处理低监督噪声数据，并提出了两种新评估指标与一个真实环境数据集，实验表明SCoRE在性能和能耗方面优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前关系抽取需求增长，尤其是在低监督环境下，高效、适应性强且能兼容预训练大语言模型的解决方案亟需开发，以解决噪声数据和多样化语料库的挑战。

Method: 提出SCoRE系统，通过结合有监督对比学习和贝叶斯kNN多标签分类，不需微调即可轻松切换预训练大语言模型，适应多语料和知识图谱，且引入两种新指标（CSD和P@R）及公开数据集Wiki20d。

Result: SCoRE在五个基准测试中表现出与或优于现有最先进方法的性能，同时显著降低能源消耗。进一步分析表明，增加模型复杂度反而降低性能，凸显了SCoRE简约设计的优势。

Conclusion: SCoRE以其高效性、模块化和可扩展性，成为现实世界关系抽取任务的理想选择，证明了无需复杂模型和微调即可实现强鲁棒性的可能性。

Abstract: The growing demand for efficient knowledge graph (KG) enrichment leveraging
external corpora has intensified interest in relation extraction (RE),
particularly under low-supervision settings. To address the need for adaptable
and noise-resilient RE solutions that integrate seamlessly with pre-trained
large language models (PLMs), we introduce SCoRE, a modular and cost-effective
sentence-level RE system. SCoRE enables easy PLM switching, requires no
finetuning, and adapts smoothly to diverse corpora and KGs. By combining
supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)
classifier for multi-label classification, it delivers robust performance
despite the noisy annotations of distantly supervised corpora. To improve RE
evaluation, we propose two novel metrics: Correlation Structure Distance (CSD),
measuring the alignment between learned relational patterns and KG structures,
and Precision at R (P@R), assessing utility as a recommender system. We also
release Wiki20d, a benchmark dataset replicating real-world RE conditions where
only KG-derived annotations are available. Experiments on five benchmarks show
that SCoRE matches or surpasses state-of-the-art methods while significantly
reducing energy consumption. Further analyses reveal that increasing model
complexity, as seen in prior work, degrades performance, highlighting the
advantages of SCoRE's minimal design. Combining efficiency, modularity, and
scalability, SCoRE stands as an optimal choice for real-world RE applications.

</details>


### [120] [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
*Ziang Ye,Yang Zhang,Wentao Shi,Xiaoyu You,Fuli Feng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文揭示了基于大型视觉语言模型的图形用户界面代理中存在的新型后门攻击漏洞，该攻击利用视觉定位环节误导代理执行错误操作。


<details>
  <summary>Details</summary>
Motivation: 随着GUI代理深度整合个人设备，其安全风险被忽视，尤其是后门攻击，此文弥补了视觉定位相关安全隐患的研究空白。

Method: 提出VisualTrap方法，在视觉定位预训练阶段注入少量隐蔽的有毒数据，误导代理将文本指令映射到触发点而非目标元素，实现后门攻击。

Result: 实验证明VisualTrap可用约5%的有毒数据成功劫持视觉定位，触发器对人眼隐形，且攻击效果能推广至多种任务和不同GUI环境，且清洗微调后依然有效。

Conclusion: 研究强调了GUI代理存在视觉定位后门风险，呼吁业界加强该领域的安全研究与防护措施。

Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models
(LVLMs) have emerged as a revolutionary approach to automating human-machine
interactions, capable of autonomously operating personal devices (e.g., mobile
phones) or applications within the device to perform complex real-world tasks
in a human-like manner. However, their close integration with personal devices
raises significant security concerns, with many threats, including backdoor
attacks, remaining largely unexplored. This work reveals that the visual
grounding of GUI agent-mapping textual plans to GUI elements-can introduce
vulnerabilities, enabling new types of backdoor attacks. With backdoor attack
targeting visual grounding, the agent's behavior can be compromised even when
given correct task-solving plans. To validate this vulnerability, we propose
VisualTrap, a method that can hijack the grounding by misleading the agent to
locate textual plans to trigger locations instead of the intended targets.
VisualTrap uses the common method of injecting poisoned data for attacks, and
does so during the pre-training of visual grounding to ensure practical
feasibility of attacking. Empirical results show that VisualTrap can
effectively hijack visual grounding with as little as 5% poisoned data and
highly stealthy visual triggers (invisible to the human eye); and the attack
can be generalized to downstream tasks, even after clean fine-tuning. Moreover,
the injected trigger can remain effective across different GUI environments,
e.g., being trained on mobile/web and generalizing to desktop environments.
These findings underscore the urgent need for further research on backdoor
attack risks in GUI agents.

</details>


### [121] [MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection](https://arxiv.org/abs/2507.06908)
*Ziyan Liu,Chunxiao Fan,Haoran Lou,Yuexin Wu,Kaiwei Deng*

Main category: cs.CL

TL;DR: 提出了MIND框架，用于无注释数据的零样本有害迷因检测，通过检索相似迷因、双向洞察机制和多智能体辩论，实现了较好效果和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上迷因快速发展，传统方法难以检测新出现的有害迷因，且缺乏最新标注数据，亟需无标注数据的有效检测方法。

Method: 提出MIND多智能体框架，包含三个核心策略：从未标注参考集中检索相似迷因；通过双向洞察机制深入理解相似迷因；利用多智能体辩论机制进行合理的决策仲裁。

Result: 在三个迷因数据集上的大规模实验显示，MIND优于现有零样本方法，且在不同模型结构和参数规模下表现出强泛化能力。

Conclusion: MIND为有害迷因的零样本检测提供了一个可扩展且鲁棒的解决方案，适用于无注释数据环境。

Abstract: The rapid expansion of memes on social media has highlighted the urgent need
for effective approaches to detect harmful content. However, traditional
data-driven approaches struggle to detect new memes due to their evolving
nature and the lack of up-to-date annotated data. To address this issue, we
propose MIND, a multi-agent framework for zero-shot harmful meme detection that
does not rely on annotated data. MIND implements three key strategies: 1) We
retrieve similar memes from an unannotated reference set to provide contextual
information. 2) We propose a bi-directional insight derivation mechanism to
extract a comprehensive understanding of similar memes. 3) We then employ a
multi-agent debate mechanism to ensure robust decision-making through reasoned
arbitration. Extensive experiments on three meme datasets demonstrate that our
proposed framework not only outperforms existing zero-shot approaches but also
shows strong generalization across different model architectures and parameter
scales, providing a scalable solution for harmful meme detection. The code is
available at https://github.com/destroy-lonely/MIND.

</details>


### [122] [MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction](https://arxiv.org/abs/2507.06909)
*Xiao Wang,Jiahuan Pei,Diancheng Shui,Zhiguang Han,Xin Sun,Dawei Zhu,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一个新的多被告多指控法律判决预测数据集MPMCP，评估不同模型在单被告单指控、单被告多指控、多被告单指控、多被告多指控四种场景下的表现，发现多被告多指控场景最具挑战性。


<details>
  <summary>Details</summary>
Motivation: 法律判决预测领域中，多被告和多指控是否应分开处理的问题尚未充分探讨。

Method: 构建MPMCP数据集，基于四种实际法律判决场景，使用多个主流法律大语言模型对指控预测和刑期预测任务进行性能评估。

Result: 多被告多指控场景（S4）性能最差，不同模型表现差异显著，如InternLM2和Lawformer在S4表现均下降明显。

Conclusion: 多被告多指控场景对法律判决预测提出更大挑战，模型性能因场景和模型不同而异，提示未来研究需针对复杂场景优化模型。

Abstract: Legal judgment prediction offers a compelling method to aid legal
practitioners and researchers. However, the research question remains
relatively under-explored: Should multiple defendants and charges be treated
separately in LJP? To address this, we introduce a new dataset namely
multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating
the performance of several prevailing legal large language models (LLMs) on
four practical legal judgment scenarios: (S1) single defendant with a single
charge, (S2) single defendant with multiple charges, (S3) multiple defendants
with a single charge, and (S4) multiple defendants with multiple charges. We
evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty
term prediction. We have conducted extensive experiments and found that the
scenario involving multiple defendants and multiple charges (S4) poses the
greatest challenges, followed by S2, S3, and S1. The impact varies
significantly depending on the model. For example, in S4 compared to S1,
InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,
while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.
Our dataset and code are available at
https://github.com/lololo-xiao/MultiJustice-MPMCP.

</details>


### [123] [Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues](https://arxiv.org/abs/2507.06910)
*Fareya Ikram,Alexander Scarlatos,Andrew Lan*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（如Llama 3和GPT-4o）在预测数学辅导对话中导师策略和学生结果的能力，发现现有模型在预测导师策略方面表现不佳，但导师策略与学生结果高度相关。


<details>
  <summary>Details</summary>
Motivation: 近年来在线学习和基于大型语言模型的AI辅导能力兴起，导师策略对学生学习效果有显著影响，急需预测导师策略及其对学生影响的方法。

Method: 利用两个数学辅导对话数据集，评估现代大型语言模型（Llama 3和GPT-4o）预测未来导师行为和学生学习结果的能力。

Result: 即使是最先进的大型语言模型，也难以准确预测未来导师策略，但导师策略对学生结果具有重要指示作用。

Conclusion: 目前的语言模型在辅导对话中预测导师策略能力有限，表明需要更强大的方法来改进这一任务。

Abstract: Tutoring dialogues have gained significant attention in recent years, given
the prominence of online learning and the emerging tutoring abilities of
artificial intelligence (AI) agents powered by large language models (LLMs).
Recent studies have shown that the strategies used by tutors can have
significant effects on student outcomes, necessitating methods to predict how
tutors will behave and how their actions impact students. However, few works
have studied predicting tutor strategy in dialogues. Therefore, in this work we
investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to
predict both future tutor moves and student outcomes in dialogues, using two
math tutoring dialogue datasets. We find that even state-of-the-art LLMs
struggle to predict future tutor strategy while tutor strategy is highly
indicative of student outcomes, outlining a need for more powerful methods to
approach this task.

</details>


### [124] [Rethinking Verification for LLM Code Generation: From Generation to Testing](https://arxiv.org/abs/2507.06920)
*Zihan Ma,Taolin Zhang,Maosong Cao,Wenwei Zhang,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文针对大型语言模型代码生成评测中的测试用例不足，提出了一种多维度指标量化测试用例完整性，并通过人机协作方式提升测试用例质量与覆盖，搭建了相关测试平台，显著提高了评测准确率。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成评测数据集中的测试用例数量有限且同质化，导致难以发现细微错误，影响性能评估的真实性和强化学习奖励的准确性。

Method: 设计多维度指标用于严格量化测试用例的全面性，提出人机协作方法SAGA结合人类编程知识与大语言模型推理能力以提升测试用例的覆盖率与质量，同时构建测试用例生成基准数据集TCGBench。

Result: SAGA在TCGBench的缺陷检测率达到90.62%，验证器准确率为32.58%，同时合成的评测基准验证器准确率比LiveCodeBench-v6提升了10.78%。

Conclusion: 所提方法有效增强了测试用例生成的覆盖与质量，为大型语言模型代码生成的可靠评测奠定基础，促进强化学习奖励估计的准确性，推动自动化对抗测试与自适应基准的研究发展。

Abstract: Large language models (LLMs) have recently achieved notable success in
code-generation benchmarks such as HumanEval and LiveCodeBench. However, a
detailed examination reveals that these evaluation suites often comprise only a
limited number of homogeneous test cases, resulting in subtle faults going
undetected. This not only artificially inflates measured performance but also
compromises accurate reward estimation in reinforcement learning frameworks
utilizing verifiable rewards (RLVR). To address these critical shortcomings, we
systematically investigate the test-case generation (TCG) task by proposing
multi-dimensional metrics designed to rigorously quantify test-suite
thoroughness. Furthermore, we introduce a human-LLM collaborative method
(SAGA), leveraging human programming expertise with LLM reasoning capability,
aimed at significantly enhancing both the coverage and the quality of generated
test cases. In addition, we develop a TCGBench to facilitate the study of the
TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a
verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)
of the code generation evaluation benchmark synthesized by SAGA is 10.78%
higher than that of LiveCodeBench-v6. These results demonstrate the
effectiveness of our proposed method. We hope this work contributes to building
a scalable foundation for reliable LLM code evaluation, further advancing RLVR
in code generation, and paving the way for automated adversarial test synthesis
and adaptive benchmark integration.

</details>


### [125] [Investigating the Robustness of Retrieval-Augmented Generation at the Query Level](https://arxiv.org/abs/2507.06956)
*Sezen Perçin,Xin Su,Qutub Sha Syed,Phillip Howard,Aleksei Kuvshinov,Leo Schwinn,Kay-Ulrich Scholl*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）系统对查询扰动的敏感性，发现检索器对小幅查询变化性能显著下降，并提出了评估框架和改进建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型更新成本高，RAG作为动态引入外部知识的方法提升了事实一致性和减少幻觉，但依赖输入查询质量存在实际挑战。

Method: 系统分析RAG流水线中不同组件对查询扰动的敏感度，单独及联合评估检索器性能，使用通用和特定领域数据集，进行1092次实验，提出查询级鲁棒性评估框架。

Result: 检索器性能在轻微查询扰动下显著下降，各模块对鲁棒性的影响明确，评估框架有效揭示了系统弱点。

Conclusion: 查询质量对RAG系统性能影响巨大，需关注查询鲁棒性，评估框架和建议为研发实践提供指导。

Abstract: Large language models (LLMs) are very costly and inefficient to update with
new information. To address this limitation, retrieval-augmented generation
(RAG) has been proposed as a solution that dynamically incorporates external
knowledge during inference, improving factual consistency and reducing
hallucinations. Despite its promise, RAG systems face practical challenges-most
notably, a strong dependence on the quality of the input query for accurate
retrieval. In this paper, we investigate the sensitivity of different
components in the RAG pipeline to various types of query perturbations. Our
analysis reveals that the performance of commonly used retrievers can degrade
significantly even under minor query variations. We study each module in
isolation as well as their combined effect in an end-to-end question answering
setting, using both general-domain and domain-specific datasets. Additionally,
we propose an evaluation framework to systematically assess the query-level
robustness of RAG pipelines and offer actionable recommendations for
practitioners based on the results of more than 1092 experiments we performed.

</details>


### [126] [FRaN-X: FRaming and Narratives-eXplorer](https://arxiv.org/abs/2507.06974)
*Artur Muratov,Hana Fatima Shaikh,Vanshikaa Jani,Tarek Mahmoud,Zhuohan Xie,Daniil Orel,Aaryamonvikram Singh,Yuxia Wang,Aadi Joshi,Hasan Iqbal,Ming Shan Hee,Dhruv Sahnan,Nikolaos Nikolaidis,Purificação Silvano,Dimitar Dimitrov,Roman Yangarber,Ricardo Campos,Alípio Jorge,Nuno Guimarães,Elisa Sartori,Nicolas Stefanovitch,Giovanni Da San Martino,Jakub Piskorski,Preslav Nakov*

Main category: cs.CL

TL;DR: FRaN-X是一种自动检测实体提及并分类叙事角色的系统，支持五种语言和两个领域，帮助媒体分析师探索和比较信息框架。


<details>
  <summary>Details</summary>
Motivation: 自动检测和标注实体在文本中的叙事角色，解决不同媒体来源中框架差异的分析挑战。

Method: 采用两阶段系统，结合序列标注和细粒度角色分类，使用涵盖22种细粒度角色的分类体系，支持多语言和多领域。

Result: 系统提供交互式网页界面，可同时分析多篇文章，支持实体检索和时间线追踪，具备直观的图形可视化功能。

Conclusion: FRaN-X有效促进了对文本中实体叙事角色的自动理解和多源信息框架比较，已开放使用，提升了媒体分析的效率和深度。

Abstract: We present FRaN-X, a Framing and Narratives Explorer that automatically
detects entity mentions and classifies their narrative roles directly from raw
text. FRaN-X comprises a two-stage system that combines sequence labeling with
fine-grained role classification to reveal how entities are portrayed as
protagonists, antagonists, or innocents, using a unique taxonomy of 22
fine-grained roles nested under these three main categories. The system
supports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)
and two domains (the Russia-Ukraine Conflict and Climate Change). It provides
an interactive web interface for media analysts to explore and compare framing
across different sources, tackling the challenge of automatically detecting and
labeling how entities are framed. Our system allows end users to focus on a
single article as well as analyze up to four articles simultaneously. We
provide aggregate level analysis including an intuitive graph visualization
that highlights the narrative a group of articles are pushing. Our system
includes a search feature for users to look up entities of interest, along with
a timeline view that allows analysts to track an entity's role transitions
across different contexts within the article. The FRaN-X system and the trained
models are licensed under an MIT License. FRaN-X is publicly accessible at
https://fran-x.streamlit.app/ and a video demonstration is available at
https://youtu.be/VZVi-1B6yYk.

</details>


### [127] [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
*Weijia Shi,Akshita Bhagia,Kevin Farhat,Niklas Muennighoff,Pete Walsh,Jacob Morrison,Dustin Schwenk,Shayne Longpre,Jake Poznanski,Allyson Ettinger,Daogao Liu,Margaret Li,Dirk Groeneveld,Mike Lewis,Wen-tau Yih,Luca Soldaini,Kyle Lo,Noah A. Smith,Luke Zettlemoyer,Pang Wei Koh,Hannaneh Hajishirzi,Ali Farhadi,Sewon Min*

Main category: cs.CL

TL;DR: FlexOlmo是一种新型语言模型，支持分布式训练且数据不共享，并允许灵活的数据访问推断。


<details>
  <summary>Details</summary>
Motivation: 解决在保护数据隐私、法规限制下，如何利用多个闭源数据集训练大规模语言模型的问题。

Method: 基于混合专家架构(MoE)，各专家独立训练于各自闭源数据，无需联合训练，通过领域知情路由集成专家模型。训练使用FlexMix语料库，包括公开和七个领域特定数据集。

Result: 在37亿参数（其中20亿活跃）模型上，结合通用专家和独立训练专家后，平均提升41%效果，优于现有模型合并方法10.1%，也超过无数据限制的标准MoE模型。

Conclusion: FlexOlmo为受监管行业提供了有效利用闭源敏感数据的解决方案，支持数据本地保留和细粒度推断控制，保障数据所有者权益。

Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1)
distributed training without data sharing, where different model parameters are
independently trained on closed datasets, and (2) data-flexible inference,
where these parameters along with their associated data can be flexibly
included or excluded from model inferences with no further training. FlexOlmo
employs a mixture-of-experts (MoE) architecture where each expert is trained
independently on closed datasets and later integrated through a new
domain-informed routing without any joint training. FlexOlmo is trained on
FlexMix, a corpus we curate comprising publicly available datasets alongside
seven domain-specific sets, representing realistic approximations of closed
sets. We evaluate models with up to 37 billion parameters (20 billion active)
on 31 diverse downstream tasks. We show that a general expert trained on public
data can be effectively combined with independently trained experts from other
data owners, leading to an average 41% relative improvement while allowing
users to opt out of certain data based on data licensing or permission
requirements. Our approach also outperforms prior model merging methods by
10.1% on average and surpasses the standard MoE trained without data
restrictions using the same training FLOPs. Altogether, this research presents
a solution for both data owners and researchers in regulated industries with
sensitive or protected data. FlexOlmo enables benefiting from closed data while
respecting data owners' preferences by keeping their data local and supporting
fine-grained control of data access during inference.

</details>


### [128] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出了将密集检索和生成响应统一到大语言模型中的方法，提升对话式搜索系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话搜索系统分离了检索与生成模型，限制了系统整体性能，且统一模型面临理解对话上下文、独立管理检索和生成响应的挑战。

Method: 通过联合微调不同目标，设计两种机制减少模型不一致风险和数据差异，对大语言模型进行统一训练。

Result: 在五个对话搜索数据集上的评测表明，统一模型在检索和生成任务上均取得提升，并优于现有基线方法。

Conclusion: 统一密集检索与响应生成的模型能够有效整合信息，提升对话搜索系统的整体性能。

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


### [129] [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
*Ashen Weligalle*

Main category: cs.CL

TL;DR: 本文探讨了扩散模型在自然语言生成中的可行性和性能，比较了离散去噪扩散概率模型（D3PM）与传统自回归（AR）语言模型的优劣。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在连续数据生成中表现优异，但在离散自然语言生成中面临生成顺序和词元依赖复杂性的挑战。本文旨在评估扩散模型在该场景下的表现及潜力。

Method: 评估离散去噪扩散概率模型（D3PM）与自回归模型在Bits Per Token (BPT)、负对数似然（NLL）、困惑度（PPL）和批处理速度上的表现，100,000词元生成，固定批次大小为4，确保公平比较。

Result: D3PM最佳模型BPT为5.72，平均8.05，处理速度高达3.97批次每秒；AR模型压缩性能更优，平均BPT为4.59，但处理速度较低。

Conclusion: 扩散模型在语言生成中展示了较高并行处理潜力，但在生成质量上仍不及AR模型。研究强调了扩散模型在离散数据生成中的优势与局限，支持未来非自回归语言生成研究方向。

Abstract: Diffusion models have emerged as a powerful class of generative models,
achieving state-of-the-art results in continuous data domains such as image and
video generation. Their core mechanism involves a forward diffusion process
that gradually transforms structured data into a Gaussian-like distribution,
followed by a learned reverse process to reconstruct the data. While successful
in continuous modalities, applying this framework to discrete data-particularly
natural language-remains challenging due to token dependency complexities and
the lack of a defined generation order.This thesis investigates the feasibility
and performance of discrete diffusion models for natural language generation.
Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model
(D3PM) and compare it with traditional autoregressive (AR) language models. To
assess generative performance, we use Bits Per Token (BPT), Negative
Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.
  Results show the best-performing D3PM model achieves a BPT of 5.72, with a
mean of 8.05. The AR model outperforms in compression with a lower mean BPT of
4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches
per sec., indicating potential for parallel generation.All evaluations were
conducted under consistent conditions-generating 100,000 tokens per model with
a fixed batch size of four-for fair comparison. This research presents a
detailed analysis of diffusion-based vs. autoregressive models, highlighting
trade-offs in generative quality and efficiency. Findings emphasize both the
promise and limitations of diffusion models for discrete data, supporting
future work in non-autoregressive language generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [130] [Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction](https://arxiv.org/abs/2507.06346)
*Li Zhou,Elvan Ceyhan*

Main category: cs.RO

TL;DR: 该论文研究了一种资源受限的随机路径消歧问题，提出了新算法COLOGR以优化路径规划。


<details>
  <summary>Details</summary>
Motivation: 面对空间环境中存在不确定障碍的导航问题，且资源受限，需要在全局预算下进行路径规划。

Method: 将问题建模为带权约束最短路径问题，提出结合拉格朗日松弛和两阶段顶点消除的新算法框架COLOGR，通过修剪不可行和次优路径并利用对偶界限高效搜索。

Result: COLOGR算法保证解的正确性和可行性，普遍达到零对偶间隙，计算复杂度优于先前方法。大量仿真表明算法在各种障碍密度、传感精度和风险模型下表现优良，超过贪婪基线方法，接近离线最优。

Conclusion: COLOGR框架有效解决资源受限下的随机路径规划问题，具有广泛应用潜力，适用于随机网络设计、运动规划和不确定性下的约束决策。

Abstract: We study a resource-constrained variant of the Random Disambiguation Path
(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,
in which a navigating agent must reach a target in a spatial environment
populated with uncertain obstacles. Each ambiguous obstacle may be
disambiguated at a (possibly) heterogeneous resource cost, subject to a global
disambiguation budget. We formulate this constrained planning problem as a
Weight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs
that incorporate probabilistic blockage and traversal penalties. To solve it,
we propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation
with a two-phase vertex elimination (TPVE) procedure. The method prunes
infeasible and suboptimal paths while provably preserving the optimal solution,
and leverages dual bounds to guide efficient search. We establish correctness,
feasibility guarantees, and surrogate optimality under mild assumptions. Our
analysis also demonstrates that COLOGR frequently achieves zero duality gap and
offers improved computational complexity over prior constrained path-planning
methods. Extensive simulation experiments validate the algorithm's robustness
across varying obstacle densities, sensor accuracies, and risk models,
consistently outperforming greedy baselines and approaching offline-optimal
benchmarks. The proposed framework is broadly applicable to stochastic network
design, mobility planning, and constrained decision-making under uncertainty.

</details>


### [131] [Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System](https://arxiv.org/abs/2507.06397)
*Michalis Chatzispyrou,Luke Horgan,Hyunkil Hwang,Harish Sathishchandra,Monika Roznere,Alberto Quattrini Li,Philippos Mordohai,Ioannis Rekleitis*

Main category: cs.RO

TL;DR: 本文提出了一种利用廉价运动相机和潜水计算机结合视觉惯性SLAM（SVIn2）及全局优化（COLMAP）技术，实现水下洞穴的轮廓及三维图像重建的方法，并通过手动测量验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 水下洞穴在淡水资源管理、水下考古和水文地质中极为重要，准确绘制其轮廓和尺寸以及 photorealistic 3D 地图对于更好地理解该领域至关重要。

Method: 利用运动相机和潜水计算机估计相机轨迹和稀疏点云，结合视觉惯性SLAM（SVIn2）进行轨迹估计和关键帧生成，随后用 COLMAP 全局优化进行选定区域的密集三维重建，同时通过 MNemo V2 手动测量进行验证。

Result: 实现了洞穴通道一维轨迹连同上下左右边界的重建，并成功生成了选定区域的高真实感密集三维模型，证明了廉价硬件配合视觉惯性框架及全局优化的可行性。

Conclusion: 基于运动相机和视觉惯性SLAM的技术能够有效绘制水下洞穴的主要构造，而结合全局优化框架能生成高质量的密集三维重建，为水下洞穴的研究和应用提供了强有力的技术支持。

Abstract: This paper presents a framework for mapping underwater caves. Underwater
caves are crucial for fresh water resource management, underwater archaeology,
and hydrogeology. Mapping the cave's outline and dimensions, as well as
creating photorealistic 3D maps, is critical for enabling a better
understanding of this underwater domain. In this paper, we present the mapping
of an underwater cave segment (the catacombs) of the Devil's Eye cave system at
Ginnie Springs, FL. We utilized a set of inexpensive action cameras in
conjunction with a dive computer to estimate the trajectories of the cameras
together with a sparse point cloud. The resulting reconstructions are utilized
to produce a one-dimensional retract of the cave passages in the form of the
average trajectory together with the boundaries (top, bottom, left, and right).
The use of the dive computer enables the observability of the z-dimension in
addition to the roll and pitch in a visual/inertial framework (SVIn2). In
addition, the keyframes generated by SVIn2 together with the estimated camera
poses for select areas are used as input to a global optimization (bundle
adjustment) framework -- COLMAP -- in order to produce a dense reconstruction
of those areas. The same cave segment is manually surveyed using the MNemo V2
instrument, providing an additional set of measurements validating the proposed
approach. It is worth noting that with the use of action cameras, the primary
components of a cave map can be constructed. Furthermore, with the utilization
of a global optimization framework guided by the results of VI-SLAM package
SVIn2, photorealistic dense 3D representations of selected areas can be
reconstructed.

</details>


### [132] [Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction](https://arxiv.org/abs/2507.06404)
*Matteo Tiezzi,Tommaso Apicella,Carlos Cardenas-Perez,Giovanni Fregonese,Stefano Dafarra,Pietro Morerio,Daniele Pucci,Alessio Del Bue*

Main category: cs.RO

TL;DR: 提出了一个基于轨迹性能的模仿学习评估框架，利用神经元元评估器（NeME）衡量类人机器人控制策略的表现，无需人工干预评价，实验证明该方法与机器人实际成功率更一致。


<details>
  <summary>Details</summary>
Motivation: 传统的自主类人机器人性能评价难以复现且无法全面反映动作轨迹复杂性，这限制了在人机交互协作中的应用效果评估。

Method: 设计了神经元元评估器NeME，一个深度学习模型，通过分析机器人关节轨迹来分类动作，作为元评估器比较控制策略的性能，消除人工介入的需要。

Result: 在ergoCub类人机器人上用远程操作数据验证，NeME方法与实际成功率高度一致，优于基线方法，能系统且可复现地比较多模态模仿学习方法的性能。

Conclusion: NeME提供了一种可靠且自动化的评价手段，有助于推动复杂人机协作任务中模仿学习方法的性能比较与提升。

Abstract: Evaluating and comparing the performance of autonomous Humanoid Robots is
challenging, as success rate metrics are difficult to reproduce and fail to
capture the complexity of robot movement trajectories, critical in Human-Robot
Interaction and Collaboration (HRIC). To address these challenges, we propose a
general evaluation framework that measures the quality of Imitation Learning
(IL) methods by focusing on trajectory performance. We devise the Neural Meta
Evaluator (NeME), a deep learning model trained to classify actions from robot
joint trajectories. NeME serves as a meta-evaluator to compare the performance
of robot control policies, enabling policy evaluation without requiring human
involvement in the loop. We validate our framework on ergoCub, a humanoid
robot, using teleoperation data and comparing IL methods tailored to the
available platform. The experimental results indicate that our method is more
aligned with the success rate obtained on the robot than baselines, offering a
reproducible, systematic, and insightful means for comparing the performance of
multimodal imitation learning approaches in complex HRI tasks.

</details>


### [133] [Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion](https://arxiv.org/abs/2507.06426)
*Devin Crowley,Whitney G. Cole,Christina M. Hospodar,Ruiting Shen,Karen E. Adolph,Alan Fern*

Main category: cs.RO

TL;DR: 本文通过开发心理学方法系统研究双足机器人Cassie的学习行为，揭示不同训练方案对行为发展的影响。


<details>
  <summary>Details</summary>
Motivation: 传统机器人控制器训练和评估方法过于粗糙，无法细致理解训练方案对学习行为的影响，借鉴婴儿发展心理学的细粒度实验评价方法，以弥补这一不足。

Method: 设计仿真环境和训练方案，模拟婴儿学习走路的条件，采用强化学习训练Cassie机器人，并对其行为进行细致评估。

Result: 结果显示不同训练方案显著影响Cassie机器人的学习行为，且行为发展模式与婴儿学习走路有相似之处，获得了新的行为学见解。

Conclusion: 跨学科结合婴儿心理学与机器人学习研究方法，有助于系统理解训练方案对复杂机器人行为发展的影响，指导未来机器人训练设计。

Abstract: Typically, learned robot controllers are trained via relatively unsystematic
regimens and evaluated with coarse-grained outcome measures such as average
cumulative reward. The typical approach is useful to compare learning
algorithms but provides limited insight into the effects of different training
regimens and little understanding about the richness and complexity of learned
behaviors. Likewise, human infants and other animals are "trained" via
unsystematic regimens, but in contrast, developmental psychologists evaluate
their performance in highly-controlled experiments with fine-grained measures
such as success, speed of walking, and prospective adjustments. However, the
study of learned behavior in human infants is limited by the practical
constraints of training and testing babies. Here, we present a case study that
applies methods from developmental psychology to study the learned behavior of
the simulated bipedal robot Cassie. Following research on infant walking, we
systematically designed reinforcement learning training regimens and tested the
resulting controllers in simulated environments analogous to those used for
babies--but without the practical constraints. Results reveal new insights into
the behavioral impact of different training regimens and the development of
Cassie's learned behaviors relative to infants who are learning to walk. This
interdisciplinary baby-robot approach provides inspiration for future research
designed to systematically test effects of training on the development of
complex learned robot behaviors.

</details>


### [134] [Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies](https://arxiv.org/abs/2507.06519)
*Yuhan Liu,Xinyu Zhang,Haonan Chang,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 本文针对机器人在高精度反复插入任务中的难题，提出了结合强化学习和失败预测的仿真到真实框架，提升了任务成功率和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 机器人在执行毫米级精度的重复插入任务时，受到螺母旋转和摩擦等复杂因素影响，难以保证高成功率和稳定性。

Method: 提出基于强化学习的插入策略与失败预测模块相结合的方法，通过在螺母坐标系表示扳手姿态，利用仿真训练结合实时6D姿态跟踪，实现精确插入并在失败时自动恢复。

Result: 在模拟和真实环境下，方法不仅实现了高一次性成功率，还表现出在长时间重复任务中的鲁棒性和稳定性。

Conclusion: 该方法有效提升了高精度重复插入任务的成功率和稳定性，具备优良的仿真到真实迁移能力，适用于复杂的工业自动化场景。

Abstract: This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where
a robot must repeatedly perform high-precision insertions, such as screwing a
nut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving
millimeter-level accuracy and maintaining consistent performance over multiple
repetitions, particularly when factors like nut rotation and friction introduce
additional complexity. We propose a sim-to-real framework that integrates a
reinforcement learning-based insertion policy with a failure forecasting
module. By representing the wrench's pose in the nut's coordinate frame rather
than the robot's frame, our approach significantly enhances sim-to-real
transferability. The insertion policy, trained in simulation, leverages
real-time 6D pose tracking to execute precise alignment, insertion, and
rotation maneuvers. Simultaneously, a neural network predicts potential
execution failures, triggering a simple recovery mechanism that lifts the
wrench and retries the insertion. Extensive experiments in both simulated and
real-world environments demonstrate that our method not only achieves a high
one-time success rate but also robustly maintains performance over long-horizon
repetitive tasks.

</details>


### [135] [KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing](https://arxiv.org/abs/2507.06562)
*Keita Yoneda,Kento Kawaharazuka,Temma Suzuki,Takahiro Hattori,Kei Okada*

Main category: cs.RO

TL;DR: 本文提出了一种带腰关节的四足机器人KLEIYN，通过引入接触引导课程学习（CGCL），实现了快速且稳定的垂直爬墙运动，显著提升了机器人在狭窄空间中的运动能力。


<details>
  <summary>Details</summary>
Motivation: 当前四足机器人在复杂起伏地形的自主行走中，尤其是需垂直移动的任务仍缺乏稳定的控制方法和机器人设计。

Method: 设计了带腰关节的机器人KLEIYN，采用强化学习结合接触引导课程学习(CGCL)方法，拓展四足机器人的运动能力，实现烟囱式爬墙。

Result: KLEIYN能以150 mm/s的速度成功爬上宽度800至1000 mm的墙面，速度是传统机器人的50倍。腰关节显著提高了狭窄墙面上的跟踪和稳定性。

Conclusion: 通过改进机器人结构和学习算法，研究实现了高效稳定的四足机器人垂直爬墙，推动了四足机器人在复杂地形中的应用潜力。

Abstract: In recent years, advancements in hardware have enabled quadruped robots to
operate with high power and speed, while robust locomotion control using
reinforcement learning (RL) has also been realized. As a result, expectations
are rising for the automation of tasks such as material transport and
exploration in unknown environments. However, autonomous locomotion in rough
terrains with significant height variations requires vertical movement, and
robots capable of performing such movements stably, along with their control
methods, have not yet been fully established. In this study, we developed the
quadruped robot KLEIYN, which features a waist joint, and aimed to expand
quadruped locomotion by enabling chimney climbing through RL. To facilitate the
learning of vertical motion, we introduced Contact-Guided Curriculum Learning
(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to
1000 mm in width at an average speed of 150 mm/s, 50 times faster than
conventional robots. Furthermore, we demonstrated that the introduction of a
waist joint improves climbing performance, particularly enhancing tracking
ability on narrow walls.

</details>


### [136] [SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments](https://arxiv.org/abs/2507.06564)
*Tianshun Li,Tianyi Huai,Zhen Li,Yichun Gao,Haoang Li,Xinhu Zheng*

Main category: cs.RO

TL;DR: 本论文提出了SkyVLN框架，结合视觉语言导航和非线性模型预测控制，提升无人机在复杂城市环境中的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 提升无人机在复杂动态城市环境中的导航精度和鲁棒性，克服传统导航方法的局限。

Method: 设计了结合大语言模型的多模态导航代理，包含细粒度空间语言器和路径记忆机制，并集成非线性模型预测控制模块实现动态避障和轨迹跟踪。

Result: 在基于AirSim的高保真三维城市仿真环境中，SkyVLN显著提高了导航成功率和效率，尤其在新环境中表现优异。

Conclusion: SkyVLN有效融合视觉语言导航和动态控制技术，增强了无人机在复杂动态城市环境中的自主导航能力，具备良好的实用价值。

Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across
various sectors, driven by their mobility and adaptability. This paper
introduces SkyVLN, a novel framework integrating vision-and-language navigation
(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in
complex urban environments. Unlike traditional navigation methods, SkyVLN
leverages Large Language Models (LLMs) to interpret natural language
instructions and visual observations, enabling UAVs to navigate through dynamic
3D spaces with improved accuracy and robustness. We present a multimodal
navigation agent equipped with a fine-grained spatial verbalizer and a history
path memory mechanism. These components allow the UAV to disambiguate spatial
contexts, handle ambiguous instructions, and backtrack when necessary. The
framework also incorporates an NMPC module for dynamic obstacle avoidance,
ensuring precise trajectory tracking and collision prevention. To validate our
approach, we developed a high-fidelity 3D urban simulation environment using
AirSim, featuring realistic imagery and dynamic urban elements. Extensive
experiments demonstrate that SkyVLN significantly improves navigation success
rates and efficiency, particularly in new and unseen environments.

</details>


### [137] [AI Space Cortex: An Experimental System for Future Era Space Exploration](https://arxiv.org/abs/2507.06574)
*Thomas Touma,Ersin Daş,Erica Tevere,Martin Feather,Ksenia Kolcio,Maurice Prather,Alberto Candela,Ashish Goel,Erik Kramer,Hari Nayar,Lorraine Fesq,Joel W. Burdick*

Main category: cs.RO

TL;DR: REASIMO项目为NASA探索冰海洋世界任务开发了具有自主异常检测与恢复能力的智能控制框架，着重解决通信延迟和严苛环境带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 未来冰海洋世界任务面临通信长延迟、能量有限和辐射损伤等严峻挑战，需要实现无需地球介入的自主异常响应以保障任务完成。

Method: REASIMO项目开发了基于AI辅助、个性化驱动的智能框架，融合多种先进技术，实现基于预训练行为的自治操作，替代传统硬编码逻辑。

Result: 在NASA喷气推进实验室的登月着陆器测试平台上验证了该框架的自主采样操作能力，模拟了任务可能遇到的复杂表面环境。

Conclusion: REASIMO展示了适用于冰海洋世界探测任务的先进自主系统，有效提升了任务对异常的响应能力和自主执行科学操作的能力。

Abstract: Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)
effort contributes to NASA's Concepts for Ocean worlds Life Detection
Technology (COLDTech) program, which explores science platform technologies for
ocean worlds such as Europa and Enceladus. Ocean world missions pose
significant operational challenges. These include long communication lags,
limited power, and lifetime limitations caused by radiation damage and hostile
conditions. Given these operational limitations, onboard autonomy will be vital
for future Ocean world missions. Besides the management of nominal lander
operations, onboard autonomy must react appropriately in the event of
anomalies. Traditional spacecraft rely on a transition into 'safe-mode' in
which non-essential components and subsystems are powered off to preserve
safety and maintain communication with Earth. For a severely time-limited Ocean
world mission, resolutions to these anomalies that can be executed without
Earth-in-the-loop communication and associated delays are paramount for
completion of the mission objectives and science goals. To address these
challenges, the REASIMO effort aims to demonstrate a robust level of
AI-assisted autonomy for such missions, including the ability to detect and
recover from anomalies, and to perform missions based on pre-trained behaviors
rather than hard-coded, predetermined logic like all prior space missions. We
developed an AI-assisted, personality-driven, intelligent framework for control
of an Ocean world mission by combining a mix of advanced technologies. To
demonstrate the capabilities of the framework, we perform tests of autonomous
sampling operations on a lander-manipulator testbed at the NASA Jet Propulsion
Laboratory, approximating possible surface conditions such a mission might
encounter.

</details>


### [138] [Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration](https://arxiv.org/abs/2507.06605)
*Xinyu Wu*

Main category: cs.RO

TL;DR: 这篇论文提出了Episodic RRT（ERRT），利用深度强化学习代替传统的随机采样，实现了高效、有向的路径搜索，显著提升运动规划性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的运动规划方法如RRT在复杂或高维空间中效率低下，主要因其依赖无方向的随机采样。

Method: 引入由深度强化学习代理生成的多步“探索剧集”作为采样单元，转变为有向的分支式增长，减少碰撞检测，增强路径连接性。

Result: 在多种维度环境下，ERRT及其变体均显著优于经典方法。尤其在6维机器人臂环境中，成功率达98%，速度提升至107倍，且碰撞检测减少99.6%以上，路径长度缩短近50%。

Conclusion: ERRT通过深度强化学习显著提升了采样基运动规划的效率和效果，其最优变体ERRT*在3D环境中也表现出更快的近似最优路径收敛能力。

Abstract: Classical sampling-based motion planners like the RRTs suffer from
inefficiencies, particularly in cluttered or high-dimensional spaces, due to
their reliance on undirected, random sampling. This paper introduces the
Episodic RRT, a novel hybrid planning framework that replaces the primitive of
a random point with a learned, multi-step "exploratory episode" generated by a
Deep Reinforcement Learning agent. By making the DRL agent the engine of
exploration, ERRT transforms the search process from a diffuse, volumetric
expansion into a directed, branch-like growth. This paradigm shift yields key
advantages: it counters the curse of dimensionality with focused exploration,
minimizes expensive collision checks by proactively proposing locally valid
paths, and improves connectivity by generating inherently connected path
segments. We demonstrate through extensive empirical evaluation across 2D, 3D,
and 6D environments that ERRT and its variants consistently and significantly
outperform their classical counterparts. In a challenging 6D robotic arm
scenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to
107x faster, reduces collision checks by over 99.6%, and finds initial paths
that are nearly 50% shorter. Furthermore, its asymptotically optimal variant,
ERRT*, demonstrates vastly superior anytime performance, refining solutions to
near-optimality up to 29x faster than standard RRT* in 3D environments. Code:
https://xinyuwuu.github.io/Episodic_RRT/.

</details>


### [139] [Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic](https://arxiv.org/abs/2507.06625)
*Shizhe Cai,Jayadeep Jacob,Zeya Yin,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文提出了一种结合贝叶斯MPC与actor-critic强化学习的新框架Q-STAC，通过约束的Stein变分梯度下降直接利用学习到的Q值优化控制序列，实现了无显式成本函数设计并保证安全边界。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在连续控制任务中虽成功，但需大量训练数据、难以处理复杂长远规划及保障安全约束；而模型预测控制虽保障安全和解释性，却通常为局部最优且成本函数设计复杂。

Method: 提出Q-STAC框架，通过约束的Stein变分梯度下降整合贝叶斯MPC与actor-critic方法，利用学习到的Q值直接优化控制序列，无需显式成本函数设计，同时利用已知动态提高样本效率和安全性。

Result: 在二维导航和机器人操作任务上的大量实验表明，Q-STAC在样本效率、鲁棒性和最优性方面优于现有先进算法，同时保持了策略分布的高表达能力。

Conclusion: Q-STAC成功融合了深度强化学习与模型预测控制的优势，解决了以往方法中的样本效率低、安全性差和成本函数设计难题，表现出优越的性能和应用前景。

Abstract: Deep reinforcement learning has shown remarkable success in continuous
control tasks, yet often requires extensive training data, struggles with
complex, long-horizon planning, and fails to maintain safety constraints during
operation. Meanwhile, Model Predictive Control (MPC) offers explainability and
constraint satisfaction, but typically yields only locally optimal solutions
and demands careful cost function design. This paper introduces the Q-guided
STein variational model predictive Actor-Critic (Q-STAC), a novel framework
that bridges these approaches by integrating Bayesian MPC with actor-critic
reinforcement learning through constrained Stein Variational Gradient Descent
(SVGD). Our method optimizes control sequences directly using learned Q-values
as objectives, eliminating the need for explicit cost function design while
leveraging known system dynamics to enhance sample efficiency and ensure
control signals remain within safe boundaries. Extensive experiments on 2D
navigation and robotic manipulation tasks demonstrate that Q-STAC achieves
superior sample efficiency, robustness, and optimality compared to
state-of-the-art algorithms, while maintaining the high expressiveness of
policy distributions. Experiment videos are available on our website:
https://sites.google.com/view/q-stac

</details>


### [140] [Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs](https://arxiv.org/abs/2507.06690)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Hongyin Zhang,Donglin Wang,Shiyu Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于技能图的分层多任务多智能体强化学习方法，有效处理无关任务并提升知识转移能力，优于现有分层MAPPO算法。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法难以处理复杂且无关的任务，且知识转移能力有限。

Method: 通过高层技能图和低层标准MARL算法的分层结构，提高处理无关任务的能力和知识转移效果。

Result: 实验表明该方法在处理无关任务和知识转移上优于最新的分层MAPPO算法。

Conclusion: 基于技能图的分层架构有效扩展了多任务多智能体强化学习的能力，特别是在处理无关任务方面表现优异。

Abstract: Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained
attention for its potential to enhance MARL's adaptability across multiple
tasks. However, it is challenging for existing multi-task learning methods to
handle complex problems, as they are unable to handle unrelated tasks and
possess limited knowledge transfer capabilities. In this paper, we propose a
hierarchical approach that efficiently addresses these challenges. The
high-level module utilizes a skill graph, while the low-level module employs a
standard MARL algorithm. Our approach offers two contributions. First, we
consider the MT-MARL problem in the context of unrelated tasks, expanding the
scope of MTRL. Second, the skill graph is used as the upper layer of the
standard hierarchical approach, with training independent of the lower layer,
effectively handling unrelated tasks and enhancing knowledge transfer
capabilities. Extensive experiments are conducted to validate these advantages
and demonstrate that the proposed method outperforms the latest hierarchical
MAPPO algorithms. Videos and code are available at
https://github.com/WindyLab/MT-MARL-SG

</details>


### [141] [Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction](https://arxiv.org/abs/2507.06700)
*Pranav Pandey,Ramviyas Parasuraman,Prashant Doshi*

Main category: cs.RO

TL;DR: 本文提出了一种参数化安全模型，通过引入个性化参数\u03c1，弥合物理安全与主观安全感知的差距，并通过模拟救援场景下的人体实验验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统安全模型侧重于基于传感器的物理安全测量，忽视了受个体特征和情境影响的主观安全感知，难以全面保障人机交互中的安全感。

Method: 引入参数化的个性化安全参数\u03c1，通过一系列假设驱动的人体实验调查情绪状态、信任和机器人行为对感知安全的影响，分析用户类型聚类和安全感受变化。

Result: 参数\u03c1能有效捕捉情绪反应、任务信任及用户类型差异，结果表明可预测且一致的机器人行为和积极的情绪显著提升感知安全，重复体验下担积极角色的用户感知安全下降。

Conclusion: 提出的自适应、以人为中心的安全模型整合了心理与行为因素，为安全关键领域中更可信、有效的人机交互提供了理论和实践路径。

Abstract: Ensuring safety in human-robot interaction (HRI) is essential to foster user
trust and enable the broader adoption of robotic systems. Traditional safety
models primarily rely on sensor-based measures, such as relative distance and
velocity, to assess physical safety. However, these models often fail to
capture subjective safety perceptions, which are shaped by individual traits
and contextual factors. In this paper, we introduce and analyze a parameterized
general safety model that bridges the gap between physical and perceived safety
by incorporating a personalization parameter, $\rho$, into the safety
measurement framework to account for individual differences in safety
perception. Through a series of hypothesis-driven human-subject studies in a
simulated rescue scenario, we investigate how emotional state, trust, and robot
behavior influence perceived safety. Our results show that $\rho$ effectively
captures meaningful individual differences, driven by affective responses,
trust in task consistency, and clustering into distinct user types.
Specifically, our findings confirm that predictable and consistent robot
behavior as well as the elicitation of positive emotional states, significantly
enhance perceived safety. Moreover, responses cluster into a small number of
user types, supporting adaptive personalization based on shared safety models.
Notably, participant role significantly shapes safety perception, and repeated
exposure reduces perceived safety for participants in the casualty role,
emphasizing the impact of physical interaction and experiential change. These
findings highlight the importance of adaptive, human-centered safety models
that integrate both psychological and behavioral dimensions, offering a pathway
toward more trustworthy and effective HRI in safety-critical domains.

</details>


### [142] [Spatial-Temporal Aware Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2507.06710)
*Zhenyang Liu,Yikai Wang,Kuanning Wang,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: 本文提出了一种名为4D扩散策略(DP4)的视觉模仿学习方法，通过动态高斯世界模型增强机器人对3D空间和4D时空感知能力，实现更优的任务执行效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于行为克隆的视觉模仿学习方法缺乏对3D空间结构和4D时空依赖的有效捕捉，限制了在真实环境中的应用表现。

Method: DP4利用动态高斯世界模型，从单视角RGB-D图像构建当前3D场景并预测未来3D场景，显式建模空间和时间依赖，优化轨迹生成，提升时空感知能力。

Result: 在17个模拟任务（173个变种）和3个真实机器人物理任务上的实验表明，DP4分别比基线方法提高了16.4%（Adroit）、14%（DexArt）、6.45%（RLBench）和8.6%的任务成功率。

Conclusion: 引入时空意识的4D扩散策略显著提升了视觉模仿学习的效果，证明其在多任务机器人执行中的优越性和实用价值。

Abstract: Visual imitation learning is effective for robots to learn versatile tasks.
However, many existing methods rely on behavior cloning with supervised
historical trajectories, limiting their 3D spatial and 4D spatiotemporal
awareness. Consequently, these methods struggle to capture the 3D structures
and 4D spatiotemporal relationships necessary for real-world deployment. In
this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation
learning method that incorporates spatiotemporal awareness into diffusion-based
policies. Unlike traditional approaches that rely on trajectory cloning, DP4
leverages a dynamic Gaussian world model to guide the learning of 3D spatial
and 4D spatiotemporal perceptions from interactive environments. Our method
constructs the current 3D scene from a single-view RGB-D observation and
predicts the future 3D scene, optimizing trajectory generation by explicitly
modeling both spatial and temporal dependencies. Extensive experiments across
17 simulation tasks with 173 variants and 3 real-world robotic tasks
demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,
improving the average simulation task success rate by 16.4% (Adroit), 14%
(DexArt), and 6.45% (RLBench), and the average real-world robotic task success
rate by 8.6%.

</details>


### [143] [LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/abs/2507.06747)
*Daojie Peng,Jiahang Cao,Qiang Zhang,Jun Ma*

Main category: cs.RO

TL;DR: 提出了LOVON框架，结合大语言模型和开放词汇视觉检测，解决长距离目标导航与任务规划挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效整合目标检测和高层任务规划，限制了机器人在复杂开源环境中执行长距离导航任务的能力。

Method: LOVON框架通过融合大语言模型进行分层任务规划，结合开放词汇视觉检测模型，并设计拉普拉斯方差滤波等方法应对视觉抖动、盲区和目标丢失，同时构建功能执行逻辑保障自主导航和任务完成。

Result: 在动态、非结构化环境中成功完成了实时检测、搜索与导航任务，验证了框架对多种腿式机器人型号的兼容性及易用性。

Conclusion: LOVON有效提升了机器人在开放环境下的长距离目标导航和任务执行能力，具备良好的适应性和实用性。

Abstract: Object navigation in open-world environments remains a formidable and
pervasive challenge for robotic systems, particularly when it comes to
executing long-horizon tasks that require both open-world object detection and
high-level task planning. Traditional methods often struggle to integrate these
components effectively, and this limits their capability to deal with complex,
long-range navigation missions. In this paper, we propose LOVON, a novel
framework that integrates large language models (LLMs) for hierarchical task
planning with open-vocabulary visual detection models, tailored for effective
long-range object navigation in dynamic, unstructured environments. To tackle
real-world challenges including visual jittering, blind zones, and temporary
target loss, we design dedicated solutions such as Laplacian Variance Filtering
for visual stabilization. We also develop a functional execution logic for the
robot that guarantees LOVON's capabilities in autonomous navigation, task
adaptation, and robust task completion. Extensive evaluations demonstrate the
successful completion of long-sequence tasks involving real-time detection,
search, and navigation toward open-vocabulary dynamic targets. Furthermore,
real-world experiments across different legged robots (Unitree Go2, B2, and
H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

</details>


### [144] [Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments](https://arxiv.org/abs/2507.06750)
*Tohid Kargar Tasooji,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 提出了一种适用于多机器人系统的分布式容错协同定位框架，通过自适应事件触发通信策略提升在传感器和通信干扰环境下的定位性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在GPS受限及通信受限环境下，传统定位方法易受传感器操控和通信干扰攻击，导致系统性能下降，亟需提升定位的鲁棒性和容错性。

Method: 设计了一种分布式容错协同定位框架，结合基于实时感知和通信质量动态调整通信阈值的自适应事件触发通信策略，并分析算法的收敛性与稳定性以保证在有限对抗环境中的准确状态估计。

Result: 基于Robotarium仿真实验表明，该算法在定位精度和通信效率上显著优于传统方法，特别是在有对抗干扰的环境中表现突出。

Conclusion: 该方法提升了多机器人系统的可扩展性、可靠性和容错性，适合在复杂对抗环境下大规模部署。

Abstract: In multi-robot systems (MRS), cooperative localization is a crucial task for
enhancing system robustness and scalability, especially in GPS-denied or
communication-limited environments. However, adversarial attacks, such as
sensor manipulation, and communication jamming, pose significant challenges to
the performance of traditional localization methods. In this paper, we propose
a novel distributed fault-tolerant cooperative localization framework to
enhance resilience against sensor and communication disruptions in adversarial
environments. We introduce an adaptive event-triggered communication strategy
that dynamically adjusts communication thresholds based on real-time sensing
and communication quality. This strategy ensures optimal performance even in
the presence of sensor degradation or communication failure. Furthermore, we
conduct a rigorous analysis of the convergence and stability properties of the
proposed algorithm, demonstrating its resilience against bounded adversarial
zones and maintaining accurate state estimation. Robotarium-based experiment
results show that our proposed algorithm significantly outperforms traditional
methods in terms of localization accuracy and communication efficiency,
particularly in adversarial settings. Our approach offers improved scalability,
reliability, and fault tolerance for MRS, making it suitable for large-scale
deployments in real-world, challenging environments.

</details>


### [145] [Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance](https://arxiv.org/abs/2507.06787)
*Sean Smith,Emmanuel Witrant,Ya-Jun Pan*

Main category: cs.RO

TL;DR: 提出了一种基于流函数的导航控制系统，结合涡流面方法和模型预测控制，实现复杂环境中实时的避障导航。


<details>
  <summary>Details</summary>
Motivation: 传统涡流面方法在处理近距离加速障碍物及相对距离控制方面存在不足，需要一种结合先进预测控制以提高避障性能的导航系统。

Method: 利用涡流面方法结合安全裕度控制流函数，通过高阶控制屏障函数的模型预测控制进行轨迹生成和状态估计，采用最小包络椭圆包围障碍物，配合自适应卡尔曼滤波器预测障碍物动态。

Result: 在PX4驱动的Gazebo仿真平台和配备360度激光雷达的COEX Clover四旋翼飞行器上进行了实时实验，验证了系统在复杂部分观测环境中有效避障和导航的能力。

Conclusion: 该方法成功集成了流函数控制与模型预测控制，能实时适应动态障碍物，提升了机器人在复杂环境的安全性和导航效率。

Abstract: This article presents a novel stream function-based navigational control
system for obstacle avoidance, where obstacles are represented as
two-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The
approach leverages the vortex panel method (VPM) and incorporates safety
margins to control the stream function and flow properties around virtual
surfaces, enabling navigation in complex, partially observed environments using
real-time sensing. To address the limitations of the VPM in managing relative
distance and avoiding rapidly accelerating obstacles at close proximity, the
system integrates a model predictive controller (MPC) based on higher-order
control barrier functions (HOCBF). This integration incorporates VPM trajectory
generation, state estimation, and constraint handling into a receding-horizon
optimization problem. The 2D rigid surfaces are enclosed using minimum bounding
ellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts
obstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid
avoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone
Gazebo simulator and real-time experiments involving a COEX Clover quadcopter
equipped with a 360 degree LiDAR sensor.

</details>


### [146] [Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand](https://arxiv.org/abs/2507.06822)
*Wei Xu,Yanchao Zhao,Weichao Guo,Xinjun Sheng*

Main category: cs.RO

TL;DR: 本研究提出了一种基于分层目标条件强化学习的框架，提高类人机器手使用关节工具（如镊子、剪刀）进行灵巧操作的能力。


<details>
  <summary>Details</summary>
Motivation: 以往研究较少探索关节工具的操作，且关节工具动态变化形状，增加了机器人灵巧操作的难度。

Method: 设计两级策略（低层策略控制工具多种形态，高层策略定义目标状态并控制机械臂），通过编码器估计工具的可用状态并使用特权启发式策略提升训练效率。

Result: 真实环境实验证明机器人能够有效操作镊子类工具，以70.8%的成功率完成多样物体的抓取任务。

Conclusion: 强化学习方法有效提升了机器人对关节工具的灵巧操作能力，展示了其在复杂工具操作中的潜力。

Abstract: Manipulating articulated tools, such as tweezers or scissors, has rarely been
explored in previous research. Unlike rigid tools, articulated tools change
their shape dynamically, creating unique challenges for dexterous robotic
hands. In this work, we present a hierarchical, goal-conditioned reinforcement
learning (GCRL) framework to improve the manipulation capabilities of
anthropomorphic robotic hands using articulated tools. Our framework comprises
two policy layers: (1) a low-level policy that enables the dexterous hand to
manipulate the tool into various configurations for objects of different sizes,
and (2) a high-level policy that defines the tool's goal state and controls the
robotic arm for object-picking tasks. We employ an encoder, trained on
synthetic pointclouds, to estimate the tool's affordance states--specifically,
how different tool configurations (e.g., tweezer opening angles) enable
grasping of objects of varying sizes--from input point clouds, thereby enabling
precise tool manipulation. We also utilize a privilege-informed heuristic
policy to generate replay buffer, improving the training efficiency of the
high-level policy. We validate our approach through real-world experiments,
showing that the robot can effectively manipulate a tweezer-like tool to grasp
objects of diverse shapes and sizes with a 70.8 % success rate. This study
highlights the potential of RL to advance dexterous robotic manipulation of
articulated tools.

</details>


### [147] [Friction Estimation for In-Hand Planar Motion](https://arxiv.org/abs/2507.06824)
*Gabriel Arslan Waltersson,Yiannis Karayiannidis*

Main category: cs.RO

TL;DR: 本文提出了一种在手内滑动操作中，利用平行夹持器通过触觉测量即时估计接触属性的方法，重点估计静摩擦、库仑摩擦及接触半径，并通过仿真和实际实验验证了该方法的有效性，同时引入一种启发式方法应对滑移-粘滞快速动态所带来的影响。


<details>
  <summary>Details</summary>
Motivation: 在复杂操作中，准确估计物体与夹持器接触的摩擦特性对于提升操作稳定性和精度至关重要。传统方法难以实时获取这些动态变化的接触参数，限制了机器人操作的性能。

Method: 本文提出利用触觉传感器测量接触力和滑动速度，结合物理摩擦模型同时估计静摩擦、库仑摩擦系数以及接触半径。为解决滑移-粘滞快速动态引起的估计误差，提出了一种启发式处理策略。所提方法在仿真和实际夹持操作中均进行了验证。

Result: 实验结果表明，该方法能够准确估计接触摩擦参数和接触半径，且对快速滑移-粘滞动态具有较好的鲁棒性，验证了估计方法的实用性和有效性。

Conclusion: 提出的基于触觉测量的在线接触属性估计方法提升了对滑动操作中接触特性的理解和控制能力，对提高机器人夹持操作的稳定性和适应性有重要意义。

Abstract: This paper presents a method for online estimation of contact properties
during in-hand sliding manipulation with a parallel gripper. We estimate the
static and Coulomb friction as well as the contact radius from tactile
measurements of contact forces and sliding velocities. The method is validated
in both simulation and real-world experiments. Furthermore, we propose a
heuristic to deal with fast slip-stick dynamics which can adversely affect the
estimation.

</details>


### [148] [Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems](https://arxiv.org/abs/2507.06884)
*Dong Bi,Yongqi Zhao,Zhengguo Gu,Tomislav Mihalj,Jia Hu,Arno Eichberger*

Main category: cs.RO

TL;DR: 提出了一套完整的工具链，实现了从真实数据集自动生成测试场景，并通过基于CarMaker、ROS和Apollo的协同仿真平台进行高效验证。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工具难以将快速自动场景生成与支持高级自动驾驶能力的仿真环境有效集成。

Method: 开发了一个全栈工具链，结合真实数据集自动生成场景，并利用CarMaker、ROS和Apollo三者的协同仿真平台进行场景验证。

Result: 仿真结果证明了该工具链在自动驾驶仿真测试中的有效性。

Conclusion: 该工具链为自动驾驶系统的虚拟测试提供了高效、自动化的解决方案，促进了自动驾驶系统的部署加速。

Abstract: Virtual testing has emerged as an effective approach to accelerate the
deployment of automated driving systems. Nevertheless, existing simulation
toolchains encounter difficulties in integrating rapid, automated scenario
generation with simulation environments supporting advanced automated driving
capabilities. To address this limitation, a full-stack toolchain is presented,
enabling automatic scenario generation from real-world datasets and efficient
validation through a co-simulation platform based on CarMaker, ROS, and Apollo.
The simulation results demonstrate the effectiveness of the proposed toolchain.
A demonstration video showcasing the toolchain is available at the provided
link: https://youtu.be/taJw_-CmSiY.

</details>


### [149] [ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation](https://arxiv.org/abs/2507.06905)
*Wandong Sun,Luying Feng,Baoshi Cao,Yang Liu,Yaochu Jin,Zongwu Xie*

Main category: cs.RO

TL;DR: 本文提出了一种统一控制策略ULC，实现了类人机器人运动与上肢同步控制，提升了跟踪精度、工作空间和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前方法多采用分层架构将上肢操作和下肢运动分开控制，限制了子系统间协调，难以实现类人统一全身控制。

Method: 提出单一策略ULC，通过序列技能获取、残差动作建模、命令多项式插值、随机延迟释放、负载随机化及重心跟踪等技术，实现端到端的根速度、高度、躯干旋转和双臂关节位置跟踪。

Result: 在Unitree G1机器人上验证，ULC较传统分离方法展现更优跟踪性能和更大工作空间，实现了在外部负载下的精准双臂操作及协调全身控制。

Conclusion: ULC证明了统一全身控制在类人运动操作中的可行性和优势，提升了机器人复杂任务的执行能力。

Abstract: Loco-Manipulation for humanoid robots aims to enable robots to integrate
mobility with upper-body tracking capabilities. Most existing approaches adopt
hierarchical architectures that decompose control into isolated upper-body
(manipulation) and lower-body (locomotion) policies. While this decomposition
reduces training complexity, it inherently limits coordination between
subsystems and contradicts the unified whole-body control exhibited by humans.
We demonstrate that a single unified policy can achieve a combination of
tracking accuracy, large workspace, and robustness for humanoid
loco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a
single-policy framework that simultaneously tracks root velocity, root height,
torso rotation, and dual-arm joint positions in an end-to-end manner, proving
the feasibility of unified control without sacrificing performance. We achieve
this unified control through key technologies: sequence skill acquisition for
progressive learning complexity, residual action modeling for fine-grained
control adjustments, command polynomial interpolation for smooth motion
transitions, random delay release for robustness to deploy variations, load
randomization for generalization to external disturbances, and
center-of-gravity tracking for providing explicit policy gradients to maintain
stability. We validate our method on the Unitree G1 humanoid robot with 3-DOF
(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better
tracking performance to disentangled methods and demonstrating larger workspace
coverage. The unified dual-arm tracking enables precise manipulation under
external loads while maintaining coordinated whole-body control for complex
loco-manipulation tasks.

</details>


### [150] [Bounomodes: the grazing ox algorithm for exploration of clustered anomalies](https://arxiv.org/abs/2507.06960)
*Samuel Matloob,Ayan Dutta,O. Patrick Kreidl,Swapnonel Roy,Ladislau Bölöni*

Main category: cs.RO

TL;DR: 本文提出了一种名为bounomodes的新型算法，用于异常簇优先的路径规划，结合了均匀覆盖和异常区域的针对性探索。


<details>
  <summary>Details</summary>
Motivation: 传统均匀覆盖的路径规划算法对异常簇的探索不足，难以有效检测植物病害、污染等聚集现象。

Method: 引入bounomodes算法，交替进行均匀的boustrophedon采样和基于深度强化学习的异常簇近距离探索。

Result: 实验表明，该方法在异常检测效率上优于多种现有基线算法。

Conclusion: 结合几何均匀采样与深度强化学习的异常簇探索策略能显著提升信息性路径规划的性能。

Abstract: A common class of algorithms for informative path planning (IPP) follows
boustrophedon ("as the ox turns") patterns, which aim to achieve uniform area
coverage. However, IPP is often applied in scenarios where anomalies, such as
plant diseases, pollution, or hurricane damage, appear in clusters. In such
cases, prioritizing the exploration of anomalous regions over uniform coverage
is beneficial. This work introduces a class of algorithms referred to as
bounom\=odes ("as the ox grazes"), which alternates between uniform
boustrophedon sampling and targeted exploration of detected anomaly clusters.
While uniform sampling can be designed using geometric principles, close
exploration of clusters depends on the spatial distribution of anomalies and
must be learned. In our implementation, the close exploration behavior is
learned using deep reinforcement learning algorithms. Experimental evaluations
demonstrate that the proposed approach outperforms several established
baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [151] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的非自主微分方程参数估计新方法HADES-NN，通过平滑近似断续信号，提升了对突变外部信号系统的建模能力。


<details>
  <summary>Details</summary>
Motivation: 非自治微分方程在建模受外部信号影响的系统时十分重要，但当信号突变时，基于数据拟合模型极具挑战性。

Method: 提出HADES-NN方法，首先用神经网络将断续信号平滑近似，然后利用此平滑信号进行模型参数估计，迭代进行。

Result: 在多个应用中取得高精度和高准确度的参数估计，包括光刺激调节的昼夜节律系统及酵母对外部费洛蒙信号的响应。

Conclusion: HADES-NN极大地扩展了可拟合现实测量数据的模型系统范围，提升了断续信号影响下非自主微分方程建模的实用性。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [152] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: 提出了SEA-DBS，一种基于强化学习的高效自适应深部脑刺激系统，能在资源受限硬件上实现稳定且个性化的治疗PD。


<details>
  <summary>Details</summary>
Motivation: 传统开放式深部脑刺激系统缺乏适应性和个性化，且能耗高，现有基于强化学习的自适应方法样本复杂度高且难以稳定探索和硬件部署。

Method: SEA-DBS结合预测奖励模型减少实时反馈依赖，利用Gumbel Softmax实现稳定可微的二元动作空间探索，提升样本效率和适应资源受限设备。

Result: 在模拟帕金森病的生物真实基底神经节模型中，SEA-DBS显示出更快收敛、更有效抑制病理性β波及对后训练FP16量化的耐受性。

Conclusion: SEA-DBS为实时、资源受限环境下的强化学习自适应深部脑刺激提供了实用且高效的框架。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [153] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux是一个深度学习框架，通过混合CNN-LSTM模型对哈密顿向量场进行符号回归，自动识别哈密顿函数的符号表达式。


<details>
  <summary>Details</summary>
Motivation: 自动发现哈密顿力学中的哈密顿函数表达式，实现物理系统的符号建模。

Method: 采用混合CNN-LSTM架构，对新开发的哈密顿向量场数据集进行训练和验证，进行符号回归。

Result: 模型能够准确恢复哈密顿函数的符号表达式，表明方法有效。

Conclusion: SymFlux推动了哈密顿力学自动发现领域的发展，实现了符号函数的自动识别。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [154] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 该论文提出了DecoyDB数据集和定制的图对比学习框架，用于蛋白-配体结合亲和力预测，显著提升了模型准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 蛋白-配体结合亲和力预测是药物发现中的关键任务，但现有带标签的数据集（如PDBbind）规模小，限制了模型性能提升。自监督学习，尤其是图对比学习，利用大量无标签数据预训练有潜力突破这一瓶颈，但缺乏合适的大规模无标签复杂体数据集及相应算法设计。

Method: 提出了DecoyDB，一个包含高分辨率真结合体和大量负样本（计算生成的结构，标注RMSD）的结构感知无标签数据集；设计定制的图对比学习框架基于DecoyDB进行预训练，再在PDBbind上的带标签数据进行微调。

Result: 预训练模型在准确性、标签效率和泛化能力方面均优于未预训练模型，验证了DecoyDB及相应方法的有效性。

Conclusion: 通过构建结构感知的大规模无标签数据集DecoyDB和定制化图对比学习策略，显著提升蛋白-配体亲和力预测模型性能，为药物发现提供了强有力的工具。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [155] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 该论文研究了深度线性卷积网络学习过程中梯度流的几何性质，证明了其在参数空间的梯度流可以写成函数空间上的黎曼梯度流。


<details>
  <summary>Details</summary>
Motivation: 动机是理解深度线性卷积网络梯度流的几何结构，以便更好地解释训练过程和优化性质。

Method: 通过数学推导，证明了在任意初始化条件下，深度线性卷积网络参数空间的梯度流能转化为函数空间上的黎曼梯度流，尤其针对不同维度的卷积操作。

Result: 结果表明，$D$维的卷积网络（$D\geq 2$）梯度流都能表示为函数空间上的黎曼梯度流，而一维卷积当所有步长大于一时结果成立，且对应的黎曼度量依赖于初始化。

Conclusion: 结论是该工作拓展了之前全连接网络的相关理论，为深度线性卷积网络学习的几何分析提供了新的视角和理论基础。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [156] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: 提出WINGs框架，通过PCA和SVR动态生成和压缩神经网络权重，大幅减少内存需求且准确率损失低。


<details>
  <summary>Details</summary>
Motivation: 复杂神经网络存储大量权重需占用巨大内存，限制了边缘设备应用。

Method: 利用PCA降维与轻量SVR预测全连接网络权重，结合敏感性分析压缩卷积网络权重，避免存储完整权重矩阵。

Result: 实现对FC层53倍，AlexNet MNIST 28倍，CIFAR-10 18倍压缩，准确率仅下降1-2%。

Conclusion: WINGs显著降低内存需求，提高推理效率，适合资源受限的边缘计算环境。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [157] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 本文提出了将梯度流分解为参数算子K和线性化流传播器P的理论工具，揭示了在梯度下降训练下循环神经网络中低维潜变量动力学的形成机制，并应用于多任务训练的目标对齐分析。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏理论工具深入理解有限非线性模型中梯度下降训练对学习表征机制的影响，尤其是在循环神经网络中。

Method: 将梯度流分解为参数算子K和线性化流传播器P，并结合Lyapunov稳定性和最优控制理论进行分析，推出潜变量动力学及多任务训练下目标对齐的理论解释。

Result: 理论与实验验证了分解方法揭示的潜变量动力学形成原因以及多任务训练中目标对齐的度量，且开发了PyTorch工具包KPFlow用于广泛应用。

Conclusion: 该工作为理解非线性循环模型中梯度下降学习机制提供了新的理论框架和分析工具，推动该领域的理论研究进展。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [158] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 本文比较了多种深度学习模型在无线心电图防篡改中的表现，并评估了基于ECG的身份验证效果。


<details>
  <summary>Details</summary>
Motivation: 随着无线心电图系统的普及，保障信号完整性免受篡改变得尤为重要，同时实现准确身份验证也是需求之一。

Method: 将一维ECG信号通过连续小波变换转化为二维时频图像，利用CNN、ResNet、混合Transformer-CNN模型进行篡改检测，且使用孪生网络进行身份验证，模拟六种篡改策略进行训练和测试。

Result: 在高度破碎的篡改情况下，多模型准确率超过99.5%；对于细微篡改，混合模型平均准确率达98%；纯Transformer孪生网络身份验证准确率98.3%，混合CNN-Transformer孪生网络实现了100%的准确率。

Conclusion: 混合Transformer-CNN模型在信号篡改检测和身份验证方面表现优异，证明了其在无线ECG系统中保护信号完整性和验证身份的有效性。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [159] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare是一个基于域适应的深度学习框架，专为重症监护室中罕见病症的临床结果预测而设计，通过自监督预训练和条件知识图缓解数据稀缺和条件内异质性问题，实验证明其在多个临床预测任务中优于现有模型和评分系统。


<details>
  <summary>Details</summary>
Motivation: 重症监护室中罕见病症由于数据稀缺和病症内异质性，现有人工智能方法难以有效服务，亟需一种有效的预测框架。

Method: KnowRare通过自监督预训练学习与病症无关的表征，并利用临床相似病症的条件知识图实现域适应，选择性地迁移知识以应对病症内异质性。

Result: 在两个ICU数据集上的五个临床预测任务中，KnowRare表现持续优于现有最先进模型及APACHE IV和IV-a评分系统，且在参数自适应、数据稀缺及病症知识选择方面表现出良好灵活性和合理性。

Conclusion: KnowRare是一种鲁棒且实用的深度学习解决方案，能够支持临床决策，改善重症监护中罕见病症的护理效果。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [160] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: 本文介绍了eegFloss，一个基于机器学习的新型开源Python工具，用于检测睡眠脑电图中的伪影，提高自动睡眠分期的准确性。


<details>
  <summary>Details</summary>
Motivation: 睡眠脑电信号易受内部和外部伪影影响，自动睡眠分期容易出现错误，需有效识别和剔除伪影以提高睡眠研究的准确性。

Method: 开发了机器学习模型eegUsability，用以识别伪影段落，并基于Zmax头带采集数据训练和验证；另有eegMobility模型实现自动床上时间检测，同时提供伪影过滤和睡眠统计生成。

Result: eegUsability在127晚数据上表现优异，F1分数约0.85，Cohens kappa约0.78，召回率高达94%。该工具适用于多种设备，增强脑电信号的可用性识别。

Conclusion: eegFloss有效解决了睡眠脑电伪影问题，提升自动睡眠分期的准确性和研究严谨性，为睡眠研究提供可靠工具。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [161] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 本文研究了可解释性方法用于预测模型在分布外数据上的行为，发现注意力模式与泛化性能存在关联。


<details>
  <summary>Details</summary>
Motivation: 传统可解释性研究关注模型机制的干预响应，但较少预测模型对未见输入数据的表现。本文旨在探索可解释性工具预测分布外行为的潜力。

Method: 通过在一个合成分类任务上训练数百个Transformer模型，分析它们的注意力模式与分布外泛化规则的对应关系，结合消融测试验证。

Result: 发现当模型在训练分布中展现出层次化的注意力模式时，其在分布外数据上也倾向于层次化泛化，即使实际实现不依赖这些模式。

Conclusion: 证明了可解释性工具能用于预测模型的未知行为，激励未来研究进一步拓展该方向。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [162] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: 本论文提出FedPhD，一种用于联邦学习环境中高效训练扩散模型的方法，显著降低通信成本并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为高质量图像生成器需要多样的数据，联邦学习中存在高通信成本和数据异构性等挑战，现有研究对这些问题关注有限。

Method: 提出FedPhD方法，利用层次化联邦学习结合同质性感知的模型聚合与选择策略，解决数据异构问题，同时通过分布式结构化剪枝提升计算效率并减少模型存储需求。

Result: 在多个数据集上的实验显示，FedPhD在FID分数方面表现优异，同时通信成本降低高达88%，相比基线方法FID提升至少34%，计算与通信资源使用仅为56%。

Conclusion: FedPhD有效解决了联邦学习中训练扩散模型的通信和异构性难题，实现了高性能和高效资源利用，具有广泛应用前景。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [163] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 本文提出了首个自动化框架，为蛋白质语言模型中的神经元赋予生物学意义的自然语言标签，实现了对数十万神经元的分析。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型编码了丰富的生物信息，但其内部神经元表示尚未被充分理解，需要系统性解释神经元功能。

Method: 引入自动标注系统，结合神经元激活引导方法，实现对神经元的生物学功能解释及基于神经元激活的蛋白质设计。

Result: 发现个别神经元对多种生物化学及结构属性高度敏感，实现了蛋白质设计能准确控制分子量、不稳定指数和二三级结构特征。

Conclusion: 不同规模模型神经元分析揭示了蛋白质语言模型的扩展规律及神经元分布的结构性。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [164] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 本论文提出了一种基于二值随机单元的前向前向算法，以降低深度学习模型在能耗上的开销，通过将矩阵乘法转换为索引操作实现硬件加速，并利用p-bits设备实现高效二值采样，在多个数据集上表现出接近实数值方法的性能同时显著节能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型规模庞大，训练时的高能耗成为制约其发展的瓶颈，尤其是反向传播算法对硬件的要求高且能耗大。

Method: 提出了适用于二值随机单元的前向前向算法，通过激活二值化将矩阵乘法转化为硬件友好的索引操作，结合带有不同偏置的绑定权重以绕过信息瓶颈，并利用基于不稳定磁体的p-bits实现快速且低成本的二值采样。

Result: 该算法在MNIST、Fashion-MNIST和CIFAR-10数据集上的性能接近传统实值前向前向算法，但能耗估计降低了约一个数量级。

Conclusion: 通过二值化激活和利用p-bits硬件，本文提出的方法有效减少了深度学习训练中的能耗，有潜力推动节能神经网络硬件的设计和应用。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [165] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 本文提出了一种名为SignSoftSGD（S3）的新优化器，通过改进Adam优化器的动量机制和更新策略，提高了训练稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: Adam优化器虽成功，但存在因更新缩放不受控引起的损失波动问题，亟需提升稳定性和性能。

Method: S3采用灵活的p阶动量替代Adam的二阶动量，实现更稳健的梯度更新；统一指数移动平均系数限制更新区间；引入等效的NAG加速模块。

Result: S3在理论上达到非凸优化的最优收敛率，在多项视觉和语言任务实验中表现出更快收敛和更佳性能，同时显著减少损失波动，支持更高学习率。

Conclusion: S3有效提升了Adam优化器的性能和稳定性，兼顾训练效率与最终任务表现，展现出在深度学习优化领域的潜力。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [166] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: 本文提出了一种利用基础模型进行自我对弈的新方法FMSP，旨在解决传统自我对弈多样性不足和局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 传统自我对弈算法经常陷入局部最优并缺乏策略多样性，限制了策略的质量和创新性。

Method: 引入基于基础模型的自我对弈(FMSP)，包括三种变体：vFMSP持续优化策略，NSSP追求策略多样性，QDSP结合两者优势实现高质量多样性策略。

Result: 在Car Tag和Gandalf两个环境中，FMSP各变体均表现优异，超越了强大的人类设计策略，成功自动攻破和修补大型语言模型的安全防御。

Conclusion: FMSP利用基础模型的强大能力改进自我对弈，开启了策略创新和多样性的新研究方向。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [167] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 本文提出了一种针对图神经网络中欺诈检测中的消息不平衡问题的新方法MimbFD，通过双视角图表示学习提升欺诈节点识别性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在诈骗检测中由于局部交互的局限性，导致全局拓扑信息传递不均衡，欺诈节点信息易被淹没，影响下游任务效果。

Method: 设计了拓扑消息可达模块提升节点表示质量，穿透欺诈者伪装；引入局部混淆去偏模块调整节点表示，增强表示与标签的稳定关联，平衡不同类别影响。

Result: 在三个公开欺诈数据集上进行实验，MimbFD方法表现优异，显著提升了欺诈检测性能。

Conclusion: 所提MimbFD有效缓解了图神经网络中因欺诈节点特征掩盖和类别不平衡导致的消息不平衡问题，提高了欺诈检测的准确性和稳定性。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [168] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 本文提出了一种结合扩散模型的联邦学习新范式FedDifRC，旨在缓解数据异质性影响，提高模型收敛与性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习面临数据异质性问题，导致模型收敛困难和性能下降。引入扩散模型，有望利用其强大表征能力缓解该问题。

Method: 提出FedDifRC框架，通过文本驱动的对比学习和噪声驱动的一致性正则化，利用扩散模型提供的类别相关语义信息和一致性收敛信号，促进本地实例与扩散去噪表示对齐。该方法可扩展至无标签自监督学习，并在非凸目标下理论保证收敛。

Result: 在多种场景下实验验证了FedDifRC的有效性，以及文本驱动对比学习和噪声驱动正则化等关键组件的效率。

Conclusion: FedDifRC成功利用扩散模型的表征优势，缓解了联邦学习中的数据异质性问题，提升了模型的收敛性和性能，并具备理论收敛保障。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [169] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: 本文提出了MoFE-Time模型，结合时间域和频率域特征，通过专家混合网络和预训练-微调框架提升时间序列预测性能，在多个公开数据集和商业数据集上实现了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的时间序列预测模型在预训练-微调框架下很少同时建模时间和频率特性，导致对复杂时间序列的预测效果不佳。

Method: 提出MoFE-Time，将时间域和频率域细胞作为专家，结合专家混合网络路由机制，利用预训练-微调框架转移不同周期分布的数据上的先验模式知识。

Result: 在六个公开基准测试中，MoFE-Time模型相比代表性方法Time-MoE，均方误差和平均绝对误差分别降低了6.95%和6.02%。在自主商业数据集NEV-sales中表现优异。

Conclusion: MoFE-Time有效整合时间和频率特征，借助MoE网络和预训练-微调策略，显著提升了时间序列预测的准确性，具备良好的应用潜力。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [170] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 本文提出了一种新的单调后验校准方法，通过线性约束的校准映射，保证了预测概率的排序一致性，同时兼具表达能力、鲁棒性和可解释性。该方法在多个数据集和深度神经网络模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常常产生过于自信的概率预测，现有的后验校准方法大多不能保证预测概率的排序单调性，导致校准效果受限。

Method: 本文设计了一类参数关于类别数线性约束的校准映射，将校准问题转化为受约束的优化问题，从而保证单调性，同时提升表达能力和鲁棒性。

Result: 在不同数据集和神经网络模型上，本方法表现出优于现有校准方法的性能，且在数据和计算资源方面更为高效。

Conclusion: 所提出的单调后验校准方法有效解决了排序单调性的问题，兼具表现力和鲁棒性，可作为深度网络概率校准的新范式。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [171] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: 提出了AdaDPIGU，一种用于深度神经网络的基于重要性的差分隐私随机梯度下降框架，通过稀疏和自适应截断机制提高隐私保护下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私随机梯度下降方法在高维度下性能下降，因注入的噪声规模随维度增大而增加。

Method: 在预训练阶段利用差分隐私高斯机制估计参数重要性，梯度更新时裁剪低重要性坐标并采用坐标自适应截断，实现稀疏、低噪声梯度更新。

Result: 在MNIST数据集上隐私预算为8时达到99.12%的准确率，接近非隐私模型；在CIFAR-10数据集上隐私预算为4时达到73.21%，优于非隐私基线71.12%。

Conclusion: AdaDPIGU在保证差分隐私的同时，通过自适应稀疏化机制提升了模型的隐私保护和实用性能。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [172] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 本文提出了一种联合优化模型和非短视采集策略的贝叶斯优化新方法，通过多步后悔最小化直接优化决策。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化依赖于手工设计的采集函数和代理模型，且通常采用短视策略，限制了优化性能。

Method: 本文利用超参数不同的高斯过程集合生成多步采集轨迹，训练决策变换器端到端学习选择查询点，采用密集训练-稀疏在线学习策略，提升多步探索能力。

Result: 实验证明本方法在合成和真实任务上均超越传统贝叶斯优化基线，表现出更低的简单后悔值和更鲁棒的探索行为。

Conclusion: 该方法有效融合模型和采集策略，显著提升了贝叶斯优化在高维和噪声场景下的性能和稳定性。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [173] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: 提出了一种名为CircuitGCL的图对比学习框架，用于增强模拟混合信号电路图的表示迁移能力，在寄生参数估计任务中性能优越。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号电路设计数据稀缺、标签分布不平衡及电路实现多样性，导致难以学习具有鲁棒性和迁移能力的电路图表示。

Method: CircuitGCL结合表征散射和标签重平衡技术，通过自监督的超球面表征散射学习拓扑不变的节点嵌入，并引入平衡的MSE和软max交叉熵损失，减缓标签分布差异。

Result: 在TSMC 28nm AMS设计的寄生电容估计和地电容分类任务中，CircuitGCL超越了现有最先进方法，回归任务$R^2$提升33.64%-44.20%，分类任务F1分数提升0.9~2.1倍。

Conclusion: CircuitGCL有效缓解了数据稀缺、多样性和标签不平衡问题，增强了电路图表示的鲁棒性和迁移能力，为寄生估计提供了有效方法。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [174] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: 本文提出CircuitGPS，一种针对模拟/混合信号电路中寄生效应预测的少样本学习方法，通过异构图表示电路并结合混合图Transformer，提高耦合电容预测准确率和降低误差，具备零样本学习能力和良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计数据稀缺，限制了深度学习模型的训练效果，需要一种高效的少样本学习方法来准确预测寄生效应。

Method: 将电路网表表示为异构图，耦合电容作为链接，采用小跳采样技术构造子图，利用混合图Transformer学习子图嵌入，并融合低成本位置编码。先进行链接预测预训练，再进行边回归微调。

Result: 与现有方法相比，CircuitGPS在耦合存在性准确率提高至少20%，电容估计平均绝对误差降低至少0.067，且具备零样本学习能力，可以直接应用于多种AMS电路设计。

Conclusion: CircuitGPS在AMS电路寄生效应预测上表现优异，具备良好的可扩展性和泛化能力，为图模型在表示学习中的应用提供了有价值的见解。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [175] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 本文研究了去中心化学习中的通信调度策略，发现集中在训练后期进行通信显著提升了性能，最终全连接通信能达到服务器训练效果，去中心化SGD在收敛速度上优于集中式方法。


<details>
  <summary>Details</summary>
Motivation: 传统参数服务器训练在通信效率和可扩展性上存在瓶颈，去中心化学习虽有优势但通信限制影响性能。

Method: 通过实验证明在训练后期集中通信有效，提出全局合并策略，并理论上分析去中心化SGD的收敛性，重新解释了局部模型差异的积极作用。

Result: 训练后期的通信显著改善了全局模型的泛化能力，最终一步的全连接通信匹配服务器训练性能，去中心化SGD的收敛速度优于集中式SGD。

Conclusion: 该工作挑战了去中心化学习在数据异质性和通信限制下表现差的常见看法，提出了有效的通信调度策略和理论基础，深化了对模型合并和神经网络损失曲面的理解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [176] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的双阶段模型，用于在布局前准确预测SRAM电路的寄生参数，从而提升设计效率和仿真速度。


<details>
  <summary>Details</summary>
Motivation: 寄生效应导致SRAM设计中布局前后仿真结果差异大，设计参数难以收敛，增加设计迭代次数。希望能够基于布局前电路预测寄生参数，实现寄生感知的前期仿真。

Method: 提出结合图神经网络分类器和多层感知器回归器的双阶段模型，同时采用Focal Loss减轻内部网络样本过多带来的影响，并将子电路信息融入图结构以抽象电路层级特性。

Result: 在4个真实SRAM设计上，模型在寄生预测精度上最高提升19倍误差减少，仿真过程速度提升达598倍。

Conclusion: 该深度学习模型有效解决了寄生参数预测问题，显著提升SRAM设计仿真准确性与效率，对于提升系统能效具有重要意义。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [177] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: 本文提出了LoRAM，一种基于权重更新幅度驱动的低秩适配初始化方法，提升了LoRA性能且保持了效率。


<details>
  <summary>Details</summary>
Motivation: 现有频谱初始化方法虽提高了LoRA性能和收敛速度，但计算和存储开销较大，影响效率。

Method: 通过理论证明权重更新幅度是性能关键，提出LoRAM初始化方案，用确定性正交基与预训练权重幅度结合，模拟频谱方法的增益效果。

Result: 实验表明LoRAM在保持LoRA原有效率的同时，性能匹配或优于频谱初始化方法。

Conclusion: LoRAM有效解决了频谱初始化的效率瓶颈，是一种实用且高效的低秩适配初始化策略。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [178] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 本文提出了一种面向边缘网络的混合专家模型加速方法，通过优化专家缓存策略显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型存在专家数量巨大导致边缘设备存储压力大问题，通过分布式推理和缓存优化缓解该问题。

Method: 基于Top-K选择策略，将延迟最小化问题转化为子模块最大化问题，设计贪心算法和动态规划分解方法，利用最大卷积技术提高求解效率。

Result: 模拟实验表明所提方法相比现有基线显著降低了推理延迟。

Conclusion: 提出的方法有效解决了存储受限条件下的专家选择和缓存问题，实现了边缘网络中混合专家模型的高效推理。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [179] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为LPPO的强化学习框架，通过前缀引导采样和学习进度加权策略，利用少量高质量示范数据提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习领域主要关注算法设计和数据规模增长，而本研究关注如何高效利用少量可信、高质量示范数据，借鉴人类解决问题时的提示与进度调整机制。

Method: 提出前缀引导采样，将专家示范中的部分解答前缀用作策略引导；引入学习进度加权策略，根据模型学习进度动态调整训练样本的重要性，促进有效学习样本的权重提升，减少停滞样本影响。

Result: 在数学推理基准测试中，LPPO方法较强基线表现出更快收敛速度和更高性能上限，验证了其优越性。

Conclusion: LPPO通过样本中心的优化策略显著提高强化学习的训练效率和模型性能，提供了一条利用少量优质数据提升大语言模型推理能力的新路径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [180] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 本文提出利用"预测信息增益"这一信息度量，通过强化学习方法，确定环境中最有信息量的区域进行探索，从而获得环境可控动态的可靠估计，效果优于多种短视探索方法。


<details>
  <summary>Details</summary>
Motivation: 当环境的显式模型不可用时，希望通过探索环境学习其动态信息。

Method: 引入预测信息增益作为信息度量，结合强化学习寻找次优的探索策略，指导探索过程。

Result: 该方法能够找到可靠的环境动态模型，且效果优于多种短视的探索办法。

Conclusion: 利用预测信息增益和强化学习指导探索，有助于更好地了解环境的可控动态，提升探索效率。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [181] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 本文提出了一种面向RAN控制的强化学习框架，通过图神经网络表示、领域随机化和分布式训练，实现了在多样化网络环境中的泛化能力提升，在5G多个基准测试中显著提升了链路自适应性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的无线资源管理算法在动态多样化的无线接入网环境中表现不佳，而现有的强化学习方法因过度拟合训练环境而难以推广到未见场景。

Method: 本文设计了一个以泛化为核心的强化学习框架：通过注意力机制的图神经网络编码小区拓扑和节点属性，采用领域随机化扩展训练分布，并在符合O-RAN原则的云架构中进行分布式数据生成和集中训练。

Result: 在5个5G基准下，所提策略相比基线在全缓冲MIMO/mMIMO场景下提升约10%吞吐率和频谱效率，高移动性场景提升超过20%。在九小区部署中，基于图注意力网络的模型吞吐率较多层感知机模型高出30%。

Conclusion: 该框架有效提升了无线接入网中强化学习方法的泛化能力和性能，结合可扩展架构，为未来AI原生的6G RAN开发提供了一种单一泛化强化学习代理的实现路径。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [182] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文提出了一种利用多种β值训练变分自编码器(VAE)以获得多样潜在表示的新框架，并引入非线性扩散模型实现潜在空间平滑过渡，提升解耦性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的β-VAE框架在因权衡β值而导致解耦表示和生成质量之间存在权衡，难以兼顾两者。

Method: 设计一种新的损失函数以控制潜在表示中信息保留度，通过训练单一VAE获得多个不同β值对应的潜在表示，并引入非线性扩散模型实现不同β潜在表示间的平滑过渡，保证信息丰富度和解耦性。

Result: 模型在解耦性和生成质量上均表现良好，实现了潜在空间平滑过渡和无输入图像的纯生成能力。

Conclusion: 通过组合多β值潜在表示与非线性扩散模型，本方法有效解决了生成质量与表示解耦性的权衡，提升了生成模型的表现与灵活性。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [183] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种新颖的跨任务策略指导（CTPG）框架，通过利用各任务间的控制策略共享，加速多任务强化学习中的技能习得，并设计门控机制提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法多侧重于参数共享和定制网络结构，忽视了通过已有任务控制策略对未掌握任务提供显式指导的潜力。

Method: CTPG框架为每个任务训练一个指导策略，用于从所有任务的控制策略中选择行为策略生成更优训练轨迹，辅以两种门控机制过滤无益控制策略和不需指导的任务。

Result: 将CTPG整合到现有参数共享方法中，在操控和运动基准测试中显著提升了多任务学习的表现。

Conclusion: CTPG作为一种普适框架，能够有效利用跨任务策略指导，提升多任务强化学习的效率和性能。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [184] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 针对医疗图像分类中因数据泄露导致的问题，提出了线性衰减噪声和裁剪阈值的SAD-DPSGD方法，有效提升小型不平衡数据集上的模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法在大规模数据集上表现良好，但对不平衡的小型医疗数据集失效，主要是因为梯度裁剪导致少数类信息丢失，模型早期陷入次优解。

Method: 提出SAD-DPSGD，通过线性递减机制动态调整噪声和裁剪阈值，在训练初期给予更多隐私预算和更高裁剪阈值，以避免模型陷入次优解。

Result: 在HAM10000数据集上，SAD-DPSGD相比Auto-DPSGD提升准确率2.15%，在隐私参数$=3.0$和$=10^{-3}$下效果显著。

Conclusion: SAD-DPSGD有效解决了小型不平衡医疗图像数据集中的隐私保护与模型性能权衡问题，增强了模型的分类准确性。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [185] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD是一个通用的异常检测框架，通过利用已有标记数据训练单一模型，实现跨领域数据集的异常点检测，避免了传统方法中模型选择和调参的繁琐。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法需针对具体数据集调参和训练，过程繁琐且计算成本高，不利于实际应用。

Method: UniOD将数据集转换为多个图，生成一致的节点特征，将异常检测建模为节点分类任务，训练出可泛化到未知领域的单一模型。

Result: 在15个基准异常检测数据集上，UniOD相较15个最先进方法表现出更好的效果，验证了其有效性。

Conclusion: UniOD显著简化了异常检测流程，降低计算成本，提升了检测准确率，增强了方法的实用性和通用性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [186] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为GO-Skill的离线多任务强化学习方法，通过目标导向的技能抽象提取和利用可复用技能，实现知识共享和多个任务的高效解决。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习面临跨任务知识共享的难题，希望借鉴人类高效的知识抽象能力以提升学习效果。

Method: GO-Skill通过目标导向的技能提取过程，使用向量量化构建离散技能库，并引入技能增强阶段解决技能类别失衡，最后结合层次化策略学习动态调度技能完成任务。

Result: 在MetaWorld基准上的多样化机器人操作任务中，GO-Skill展现出良好的效果和通用性。

Conclusion: GO-Skill有效提升了离线多任务强化学习中知识的复用和传递能力，增强了多任务性能和策略的灵活性。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [187] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 本文提出了一种基于Laplace算子导数检测和防止回归过拟合的方法，应用于网格结构数据，通过优化超参数减少训练模型的振荡，实现无需划分训练测试集的直接训练与测试。


<details>
  <summary>Details</summary>
Motivation: 解决回归模型在网格结构数据上的过拟合问题，尤其是通过检测模型内部未被捕捉的振荡以增强模型泛化能力。

Method: 利用网格结构计算训练数据的Laplace算子二阶导数作为真实标签，在训练后的数据上以错位网格计算导数，检测内部振荡；通过最小化训练模型中Laplace导数的熵来优化超参数，减少振荡。

Result: 成功实现了通过Laplace算子导数的损失函数对超参数进行优化，显著降低了模型内的无效振荡，提升了模型的稳定性和泛化性能。

Conclusion: 该方法无需传统的训练集和测试集划分，通过基于扩散性质的Laplace算子导数作为替代测试指标，实现了有效的过拟合检测和防止，适用于网格状数据回归问题。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [188] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的个体治疗效果估计算法，通过多头注意力的专家混合和线性正交正则化，实现软分解预处理变量，同时利用重要性采样消除选择偏差，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的因果推断方法多依赖于生成模型或硬分解，难以确保准确分解协变量，从而影响个体治疗效果估计的准确性。

Method: 提出结合多头注意力的专家混合模型和线性正交正则器的软分解方法，同时采用重要性采样重加权技术消除选择偏差，提升治疗效果估计的准确性。

Result: 在公开的半合成数据集和真实生产数据集上实验，结果显示该算法显著优于当前最先进的个体治疗效果估计方法。

Conclusion: 该算法有效分解协变量且能消除选择偏差，为个体治疗效果估计提供了更准确、更鲁棒的解决方案。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [189] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文提出将机器学习和联邦学习技术应用于模糊系统，以提升其处理不确定性的能力。


<details>
  <summary>Details</summary>
Motivation: 传统二元系统难以处理不确定性，模糊系统虽已应用但仍有提升空间，引入机器学习和联邦学习可望优化其性能。

Method: 结合机器学习技术和联邦学习机制，特别是通过更新模糊规则的方式，持续改进模糊系统的表现。

Result: 初步分析显示，通过上述方法模糊系统的性能有潜力得到提升，但具体效果仍需进一步验证。

Conclusion: 本文认为将机器学习与联邦学习引入模糊系统具有改进潜力，建议进行深入研究来评估其实际效果及可能的限制。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [190] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 本文提出了一种基于异构图注意力网络的多域传感器数据短期状态预测方法，有效提升了电力系统状态预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统跨越多物理领域，现有基于图神经网络（GNN）的状态预测方法多针对单一领域且假设传感器关系同质，难以处理现实中的异构传感器数据，限制了预测性能。

Method: 本文使用异构图注意力网络，结合水力和电力两个物理领域中传感器间的同质和异质关系，捕捉多域传感器的不同时间动态，实现精准短期状态预测。

Result: 实验结果显示，所提方法在归一化均方根误差上平均提升35.5%，显著优于传统基线方法。

Conclusion: 该方法有效解决多域多速率能量系统中异构传感器数据的融合与预测问题，为现代电力系统的稳定运行提供了强有力的技术保障。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [191] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文研究了观察模仿学习（IfO）在复杂数据分布下的应用，提出了一种利用无动作演示进行自我迭代改进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有IfO研究多集中于理想化的双峰数据分布，限制了其实用性和推广性，需解决更细腻、真实的数据分布环境。

Method: 提出一种基于强化学习的IfO方法，利用价值函数在专家和非专家数据间传递信息，实现无动作演示的模仿学习。

Result: 通过综合评估，揭示了不同数据分布对算法适用性的影响，强调了现有方法的局限性。

Conclusion: 研究为开发更稳健且实用的IfO技术提供了洞见，有助于实现可扩展的行为学习。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [192] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络的自适应观测器（PINN-Obs），用于非线性系统的状态估计，直接融合系统动力学与传感器数据，避免了传统观测器的线性化限制，并自适应学习最优增益矩阵，保障状态估计收敛。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的观测器依赖系统线性化或显式转换，且在部分观测和含噪声测量条件下准确估计非线性系统状态存在挑战。

Method: 提出PINN-Obs，结合物理信息神经网络与传感器数据，通过自适应学习最优增益矩阵，实现状态估计的统一误差最小化，同时进行形式化的收敛性分析。

Result: 通过诱导电机模型、卫星运动系统和学术基准案例的数值仿真验证了PINN-Obs的有效性，相较现有观测器设计表现出更高的准确性、鲁棒性和适应性。

Conclusion: PINN-Obs为非线性系统状态估计提供了一种无需线性化的新型观测方案，理论和实验结果均证明其优越性能和收敛保证。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [193] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 本文提出了一种新的数学人工数据（MAD）框架，结合物理定律与数据驱动方法，实现高效准确的微分方程算子学习，避免依赖昂贵的实验或模拟数据。


<details>
  <summary>Details</summary>
Motivation: 现有微分方程求解方法受限于对高成本标签数据的依赖或效率与精度的权衡。

Method: 利用微分方程的数学结构生成内嵌物理信息的解析解及合成数据，实现大规模算子发现。

Result: 在二维参数问题中展示了MAD的泛化能力及优异的效率和准确性，适用于边界值和源项均为函数的复杂情形。

Conclusion: MAD框架融合物理信息与数据驱动，可广泛应用于科学计算中的物理智能机器学习，具备成为通用范式的潜力。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [194] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 本文提出了一种参数化激活函数，旨在提升多维非线性数据回归的性能，强调激活函数的平滑性和梯度特性对神经网络表现的影响。


<details>
  <summary>Details</summary>
Motivation: 非线性激活函数对于学习非线性数据集是必要的，同时激活函数的平滑性和梯度性质会影响大规模神经网络的过拟合和参数敏感性。现有的激活函数存在性能受限或模型不连续性问题。

Method: 提出了一种平滑且可训练的“Leaky Exponential Linear Unit”激活函数，同时引入了一种新的扩散损失指标，用于评估训练模型的过拟合情况。

Result: 新激活函数在性能上优于ELU、SiLU及RELU类函数，能够有效减少模型过拟合和增加训练的稳定性。

Conclusion: 通过设计具有非零梯度且平滑的参数化激活函数，可以提升多维非线性数据回归的效果，并用新的扩散损失指标更好地衡量模型过拟合。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [195] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 本文旨在为随机优化算法提供无难解互信息项的拓扑泛化误差界，通过引入轨迹稳定性扩展假设集稳定性，结合拓扑数据分析（TDA）量化优化轨迹复杂度，建立可解释性的泛化误差界。


<details>
  <summary>Details</summary>
Motivation: 当前随机优化算法的泛化保证面临挑战，已有基于拓扑数据分析的泛化界虽然有效但依赖难以计算的互信息项，限制了其应用价值。

Method: 提出基于算法稳定性的学习理论框架，将假设集稳定性扩展为轨迹稳定性，利用该框架与TDA度量参数空间优化轨迹复杂度结合，推导出不含难解信息论项的泛化误差上界。

Result: 理论证明轨迹稳定算法的泛化误差可由算法的轨迹稳定参数及优化轨迹的TDA复杂度共同上界，实验验证了TDA度量在样本数增大时的显著作用，解释了拓扑泛化界应用的经验成效。

Conclusion: 本文构建了不含复杂信息论项的拓扑泛化误差界，为理解和保证现代随机优化算法的泛化性能提供了新视角和理论支持。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [196] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 本文提出了一种模仿学习方法，通过最大熵策略学习满足专家展示的约束条件的策略。


<details>
  <summary>Details</summary>
Motivation: 通过连接演示策略与学习策略之间的KL散度界限，结合强化学习的概率推断框架，旨在学习遵守约束的最大熵策略。

Method: 利用强化学习的概率推断框架，将最大熵策略学习问题形式化，采用对偶梯度下降方法优化目标函数，实现有效稳定的训练。

Result: 实验表明该方法能够学习满足多种不同类型约束的策略，支持多模态演示行为，并具备较好的泛化能力。

Conclusion: 提出的方法有效实现了在多约束、多模态条件下的最大熵模仿学习，具备稳健且泛化性能良好的策略学习能力。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [197] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 本文提出了一种新型语音分词器，能同时编码语言和声学信息，提升语音表示的保真度，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有残差矢量量化方法在引入语义元素时，常忽视关键的声学特征，限制了语音表示的质量和应用范围。

Method: 提出一种同时编码语言和声学信息的新方法，保留韵律和情感内容，以提高语音表示的整体质量。

Result: 实验证明该方法在语音编码、语音转换、情感识别和多模态语言建模等领域表现优异，且无需额外训练。

Conclusion: 该方法具备广泛适用性，能够显著推动基于AI的语音处理技术发展。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [198] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 本文提出了一种无需修改模型架构和损失函数的联邦学习模型合并方法LIVAR，通过利用本征的训练信号实现分类器和参数的有效融合，取得了多项基准测试的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习模型合并方法多需修改架构或损失函数，带来额外复杂性和开销。本文旨在通过利用已有训练信号，简化合并过程，提高效率和性能。

Method: 提出了LIVAR方法，包括基于特征统计的方差加权分类器聚合方案和基于SHAP分析的解释驱动LoRA合并技术，无需架构改动即可实现高效模型融合。

Result: LIVAR方法在多个基准测试中表现出色，达到了最新的性能水平，并能无缝集成到现有联邦学习框架中。

Conclusion: 本文证明了通过已有训练信号即可有效实现联邦学习中模型合并，开创了高效联邦模型聚合的新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [199] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文深入分析了三种主流原型模型（ProtoPNet, ProtoPool, PIPNet），并在多个不同数据集上进行性能对比，同时提出了新的解释性评估指标，且发布了开源代码库。


<details>
  <summary>Details</summary>
Motivation: 当前解释型人工智能和可解释机器学习中，原型模型是重要工具，但缺乏全面的评估和对比研究。

Method: 本文选取ProtoPNet, ProtoPool, PIPNet三种原型模型，利用文献中的标准指标及新提出的多项指标，在细粒度分类、非IID设置和多标签分类等数据集上进行综合性能和解释性评估。

Result: 分析结果揭示了各个模型在不同数据集和任务中的表现差异，同时新指标进一步丰富了对模型解释性的衡量。

Conclusion: 通过系统的分析和多指标评估，本文推动了原型模型的全面理解和实际应用，同时提供了一个可扩展的开源工具，便于后续研究者使用和扩展评估方法。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [200] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为HeLo的多模态情绪分布学习框架，通过融合生理信号和行为数据，挖掘多模态数据的异质性和标签之间的相关性，实现更准确的情绪分布识别。


<details>
  <summary>Details</summary>
Motivation: 现有情绪分布学习方法难以充分挖掘多模态数据间的异质性及不同基本情绪间丰富的语义关联。

Method: 采用交叉注意力融合生理数据，设计基于最优传输的异质性挖掘模块，利用可学习的标签嵌入及标签相关矩阵对标签相关性进行建模，并通过标签相关驱动的交叉注意力机制融合表示，实现精准情绪分布学习。

Result: 在两个公开数据集上的实验结果表明，所提方法在情绪分布学习任务中表现优越。

Conclusion: HeLo框架有效利用多模态数据的异质性及标签相关性，显著提升了多模态情绪分布识别的性能。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [201] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 本文介绍了一个基于Generals.io的实时战略游戏环境，支持高效运行，并通过监督学习和自我对弈训练出顶尖AI代理。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个高效、可扩展且具有挑战性的多智能体强化学习研究平台。

Method: 基于Generals.io构建兼容Gymnasium和PettingZoo的环境，采用监督预训练和自我对弈训练代理，结合潜在奖励塑造和记忆功能。

Result: 参考代理仅用36小时单GPU训练，即达到1v1人类排行榜前0.003%。

Conclusion: 该环境和智能体为多智能体强化学习提供了一个模块化、高性能且具竞争力的基准平台，推动相关研究发展。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [202] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: 本文提出了一种名为ReMix的离策略强化学习方法，用于提升大语言模型的推理能力，实现高效的强化微调。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法大多是基于在策略强化学习，无法充分利用过往数据，计算与时间成本高昂，限制了经济高效的模型扩展。

Method: 提出ReMix框架，包括混合策略近端策略梯度提高训练效率、KL凸策略约束平衡稳定性与灵活性、策略复兴实现高效早期学习与持续改进的平滑过渡。

Result: 在多个数学推理基准测试中，ReMix在1.5B和7B规模模型上均达到先进水平，训练资源消耗比现有方法降低30到450倍。

Conclusion: ReMix有效利用离策略数据显著提升强化微调效率，展现了在大语言模型推理能力提升中的巨大潜力，同时揭示了一些强化学习中的行为特征与挑战。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [203] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 论文提出结合迭代方法与路径条件技术，提高高斯过程在大数据和现代并行硬件上的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程方法难以应对大规模数据和现代并行计算硬件的需求，导致计算效率和内存使用受限。

Method: 将高昂计算转化为线性方程组求解，通过迭代线性系统求解器完成，减少内存占用，利用矩阵乘法适配现代硬件特性。

Result: 显著降低了内存需求，实现了高斯过程在大规模数据上的应用，提升了计算效率。

Conclusion: 结合迭代方法与路径条件能有效提升高斯过程模型的扩展性，适用于现代大规模计算环境。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [204] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra是一个利用扩散模型从多模态光谱数据直接生成2D和3D分子结构的框架，表现出高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分子结构解析依赖专家经验且扩展性差，现有机器学习方法依赖有限库并难以泛化，新兴生成模型大多忽视3D几何信息及多光谱整合。

Method: 提出DiffSpectra框架，结合扩散分子变换器（SE(3)等变架构）与变换器光谱编码器SpecFormer，实现条件生成式分子结构推断，并通过多模态光谱数据建模。

Result: DiffSpectra在结构解析任务中顶部准确率达到16.01%，前20名准确率高达96.86%，且从3D几何建模、预训练及多模态条件化中获益显著。

Conclusion: DiffSpectra有效结合多模态光谱信息和联合2D/3D生成模型，显著提升了分子结构解析的准确性，开创了新颖的去零起点结构解析方法。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [205] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文研究了包含资源限制的非平稳情境bandit问题，提出了一个在线算法，能在多轮重复试验中获得次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 针对现实中按情境变化且资源有限的动态决策问题（如动态定价和拍卖），当前方法难以处理非平稳情境和资源约束，亟需有效算法。

Method: 设计一个依赖置信界惩罚器（confidence bound oracle）的在线算法，解决了拥有无界状态空间的强化学习难题，并创新性地引入未标记特征数据以改善遗憾界。

Result: 所提算法在拥有置信界惩罚器的前提下，实现了遗憾在重复试验次数$T$下的次线性增长，超越了现有情境bandit-with-knapsack文献。

Conclusion: 本文成功应对了非平稳情境和资源约束的组合决策问题，且利用未标记数据提升性能，丰富了情境BwK领域的方法论。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [206] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种结合水平和垂直联邦设置的因果结构学习方法，通过高阶累积量聚合各客户端数据，恢复全局因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对水平联邦设置，忽视了不同客户端数据变量不完全重合的问题，导致因果关系识别偏差。

Method: 利用高阶累积量聚合多个客户端数据，构建全局累积量估计，通过递归源识别获得全局因果强度矩阵。

Result: 该方法在合成数据和真实数据上的实验表现优异，成功恢复了因果图及因果强度系数。

Conclusion: 本文方法有效解决了水平和垂直联邦设置下的因果结构学习问题，提升了因果发现的准确性和实用性。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [207] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的动态优化多维数据流滑动窗口大小的方法，用于应对数据流的高维度和非平稳性问题。


<details>
  <summary>Details</summary>
Motivation: 多维数据流具有高速、无界和复杂的维度间依赖，传统固定大小的滑动窗口难以适应动态变化如概念漂移和突发模式。

Method: 基于强化学习，将滑动窗口大小选择建模为RL问题，利用Dueling深度Q网络和优先经验回放，实现对窗口大小的动态自适应调整。

Result: 在多个基准数据集上，RL-Window在分类准确率、抗漂移能力和计算效率方面均优于现有方法ADWIN和CNN-Adaptive。

Conclusion: RL-Window通过高适应性和稳定性，适合实时多维数据流处理，提升了滑动窗口技术在动态数据环境中的表现。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [208] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 本文提出了一种结合安全感知加权软投票机制的多版本机器学习框架，以提升自动驾驶交通标志识别系统的对抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的交通标志识别易受对抗攻击威胁，这直接影响行车安全，因此需要提高系统的安全性和鲁棒性。

Method: 设计了一种基于故障模式及影响分析（FMEA）的安全感知加权软投票机制，在多版本机器学习框架中动态分配安全权重，并采用FGSM和PGD两种对抗攻击方法进行评估。

Result: 实验结果表明，所提框架显著增强了三版本多模型系统在面对对抗样本时的鲁棒性和安全性。

Conclusion: 结合安全感知权重的多版本机器学习策略有效提升了自动驾驶交通标志识别系统的安全防护能力。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [209] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 本文提出了在去中心化学习环境中估计数据影响级联（DICE）的方法，以解决贡献归因难题，从而实现公平激励。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化学习缺乏有效的激励机制，主要因贡献归因困难，影响参与积极性。

Method: 设计了DICE方法，通过理论推导影响级联的可计算近似，考虑数据、通信拓扑及损失曲率。

Result: 提出的DICE框架有效估计了去中心化网络中数据影响的传播，能用于协作者选择和恶意行为识别。

Conclusion: DICE为去中心化学习中的公平贡献归因和激励机制提供了理论基础，有助于激发大众参与。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [210] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 本文提出了一种评估基础模型归纳偏好的方法，通过合成数据检验模型是否具备对底层世界模型的理解。研究发现，尽管基础模型在训练任务上表现优异，但在适应新任务时未能体现对底层物理规律（如牛顿力学）的归纳偏好，表现为只学会了任务特定的启发式方法，难以泛化。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型是否能捕捉更深层次的结构尚难评估，迫切需要有效方法验证模型是否具备对底层世界模型的归纳偏好。

Method: 设计了一种归纳偏好探针，利用从假设的世界模型生成的合成数据，评估基础模型在适应新任务时是否能够体现对该世界模型的归纳偏好。

Result: 在多个领域实验中发现，基础模型虽然在训练任务表现良好，但在新任务上未能发展出向底层世界模型对齐的归纳偏好，尤其是在轨道运动任务中，未能应用牛顿力学。

Conclusion: 基础模型更多表现为学习任务特定的启发式策略，缺乏对更广泛物理规律的归纳能力，限制了其泛化性能。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [211] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 本文证明了在带噪声监督数据下，为使PINNs的经验风险低于噪声方差，网络参数规模需满足一定下限，并探讨了这一限制在完全无监督设定中的适用性，同时实验证明该条件下PINNs仍能达到较低经验风险。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据常含噪声，了解PINNs在噪声监督下有效训练的条件至关重要，但目前相关理论较少。

Method: 通过理论推导得到监督PINN经验风险低于噪声方差时网络参数数量的下界，分析无监督情况下的类似约束，结合实际数据验证理论结果。

Result: 明确了经验风险低于噪声方差时，神经网络参数数量必须满足$d_N\log d_N\gtrsim N_s \eta^2$，验证了通过增加带噪监督样本不能无限降低经验风险。

Conclusion: 提出了PINNs在带噪声环境中训练所需参数规模的理论基础，为后续设计更有效的PINN模型提供了定量指导。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [212] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: 本文提出了基于假设检验解释的差分隐私统一风险界限方法，使隐私风险评估更一致、更灵活，并减少了添加噪声量，提高了实际任务精度。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私参数映射至具体隐私风险的方法过于悲观且不一致，难以解释和校准。

Method: 利用差分隐私的假设检验解释($f$-DP)，统一构建了针对重新识别、属性推断和数据重构的攻击成功率界限，这些界限具有一致性和可调性，可适应不同基线风险水平。

Result: 该方法在实证上优于传统的$$-DP、R E9nyi DP和集中DP，噪声需求降低20%，文本分类准确率提升超过15个百分点。

Conclusion: 提出的统一视角为解读和校准差分隐私在特定隐私风险下的保护程度提供了原则性框架，提高了隐私风险评估的准确性和实用性。

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [213] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 本文提出了两种新型多视图对比学习损失函数MV-InfoNCE和MV-DHEL，解决了现有方法在处理多视图时的四大局限，提升了多视图对比学习的效果和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习方法在利用多个数据视图时存在目标冲突、交互建模不足、基础方法局限及难以充分利用多视图优势等问题。

Method: 提出MV-InfoNCE扩展InfoNCE以同时建模所有视图交互，MV-DHEL则解耦视图间的对齐和均匀性，提高交互复杂度，具有理论保障。

Result: 在ImageNet1K及其他三个数据集上，两种方法均优于现有多视图方法，并能扩展到多模态数据，多视图增多时MV-DHEL有效缓解了维度坍缩问题。

Conclusion: 所提多视图损失函数理论与实验证明能全面优化视图间对齐与均匀性，充分释放多视图学习潜力，提升性能并扩展应用范围。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [214] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: 本文提出了RawMed框架，旨在生成高度逼真的多表格时间序列电子健康记录（EHR）合成数据，以解决隐私限制导致的真实数据共享问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和法规限制，真实EHR数据难以共享，现有合成方法通常仅生成专家选择的部分特征，无法全面还原原始EHR的复杂结构和时间动态，迫切需要一种更真实的EHR数据合成方法。

Method: RawMed采用基于文本的表示和压缩技术，最小化预处理，能够捕捉EHR数据中的复杂多表结构和时间动态。同时，提出新的评价框架，综合评估合成数据的分布相似性、表间关联、时间动态和隐私保护。

Result: 在两个开源EHR数据集上的验证结果表明，RawMed在数据真实度和实用性方面明显优于基线模型。

Conclusion: RawMed为生成复杂结构的多表时间序列合成EHR数据提供了有效解决方案，有助于在保护患者隐私的前提下促进医疗数据的研究与应用。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [215] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文研究了扩散模型作为贝叶斯逆问题先验时在去模糊任务中的表现，通过计算扩散模型采样分布与理想解分布之间的Wasserstein距离，精确分析其理论分辨率与实际分辨率的差异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因其灵活性和高方差被广泛用于生成多解任务中，但其性能准确性尚存在疑问，尤其是在逆问题领域。

Method: 在高斯数据分布约束下，通过计算扩散模型采样分布与逆问题理想解分布的Wasserstein距离，定量分析模型在去模糊任务中的表现。

Result: 精确测量了扩散模型采样分布与理想解分布之间的差异，为不同算法的性能比较提供了客观依据。

Conclusion: 本文方法有效地定量评估了扩散模型在逆问题中的分辨率表现，为未来算法设计和性能提升提供了理论支持。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [216] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 本文研究了资源受限智能电表上的边缘侧模型训练，特别是在光伏功率预测任务中使用梯度提升树和循环神经网络模型，并设计了混合及降精度训练方案，提高了在有限资源下的训练可行性。


<details>
  <summary>Details</summary>
Motivation: 推动电网边缘智能发展，实现设备端模型训练，提升智能电表的自主学习能力，利用现有计量基础设施经济高效地实现智能化。

Method: 介绍了设备端训练的技术准备步骤，采用梯度提升树和循环神经网络两种机器学习模型进行光伏功率预测；并设计了适应资源限制的混合精度和降精度训练方案。

Result: 实验结果表明，在资源有限的智能电表上使用提出的训练方法，具备经济实用性，有效实现了边缘智能。

Conclusion: 该研究证明通过既有先进计量基础设施，在资源受限设备上进行设备端训练是可行的，为电网边缘智能提供了一种经济有效的实现途径。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [217] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME模型通过利用预训练蛋白语言模型的进化嵌入，提升了多序列比对（MSA）的设计和筛选，解决了低同源性和孤立蛋白预测的难题。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白折叠模型高度依赖MSA信息，但在低同源性和孤立蛋白中MSA稀缺，限制了预测性能。

Method: 提出PLAME，结合预训练语言模型的进化嵌入与保守-多样性损失增强MSA生成，引入MSA筛选方法和序列质量评估指标。

Result: 在AlphaFold2低同源和孤立蛋白基准测试中，PLAME表现出领先的折叠增强和序列质量评估效果，同时AlphaFold3上也有一致提升。消融实验和案例分析验证了方法有效性。

Conclusion: PLAME不仅提升MSA质量和结构预测准确率，还能作为适配器实现ESMFold速度下AlphaFold2水平的精度，推动蛋白质结构预测技术进步。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [218] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 本文研究了自监督学习在资源受限的边缘设备上的应用，旨在平衡模型性能与能耗。


<details>
  <summary>Details</summary>
Motivation: 自监督学习通常需要大量数据和计算资源，限制了其在边缘设备上的应用。

Method: 分析不同自监督学习技术在有限计算、数据及能量预算下的表现，并考虑半监督学习以降低标注数据的能耗。

Result: 定制的自监督学习策略在减少资源消耗最多4倍的同时，仍保持竞争性的性能。

Conclusion: 适当的自监督学习方法具有在边缘设备实现能效学习的潜力。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [219] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 提出一种结合多模型的集成嵌入方法，通过训练的元编码器提升LLM语义缓存系统的相似度检测，显著提高缓存命中率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存依赖单一嵌入模型，难以捕获真实查询中的多样化语义关系，限制缓存效率和准确性。

Method: 采用多嵌入模型结合训练的元编码器，构建集成嵌入方法来增强语义相似度识别能力。

Result: 在Quora数据集上，集成方法实现92%缓存命中率和85%准确率，显著优于单模型方法，提升缓存效果并减少计算开销。

Conclusion: 集成嵌入方法有效区分语义相似与不相似查询，提升LLM缓存系统性能和效率。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [220] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: 本文提出了针对领域增量学习中数据不平衡问题的双重平衡协作专家框架，实现了跨领域知识迁移与类内不平衡的有效解决。


<details>
  <summary>Details</summary>
Motivation: 领域增量学习中存在领域内类别不平衡和跨领域类别分布变化，导致模型难以兼顾历史知识保留与新知识学习。

Method: 提出Dual-Balance Collaborative Experts（DCE）框架，包含频次感知专家组以解决类内不平衡，并通过动态专家选择器结合平衡高斯采样合成伪特征，权衡多样本知识保持与少样本性能提升。

Result: 在四个基准数据集上的广泛实验表明，DCE框架达到了先进的性能表现。

Conclusion: DCE有效应对了领域增量学习中的数据不平衡挑战，提升了模型在非平稳环境下的持续学习能力。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [221] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 本文挑战了小批量训练不稳定的传统认知，提出调整Adam优化器超参数以适应小批量，甚至批量大小为1，取得了稳定且鲁棒的训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为小批量训练语言模型不稳定，常用梯度累积来扩大有效批量，但通常只调整学习率而忽略其他超参数，导致训练效果不佳。

Method: 本文提出了一个适用于小批量甚至单样本的Adam优化器超参数缩放规则，系统地测试了不同批量大小下的训练稳定性和性能。

Result: 小批量训练（包括批量大小为1）表现稳定，超参数鲁棒性更强，计算效率更高，甚至允许使用无动量的SGD实现稳定训练。

Conclusion: 建议合理选择小批量训练并调整优化器超参数，除非有多设备并行需求，不必使用梯度累积。该研究刷新了小批量训练的认知并提供了实用指导。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [222] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 本文研究了视觉模型在组合理解方面的表现，发现数据多样性而非数据规模驱动组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流观点认为扩大数据和模型规模能够提升模型在分布外任务中的表现，包括组合泛化，但实际效果尚不明确。

Method: 通过系统控制实验，改变数据规模、概念多样性和组合覆盖范围，分析这些因素对组合泛化的影响。

Result: 研究发现组合泛化主要由数据多样性驱动，增加组合覆盖促使模型形成线性分解的表示结构，预训练模型如DINO和CLIP表现尚可，但未达到理想状态。

Conclusion: 强调构建多样化数据集的重要性，并关注表示结构在高效组合学习中的作用，从而推动视觉组合泛化能力的发展。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [223] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: 本文介绍了美国陆军使用的医疗撤离战棋模拟系统MEWI，用于模拟战场医疗撤离任务并提升学员决策能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏中等规模的模拟工具来课堂环境中训练和评估医疗撤离的规划和决策。

Method: 开发了基于Unity的三维多人模拟系统MEWI，设计了两种战场撤离场景，并通过军事课程中学生的实地测试和数据收集验证效果。

Result: 实验证明MEWI显著提升了学生对医疗撤离知识的掌握和合作决策能力，同时收集到关键决策点和经验教训。

Conclusion: MEWI为医疗撤离训练提供了高保真工具，推动了医疗撤离教育和作战的改进。

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [224] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: 本文提出了Prompt Declaration Language（PDL），通过将提示（prompt）放在核心位置，简化了大语言模型（LLMs）提示工程的复杂性，实现了手动和自动提示调优，提升了程序员效率并支持优化。


<details>
  <summary>Details</summary>
Motivation: 现有的提示框架要么通过限制性API隐藏复杂性，要么提供固定模板，难以定制和实现复杂的agent程序设计。

Method: 提出PDL，一种将提示组合、规则代码及外部工具整合在一起的声明式语言，抽象底层实现细节，使得提示调优更灵活高效。

Result: 通过一个合规代理的案例研究，使用PDL调优提示模式使性能提升了最多4倍，相较于使用预设代理和提示模板。

Conclusion: PDL有效解决了提示工程中的复杂性问题，提升了定制化能力和程序员生产力，具有实际应用价值。

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [225] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: 本文提出并分析了“颠簸技术假说”，即人工智能能力呈超指数增长趋势，并通过理论和蒙特卡洛模拟建立和验证检测方法。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能能力是否存在加速增长的趋势，并开发工具为未来实证研究提供基础，以理解这一增长模式可能带来的影响。

Method: 构建理论框架，利用蒙特卡洛模拟验证检测超指数增长的方法，研究想法到行动时间缩短和迭代改进如何推动加速模式。

Result: 成功建立了检测理论和方法，通过模拟验证了其有效性，为未来随着数据积累的实证研究奠定基础。

Conclusion: 正式化了颠簸技术增长动态，提供数学基础帮助理解AI发展轨迹及其对AGI出现的潜在影响，为研究和政策制定提供参考。

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [226] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 本文研究了三种辩证系统模型，证明了q-辩证系统的表达能力严格强于p-辩证系统，后者又强于(d-)辩证系统。


<details>
  <summary>Details</summary>
Motivation: 辩证系统作为一个统一且可计算的框架，用来描述自动化智能体动态管理信念的能力，但不同模型之间的能力比较存在未解问题。

Method: 通过数学证明展示q-辩证系统在信念修正能力上比p-辩证系统更强，而p-辩证系统又强于(d-)辩证系统。

Result: 解决了文献中的一个开放性问题，明确了三种辩证系统的表达能力层级：q > p > (d-)。

Conclusion: 强调了反例与矛盾在自动化信念修正和数学家推理过程中的互补作用，增强了对信念管理模型的理解。

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [227] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 本文探讨了争论框架（AFs）中递归方法在无限框架中的扩展问题，提出两种新的方法并评估其性能，推动了无限论证理论的发展。


<details>
  <summary>Details</summary>
Motivation: 已有的强连通分量递归(SCC-recursive)语义在有限论证框架中表现良好，但无法可靠地推广到无限框架，因其涉及良基性问题，需要解决这一瓶颈以支持更广泛的应用。

Method: 提出两种将SCC递归扩展到无限论证框架的方法，利用Baroni和Giacomin的标准对这些语义进行系统评估，验证其相应性能和性质，特别考察其在有限性框架中的表现。

Result: 发现方向性原则在一般情况下不成立，但在有限性框架中部分新提出的语义满足方向性，从而揭示了无限框架中递归语义的性质及其限制。

Conclusion: 本文的方法丰富了无限论证框架的理论基础，为设计能够处理无限、动态领域的推理系统奠定了基础。

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [228] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: 本文提出了一种系统化的指令数据构建框架，通过层级标注、种子选择、进化合成和模型缺陷诊断等方法构建了高质量的指令数据集InfinityInstruct-Subject，有效提升了大规模预训练模型的指令执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模预训练模型虽然使用了千万级别的指令数据集进行微调，但在复杂指令和冷门领域任务上的表现仍不理想，主要因指令数据集在任务类型和知识覆盖（覆盖度）及复杂度（深度）方面扩展有限。

Method: 提出系统化指令数据构建框架，包括层级标注系统、信息性种子选择算法、进化数据合成过程及针对模型弱点的定向数据生成，形成一个迭代闭环，不断提升指令数据的覆盖度和深度。

Result: 基于该框架构建的InfinityInstruct-Subject数据集包含约150万条高质量指令，在多种基础模型和基准任务上均显著提升了模型的指令执行能力，与现有合成指令数据集相比，覆盖度和深度都有显著增强。

Conclusion: 本文工作为指令数据集的高效、持续演进提供了理论和实践基础，推动从数据量扩展向数据质量提升发展，有助于促进大规模预训练模型更好地完成复杂和多样化任务。

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [229] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一个由三个合作代理组成的旅行规划系统，解决了传统系统中的智能规划、精准导航和动态调整问题。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划系统静态且碎片化，难以应对现实中的环境变化和行程中断，导致用户体验差。

Method: 设计并实现了三个合作代理：旅行规划代理利用网格空间定位和地图分析处理复杂多模式查询；目的地助手代理提供最后100米的精准导航；本地发现代理通过图像嵌入和增强检索生成技术检测行程中断并响应。

Result: 系统在查询理解、导航精准度和应对中断能力方面均表现出显著提升。

Conclusion: 提出的多代理协作系统有效提升了旅行规划的智能化水平和适应性，适用于城市探索和紧急响应等场景。

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [230] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: 提出了FR3E框架，通过识别高不确定性决策点进行有针对性的探索，从而提升大语言模型的推理能力和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习在大语言模型推理中存在探索不稳定的问题。

Method: 设计了FR3E，一种结构化探索方法，通过识别推理轨迹中的高不确定决策点，并进行针对性回滚，构造语义层面的中间反馈指导学习，无需密集监督。

Result: 在数学推理基准AIME24上，FR3E实现了更稳定的训练，生成了更长且连贯的回答，且完全正确的推理轨迹比例提升。

Conclusion: FR3E框架通过更稳健和结构化的探索显著提升了大语言模型的推理能力，增强了训练的稳定性和结果的准确性。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>
